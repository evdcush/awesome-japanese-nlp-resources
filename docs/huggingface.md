# awesome-japanese-nlp-resources

This page lists the models and datasets registered with [Haggingface](https://huggingface.co) that are specific to Japanese NLP. At present, 459 models and 89 datasets are listed.

# Contents

 * [Models](#models)
 * [Datasets](#datasets)

## Models

This list is sorted by downloads as of May 13, 2024.

 * 📥 26775140 [tohoku-nlp/bert-base-japanese](https://huggingface.co/tohoku-nlp/bert-base-japanese) - BERT base Japanese (IPA dictionary)This is a BERT model pretrained on texts in the Japanese language.
 * 📥 1213634 [sonoisa/sentence-bert-base-ja-mean-tokens-v2](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens-v2) - This is a Japanese sentence-BERT model.
 * 📥 1046750 [tsmatz/xlm-roberta-ner-japanese](https://huggingface.co/tsmatz/xlm-roberta-ner-japanese) - xlm-roberta-ner-japanese(Japanese caption : 日本語の固有表現抽出のモデル)This model is a fine-tuned version of xlm-roberta-base (pre-trained cross-lingual RobertaModel) trained for named entity recognition (NER) token classification.
 * 📥 516521 [jonatasgrosman/wav2vec2-large-xlsr-53-japanese](https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-japanese) - Fine-tuned XLSR-53 large model for speech recognition in JapaneseFine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the train and validation splits of Common Voice 6.1, CSS10 and JSUT.When using this model, make sure that your speech input is sampled at 16kHz.
 * 📥 509124 [augmxnt/shisa-gamma-7b-v1](https://huggingface.co/augmxnt/shisa-gamma-7b-v1) - shisa-gamma-7b-v1For more information see our main Shisa 7B modelWe applied a version of our fine-tune data set onto Japanese Stable LM Base Gamma 7B and it performed pretty well, just sharing since it might be of interest.
 * 📥 420870 [sonoisa/sentence-luke-japanese-base-lite](https://huggingface.co/sonoisa/sentence-luke-japanese-base-lite) - This is a Japanese sentence-LUKE model.
 * 📥 185829 [tohoku-nlp/bert-base-japanese-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-whole-word-masking) - BERT base Japanese (IPA dictionary, whole word masking enabled)This is a BERT model pretrained on texts in the Japanese language.
 * 📥 125132 [kha-white/manga-ocr-base](https://huggingface.co/kha-white/manga-ocr-base) - Manga OCROptical character recognition for Japanese text, with the main focus being Japanese manga.
 * 📥 123052 [hotchpotch/japanese-reranker-cross-encoder-xsmall-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-xsmall-v1) - hotchpotch/japanese-reranker-cross-encoder-xsmall-v1日本語で学習させた Reranker (CrossEncoder) シリーズです。モデル名layershidden_sizehotchpotch/japanese-reranker-cross-encoder-xsmall-v16384hotchpotch/japanese-reranker-cross-encoder-small-v112384hotchpotch/japanese-reranker-cross-encoder-base-v112768hotchpotch/japanese-reranker-cross-encoder-large-v1241024hotchpotch/japanese-bge-reranker-v2-m3-v1241024Reranker についてや、技術レポート・評価等は以下を参考ください。日本語最高性能のRerankerをリリース / そもそも Reranker とは?日本語 Reranker 作成のテクニカルレポート使い方SentenceTransformersfr
 * 📥 110673 [tohoku-nlp/bert-base-japanese-char-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v2) - BERT base Japanese (character-level tokenization with whole word masking, jawiki-20200831)This is a BERT model pretrained on texts in the Japanese language.
 * 📥 104772 [tohoku-nlp/bert-base-japanese-char](https://huggingface.co/tohoku-nlp/bert-base-japanese-char) - BERT base Japanese (character tokenization)This is a BERT model pretrained on texts in the Japanese language.
 * 📥 77775 [elyza/ELYZA-japanese-Llama-2-7b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-instruct) - ELYZA-japanese-Llama-2-7bModel DescriptionELYZA-japanese-Llama-2-7b
 * 📥 60368 [sonoisa/sentence-bert-base-ja-mean-tokens](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens) - This is a Japanese sentence-BERT model.
 * 📥 59719 [colorfulscoop/sbert-base-ja](https://huggingface.co/colorfulscoop/sbert-base-ja) - Sentence BERT base Japanese modelThis repository contains a Sentence BERT base model for Japanese.
 * 📥 48568 [llm-book/bert-base-japanese-v3-ner-wikipedia-dataset](https://huggingface.co/llm-book/bert-base-japanese-v3-ner-wikipedia-dataset) - llm-book/bert-base-japanese-v3-ner-wikipedia-dataset「大規模言語モデル入門」の第6章で紹介している固有表現認識のモデルです。cl-tohoku/bert-base-japanese-v3をllm-book/ner-wikipedia-datasetでファインチューニングして構築されています。関連リンクGitHubリポジトリColabノートブックデータセット大規模言語モデル入門（Amazon.co.jp）大規模言語モデル入門（gihyo.jp）使い方from transformers import pipelinefrom pprint import pprintner_pipeline = pipeline(model="llm-book/bert-base-japanese-v3-ner-wikipedia-dataset",aggregation_strategy="simple",)text = "大谷翔平は岩手県水沢市出身のプロ野球選手"# text中の固有表現を抽出pprint(ner_pipeline(text))
 * 📥 36492 [ku-nlp/deberta-v2-base-japanese](https://huggingface.co/ku-nlp/deberta-v2-base-japanese) - Model Card for Japanese DeBERTa V2 baseModel
 * 📥 35226 [sazyou-roukaku/BracingEvoMix](https://huggingface.co/sazyou-roukaku/BracingEvoMix) - License:CreativeML Open RAIL-MAdditional Copyright: sazyou_roukaku (TwitterID @sazyou_roukaku) as of May 31, 2023このモデルは『CreativeML Open RAIL-M』でLicenseそのものに変更はありません。
 * 📥 27874 [setu4993/LaBSE](https://huggingface.co/setu4993/LaBSE) - LaBSEModel descriptionLanguage-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages.
 * 📥 25938 [tohoku-nlp/bert-base-japanese-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-v3) - BERT base Japanese (unidic-lite with whole word masking, CC-100 and jawiki-20230102)This is a BERT model pretrained on texts in the Japanese language.
 * 📥 25928 [pkshatech/GLuCoSE-base-ja](https://huggingface.co/pkshatech/GLuCoSE-base-ja) - GLuCoSE (General Luke-based Contrastive Sentence Embedding)-base-Japanese日本語のREADME/Japanese READMEGLuCoSE (General LUke-based COntrastive Sentence Embedding, "glucose") is a Japanese text embedding model based on LUKE.
 * 📥 17520 [cyberagent/open-calm-medium](https://huggingface.co/cyberagent/open-calm-medium) - OpenCALM-MediumModel DescriptionOpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by
 * 📥 14498 [tokyotech-llm/Swallow-70b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-instruct-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
 * 📥 13867 [rinna/japanese-roberta-base](https://huggingface.co/rinna/japanese-roberta-base) - japanese-roberta-baseThis repository provides a base-sized Japanese RoBERTa model.
 * 📥 13658 [tohoku-nlp/bert-base-japanese-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-v2) - BERT base Japanese (unidic-lite with whole word masking, jawiki-20200831)This is a BERT model pretrained on texts in the Japanese language.
 * 📥 13281 [tokyotech-llm/Swallow-7b-NVE-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-NVE-instruct-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
 * 📥 12699 [rinna/japanese-clip-vit-b-16](https://huggingface.co/rinna/japanese-clip-vit-b-16) - rinna/japanese-clip-vit-b-16This is a Japanese CLIP (Contrastive Language-Image Pre-Training) model trained by rinna Co., Ltd..Please see japanese-clip for the other available models.
 * 📥 12559 [sazyou-roukaku/chilled_remix](https://huggingface.co/sazyou-roukaku/chilled_remix) - 【告知】chilled_remix及びreversemixは2023年5月21日にVersion変更を行い、v2へ移行いたしました。
 * 📥 11143 [reazon-research/reazonspeech-nemo-v2](https://huggingface.co/reazon-research/reazonspeech-nemo-v2) - reazonspeech-nemo-v2reazonspeech-nemo-v2 is an automatic speech recognition model trainedon ReazonSpeech v2.0 corpus.
 * 📥 10859 [tokyotech-llm/Swallow-7b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-instruct-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
 * 📥 9977 [elyza/ELYZA-japanese-Llama-2-7b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b) - ELYZA-japanese-Llama-2-7bModel DescriptionELYZA-japanese-Llama-2-7b
 * 📥 9885 [Lasorco/lametta](https://huggingface.co/Lasorco/lametta) - このモデルは何？
 * 📥 9256 [cyberagent/open-calm-3b](https://huggingface.co/cyberagent/open-calm-3b) - OpenCALM-3BModel DescriptionOpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by CyberAgent, Inc.
 * 📥 9129 [elyza/ELYZA-japanese-Llama-2-13b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-instruct) - ELYZA-japanese-Llama-2-13bModel DescriptionELYZA-japanese-Llama-2-13b は、 Llama 2をベースとして日本語能力を拡張するために追加事前学習を行ったモデルです。詳細は
 * 📥 9111 [alabnii/jmedroberta-base-sentencepiece-vocab50000](https://huggingface.co/alabnii/jmedroberta-base-sentencepiece-vocab50000) - alabnii/jmedroberta-base-sentencepiece-vocab50000Model descriptionThis is a Japanese RoBERTa base model pre-trained on academic articles in medical sciences collected by Japan Science and Technology Agency (JST).This model is released under the Creative Commons 4.0 International License (CC BY-NC-SA
 * 📥 7983 [cyberagent/open-calm-7b](https://huggingface.co/cyberagent/open-calm-7b) - OpenCALM-7BModel DescriptionOpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by CyberAgent, Inc.
 * 📥 7919 [oshizo/sbert-jsnli-luke-japanese-base-lite](https://huggingface.co/oshizo/sbert-jsnli-luke-japanese-base-lite) - sbert-jsnli-luke-japanese-base-liteThis is a sentence-transformers model: It maps sentences &amp; paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.
 * 📥 7884 [elyza/ELYZA-japanese-Llama-2-7b-fast-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast-instruct) - ELYZA-japanese-Llama-2-7bModel DescriptionELYZA-japanese-Llama-2-7b
 * 📥 7812 [tohoku-nlp/bert-large-japanese](https://huggingface.co/tohoku-nlp/bert-large-japanese) - BERT large Japanese (unidic-lite with whole word masking, jawiki-20200831)This is a BERT model pretrained on texts in the Japanese language.
 * 📥 6638 [abeja/gpt-neox-japanese-2.7b](https://huggingface.co/abeja/gpt-neox-japanese-2.7b) - gpt-neox-japanese-2.7bThe open PR is merged on 2022/9/14.You can use this model with v4.23 and higher versions of transformers as follows,pip install transformersThis repository provides a 2.7B-parameter Japanese GPT-NeoX-based model.
 * 📥 6491 [stabilityai/japanese-stablelm-base-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-gamma-7b) - Japanese Stable LM Base Gamma 7BModel
 * 📥 6162 [mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf) - ELYZA-japanese-Llama-2-7b-fast-instruct-ggufELYZAさんが公開しているELYZA-japanese-Llama-2-7b-fast-instructのggufフォーマット変換版です。他のモデルはこちら通常版: llama2に日本語のデータセットで学習したモデルmmnga/ELYZA-japanese-Llama-2-7b-ggufmmnga/ELYZA-japanese-Llama-2-7b-instruct-ggufFast版 日本語の語彙を追加してトークンコストを減らし、1.8倍高速化したモデルmmnga/ELYZA-japanese-Llama-2-7b-fast-ggufmmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-ggufCodellama版 GGUFmmnga/ELYZA-japanese-CodeLlama-7b-ggufmmnga/ELYZA-japanese-CodeLlama-7b-instruct-ggufCodellama版 GPTQmmnga/ELYZA-japanese-CodeLlama-
 * 📥 6087 [elyza/ELYZA-japanese-Llama-2-13b-fast-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-fast-instruct) - ELYZA-japanese-Llama-2-13b-fast-instructModel DescriptionELYZA-japanese-Llama-2-13b は、 Llama 2をベースとして日本語能力を拡張するために追加事前学習を行ったモデルです。詳細は
 * 📥 6051 [mmnga/umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf](https://huggingface.co/mmnga/umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf) - umiyuki-Japanese-Chat-Umievo-itr001-7b-ggufumiyukiさんが公開しているJapanese-Chat-Umievo-itr001-7bのggufフォーマット変換版です。imatrixのデータはTFMC/imatrix-dataset-for-japanese-llmを使用して作成しました。Usagegit clone https://github.com/ggerganov/llama.cpp.gitcd llama.cppmake -j./main
 * 📥 5823 [cheonboy/sentence_embedding_japanese](https://huggingface.co/cheonboy/sentence_embedding_japanese) - This is a Japanese sentence-LUKE model.
 * 📥 5546 [kotoba-tech/kotoba-whisper-v1.0](https://huggingface.co/kotoba-tech/kotoba-whisper-v1.0) - Kotoba-WhisperKotoba-Whisper is a collection of distilled Whisper models for Japanese ASR, developed through the collaboration bewteenAsahi Ushio and Kotoba Technologies.
 * 📥 5175 [stabilityai/japanese-stablelm-instruct-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-gamma-7b) - Japanese Stable LM Instruct Gamma 7BModel
 * 📥 5171 [megagonlabs/transformers-ud-japanese-electra-base-ginza-510](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza-510) - transformers-ud-japanese-electra-ginza-510 (sudachitra-wordpiece, mC4 Japanese)This is an ELECTRA model pretrained on approximately 200M Japanese sentences extracted from the mC4 and finetuned by spaCy v3 on UD_Japanese_BCCWJ r2.8.The base pretrain model is megagonlabs/transformers-ud-japanese-electra-base-discrimininator.
 * 📥 4985 [mmnga/gemma-7b-it-gguf](https://huggingface.co/mmnga/gemma-7b-it-gguf) - gemma-7b-it-ggufgoogleさんが公開しているgemma-7b-itのggufフォーマット変換版です。現在量子化された出力が不安定な問題があるらしくQ8_0を推奨します。gemma : token_embd.weight テンソルに Q8_0 を使用します #5650Licencegemma-terms-of-use 利用規約をご利用前に必ずご確認ください。他のモデルmmnga/codegemma-1.1-7b-it-ggufmmnga/codegemma-1.1-2b-ggufmmnga/gemma-2b-it-ggufmmnga/gemma-7b-it-ggufmmnga/gemma-1.1-7b-it-ggufmmnga/codegemma-7b-it-ggufUsagegit clone https://github.com/ggerganov/llama.cpp.gitcd llama.cppmake -j./main
 * 📥 4965 [tokyotech-llm/Swallow-13b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-instruct-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
 * 📥 4925 [rinna/youri-7b](https://huggingface.co/rinna/youri-7b) - rinna/youri-7bOverviewWe conduct continual pre-training of llama2-7b on 40B tokens from a mixture of Japanese and English datasets.
 * 📥 4801 [elyza/ELYZA-japanese-Llama-2-13b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b) - ELYZA-japanese-Llama-2-13bModel DescriptionELYZA-japanese-Llama-2-13b は、 Llama 2をベースとして日本語能力を拡張するために追加事前学習を行ったモデルです。詳細は
 * 📥 4737 [tokyotech-llm/Swallow-70b-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
 * 📥 4704 [mmnga/aixsatoshi-Llama-3-8b-Cosmopedia-japanese-gguf](https://huggingface.co/mmnga/aixsatoshi-Llama-3-8b-Cosmopedia-japanese-gguf) - aixsatoshi-Llama-3-8b-Cosmopedia-japanese-ggufaixsatoshiさんが公開しているLlama-3-8b-Cosmopedia-japaneseのggufフォーマット変換版です。imatrixのデータはTFMC/imatrix-dataset-for-japanese-llmを使用して作成しました。他のモデルmmnga/aixsatoshi-Llama-3-8b-Cosmopedia-japanese-ggufmmnga/aixsatoshi-Honyaku-7b-v2-ggufmmnga/aixsatoshi-Honyaku-Multi-Translator-Swallow-ms7b-ggufmmnga/aixsatoshi-Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2-ggufmmnga/aixsatoshi-Mixtral-8x7B-ja-sft-ChatbotArenaJAcalm2-bnb4bitmmnga/aixsatoshi-calm2-7b-chat-7b-moe-ggufUsagegit c
 * 📥 4679 [tokyotech-llm/Swallow-7b-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
 * 📥 4609 [mmnga/lightblue-suzume-llama-3-8B-japanese-gguf](https://huggingface.co/mmnga/lightblue-suzume-llama-3-8B-japanese-gguf) - lightblue-suzume-llama-3-8B-japanese-gguflightblueさんが公開しているsuzume-llama-3-8B-japaneseのggufフォーマット変換版です。imatrixのデータはTFMC/imatrix-dataset-for-japanese-llmを使用して作成しました。他のモデルmmnga/lightblue-Karasu-Mixtral-8x22B-v0.1-ggufmmnga/lightblue-suzume-llama-3-8B-multilingual-ggufmmnga/lightblue-suzume-llama-3-8B-japanese-ggufmmnga/lightblue-ao-karasu-72B-ggufmmnga/lightblue-karasu-1.1B-ggufmmnga/lightblue-karasu-7B-chat-plus-unleashed-ggufmmnga/lightblue-qarasu-14B-chat-plus-unleashed-ggufUsagegit clone https://github.com
 * 📥 4589 [mmnga/haqishen-Llama-3-8B-Japanese-Instruct-gguf](https://huggingface.co/mmnga/haqishen-Llama-3-8B-Japanese-Instruct-gguf) - haqishen-Llama-3-8B-Japanese-Instruct-ggufhaqishenさんが公開しているLlama-3-8B-Japanese-Instructのggufフォーマット変換版です。imatrixのデータはTFMC/imatrix-dataset-for-japanese-llmを使用して作成しました。他のモデルmmnga/haqishen-Llama-3-8B-Japanese-Instruct-ggufUsagegit clone https://github.com/ggerganov/llama.cpp.gitcd llama.cppmake
 * 📥 4582 [ku-nlp/deberta-v2-tiny-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-tiny-japanese-char-wwm) - Model Card for Japanese character-level DeBERTa V2 tinyModel descriptionThis is a Japanese DeBERTa V2 tiny model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.This model is trained with character-level tokenization and whole word masking.
 * 📥 4535 [stabilityai/japanese-stable-clip-vit-l-16](https://huggingface.co/stabilityai/japanese-stable-clip-vit-l-16) - By downloading, using, or distributing any portion or element of this model, you agree to be bound by the agreement described in the LICENSE file.
 * 📥 4488 [elyza/ELYZA-japanese-Llama-2-13b-fast](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-fast) - ELYZA-japanese-Llama-2-13b-fastModel DescriptionELYZA-japanese-Llama-2-13b は、 Llama 2をベースとして日本語能力を拡張するために追加事前学習を行ったモデルです。詳細は
 * 📥 4444 [bclavie/JaColBERT](https://huggingface.co/bclavie/JaColBERT) - このドキュメントの日本語版はまだ作成中です。申し訳ありません。IntroDetailed report in the arXiv ReportIf you just want to check out how to use the model, please check out the Usage section below!Welcome to JaColBERT version 1, the initial release of JaColBERT, a Japanese-only document retrieval model based on ColBERT.It outperforms previous common Japanese models used for document retrieval, and gets close to the performance of multilingual models, despite the evaluation datasets being out-of-domain for our models but in-domain for multi
 * 📥 4399 [tohoku-nlp/bert-base-japanese-char-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-whole-word-masking) - BERT base Japanese (character tokenization, whole word masking enabled)This is a BERT model pretrained on texts in the Japanese language.
 * 📥 4316 [elyza/ELYZA-japanese-Llama-2-7b-fast](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast) - ELYZA-japanese-Llama-2-7bModel DescriptionELYZA-japanese-Llama-2-7b
 * 📥 4296 [rinna/japanese-gpt2-small](https://huggingface.co/rinna/japanese-gpt2-small) - japanese-gpt2-smallThis repository provides a small-sized Japanese GPT-2 model.
 * 📥 4115 [KoichiYasuoka/bert-base-japanese-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-upos) - bert-base-japanese-uposModel DescriptionThis is a BERT model pre-trained on Japanese Wikipedia texts for POS-tagging and dependency-parsing, derived from bert-base-japanese-char-extended.
 * 📥 4090 [sonoisa/t5-base-japanese](https://huggingface.co/sonoisa/t5-base-japanese) - 日本語T5事前学習済みモデルThis is a T5 (Text-to-Text Transfer Transformer) model pretrained on Japanese corpus.
 * 📥 3879 [cyberagent/open-calm-large](https://huggingface.co/cyberagent/open-calm-large) - OpenCALM-LargeModel DescriptionOpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by
 * 📥 3762 [Helsinki-NLP/opus-tatoeba-en-ja](https://huggingface.co/Helsinki-NLP/opus-tatoeba-en-ja) - en-jasource group: Englishtarget group: JapaneseOPUS readme: eng-jpnmodel: transformer-alignsource language(s): engtarget language(s): jpnmodel: transformer-alignpre-processing: normalization + SentencePiece (spm32k,spm32k)download original weights: opus+bt-2021-04-10.ziptest set translations: opus+bt-2021-04-10.test.txttest set scores: opus+bt-2021-04-10.eval.txtBenchmarkstestsetBLEUchr-F#sent#wordsBPTatoeba-test.eng-jpn15.20.25810000992061.000System Info:hf_name: en-jasource_languages: engtarget_languages
 * 📥 3759 [FINGU-AI/FinguAI-Chat-v1](https://huggingface.co/FINGU-AI/FinguAI-Chat-v1) - FINGU-AI/FinguAI-Chat-v1OverviewThe FINGU-AI/FinguAI-Chat-v1 model offers a specialized curriculum tailored to English, Korean, and Japanese speakers interested in finance, investment, and legal frameworks.
 * 📥 3720 [den2nova/FlexDreamHK](https://huggingface.co/den2nova/FlexDreamHK) - 🎈 FlexDreamHKFlexDreamHKはリークされたNovelAIモデルの入っていない、あるいはそのリスクを可能な限り低くしたモデルを目指して作成しました。
 * 📥 3582 [rinna/bilingual-gpt-neox-4b](https://huggingface.co/rinna/bilingual-gpt-neox-4b) - bilingual-gpt-neox-4bOverviewThis repository provides an English-Japanese bilingual GPT-NeoX model of 3.8 billion parameters.
 * 📥 3507 [rinna/japanese-hubert-base](https://huggingface.co/rinna/japanese-hubert-base) - rinna/japanese-hubert-baseOverviewThis is a Japanese HuBERT Base model trained by rinna Co., Ltd.Model summaryThe model architecture is the same as the original HuBERT Base model, which contains 12 transformer layers with 12 attention heads.
 * 📥 3472 [pkshatech/simcse-ja-bert-base-clcmlp](https://huggingface.co/pkshatech/simcse-ja-bert-base-clcmlp) - Japanese SimCSE (BERT-base)日本語のREADME/Japanese READMEsummarymodel name: pkshatech/simcse-ja-bert-base-clcmlpThis is a Japanese SimCSE model.
 * 📥 3388 [augmxnt/shisa-base-7b-v1](https://huggingface.co/augmxnt/shisa-base-7b-v1) - shisa-base-7b-v1shisa-base-7b-v1 takes Mistral 7B and adds an additional 8B tokens of primarily Japanese pre-training.
 * 📥 3350 [line-corporation/line-distilbert-base-japanese](https://huggingface.co/line-corporation/line-distilbert-base-japanese) - LINE DistilBERT
 * 📥 3220 [augmxnt/shisa-7b-v1](https://huggingface.co/augmxnt/shisa-7b-v1) - Shisa 7BShisa 7B (shisa-7b-v1) is a bilingual Japanese and English (JA/EN) general-purpose chat model that aims to achieve strong Japanese language performance while retaining robust English capabilities, using a synthetic-data driven approach.
 * 📥 3117 [cyberagent/calm2-7b](https://huggingface.co/cyberagent/calm2-7b) - CyberAgentLM2-7B (CALM2-7B)Model DescriptionCyberAgentLM2 is a decoder-only language model pre-trained on the 1.3T tokens of publicly available Japanese and English datasets.
 * 📥 3094 [OrionStarAI/Orion-14B-Chat](https://huggingface.co/OrionStarAI/Orion-14B-Chat) - Orion-14B🌐English | 🇨🇳中文 | 🇯🇵日本語 | 🇰🇷한국어🤗 HuggingFace Mainpage | 🤖 ModelScope Mainpage🎬 HuggingFace Demo | 🎫 ModelScope Demo😺 GitHub📖 Tech ReportTable of Contents📖 Model Introduction🔗 Model Download🔖 Model Benchmark📊 Model Inference📜 Declarations &amp; License🥇 Company Introduction1.
 * 📥 3081 [ku-nlp/deberta-v2-tiny-japanese](https://huggingface.co/ku-nlp/deberta-v2-tiny-japanese) - Model Card for Japanese DeBERTa V2 tinyModel descriptionThis is a Japanese DeBERTa V2 tiny model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained('ku-nlp/deberta-v2-tiny-japanese')model = AutoModelForMaskedLM.from_pretrained('ku-nlp/deberta-v2-tiny-japanese')sentence = '京都 大学 で 自然 言語
 * 📥 3058 [ku-nlp/deberta-v2-large-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-large-japanese-char-wwm) - Model Card for Japanese character-level DeBERTa V2 largeModel descriptionThis is a Japanese DeBERTa V2 large model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.This model is trained with character-level tokenization and whole word masking.
 * 📥 3028 [rinna/bilingual-gpt-neox-4b-8k](https://huggingface.co/rinna/bilingual-gpt-neox-4b-8k) - bilingual-gpt-neox-4b-8kOverviewNotice: This model requires transformers&gt;=4.31.0 to work properly.
 * 📥 2867 [rinna/japanese-gpt2-xsmall](https://huggingface.co/rinna/japanese-gpt2-xsmall) - japanese-gpt2-xsmallThis repository provides an extra-small-sized Japanese GPT-2 model.
 * 📥 2783 [rinna/japanese-gpt-neox-3.6b](https://huggingface.co/rinna/japanese-gpt-neox-3.6b) - japanese-gpt-neox-3.6bOverviewThis repository provides a Japanese GPT-NeoX model of 3.6 billion parameters.
 * 📥 2674 [Aratako/c4ai-command-r-v01-japanese-instruct](https://huggingface.co/Aratako/c4ai-command-r-v01-japanese-instruct) - c4ai-command-r-v01-japanese-instructGGUF版はこちら/Click here for the GGUF version概要CohereForAI/c4ai-command-r-v01を、ichikara-instructionを使って追加で日本語インストラクションチューニングを施したモデルです。学習の設定RunpodでGPUサーバを借り、A6000x4で学習を行いました。主な学習パラメータは以下の通りです。lora_r: 64lisa_alpha: 128lora_dropout: 0.05lora_target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]learning_rate: 2e-5num_train_epochs: 10epochsbatch_size: 50max_seq_length: 2048評価jsquad(jsquad-1.1-0.3, 2-shots)、jcommonsenseqa(jcommonsenseqa-1.1-0
 * 📥 2665 [tohoku-nlp/bert-large-japanese-v2](https://huggingface.co/tohoku-nlp/bert-large-japanese-v2) - BERT large Japanese (unidic-lite with whole word masking, CC-100 and jawiki-20230102)This is a BERT model pretrained on texts in the Japanese language.
 * 📥 2661 [rinna/japanese-gpt2-medium](https://huggingface.co/rinna/japanese-gpt2-medium) - japanese-gpt2-mediumThis repository provides a medium-sized Japanese GPT-2 model.
 * 📥 2658 [megagonlabs/transformers-ud-japanese-electra-base-discriminator](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-discriminator) - transformers-ud-japanese-electra-ginza (sudachitra-wordpiece, mC4 Japanese) -
 * 📥 2599 [karakuri-ai/karakuri-lm-70b-chat-v0.1](https://huggingface.co/karakuri-ai/karakuri-lm-70b-chat-v0.1) - KARAKURI LMKARAKURI LM is a pretrained language model that builds upon Llama 2.Our model enhances Llama 2's capabilities by incorporating additional Japanese vocabulary and further pretraining on a mixture of Japanese and multilingual corpora.
 * 📥 2525 [mmnga/ELYZA-japanese-Llama-2-7b-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-instruct-gguf) - ELYZA-japanese-Llama-2-7b-instruct-ggufELYZAさんが公開しているELYZA-japanese-Llama-2-7b-instructのggufフォーマット変換版です。他のモデルはこちら通常版: llama2に日本語のデータセットで学習したモデルmmnga/ELYZA-japanese-Llama-2-7b-ggufmmnga/ELYZA-japanese-Llama-2-7b-instruct-ggufFast版 日本語の語彙を追加してトークンコストを減らし、1.8倍高速化したモデルmmnga/ELYZA-japanese-Llama-2-7b-fast-ggufmmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-ggufCodellama版 GGUFmmnga/ELYZA-japanese-CodeLlama-7b-ggufmmnga/ELYZA-japanese-CodeLlama-7b-instruct-ggufCodellama版 GPTQmmnga/ELYZA-japanese-CodeLlama-7b-instruc
 * 📥 2510 [rinna/bilingual-gpt-neox-4b-instruction-ppo](https://huggingface.co/rinna/bilingual-gpt-neox-4b-instruction-ppo) - bilingual-gpt-neox-4b-instruction-ppoOverviewThis repository provides an English-Japanese bilingual GPT-NeoX model of 3.8 billion parameters.
 * 📥 2394 [stabilityai/japanese-stablelm-instruct-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-beta-70b) - Japanese-StableLM-Instruct-Beta-70BA cute robot wearing a kimono writes calligraphy with one single brush — Stable Diffusion XLModel Descriptionjapanese-stablelm-instruct-beta-70b is a 70B-parameter decoder-only language model based on japanese-stablelm-base-beta-70b and further fine tuned on Databricks Dolly-15k, Anthropic HH, and other public data.
 * 📥 2312 [hotchpotch/japanese-reranker-cross-encoder-large-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-large-v1) - hotchpotch/japanese-reranker-cross-encoder-large-v1日本語で学習させた Reranker (CrossEncoder) シリーズです。モデル名layershidden_sizehotchpotch/japanese-reranker-cross-encoder-xsmall-v16384hotchpotch/japanese-reranker-cross-encoder-small-v112384hotchpotch/japanese-reranker-cross-encoder-base-v112768hotchpotch/japanese-reranker-cross-encoder-large-v1241024hotchpotch/japanese-bge-reranker-v2-m3-v1241024Reranker についてや、技術レポート・評価等は以下を参考ください。日本語最高性能のRerankerをリリース / そもそも Reranker とは?日本語 Reranker 作成のテクニカルレポート使い方SentenceTransformersfro
 * 📥 2283 [mmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf) - ELYZA-japanese-Llama-2-13b-fast-instruct-ggufELYZAさんが公開しているELYZA-japanese-Llama-2-13b-fast-instructのggufフォーマット変換版です。他のモデルはこちら通常版: llama2に日本語のデータセットで学習したモデルmmnga/ELYZA-japanese-Llama-2-7b-ggufmmnga/ELYZA-japanese-Llama-2-7b-instruct-ggufFast版 日本語の語彙を追加してトークンコストを減らし、1.8倍高速化したモデルmmnga/ELYZA-japanese-Llama-2-7b-fast-ggufmmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-ggufmmnga/ELYZA-japanese-Llama-2-13b-fast-ggufmmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-ggufCodellama版 GGUFmmnga/ELYZA-japanese-CodeLlama-7b-gg
 * 📥 2238 [SakanaAI/EvoLLM-JP-v1-7B](https://huggingface.co/SakanaAI/EvoLLM-JP-v1-7B) - 🐟 EvoLLM-JP-v1-7B🤗 Models | 📚 Paper | 📝 Blog | 🐦
 * 📥 2160 [rinna/japanese-gpt-neox-3.6b-instruction-ppo](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-ppo) - japanese-gpt-neox-3.6b-instruction-ppoOverviewThis repository provides a Japanese GPT-NeoX model of 3.6 billion parameters.
 * 📥 2154 [TKU410410103/hubert-base-japanese-asr](https://huggingface.co/TKU410410103/hubert-base-japanese-asr) - hubert-base-asrThis model is a fine-tuned version of rinna/japanese-hubert-base on the common_voice_11_0 dataset for ASR tasks.
 * 📥 2153 [MCZK/Assistance-7B-GGUF](https://huggingface.co/MCZK/Assistance-7B-GGUF) - Local-Novel-LLM-project様の Assistance をGGUF形式に変換したものです。
 * 📥 2104 [SakanaAI/EvoVLM-JP-v1-7B](https://huggingface.co/SakanaAI/EvoVLM-JP-v1-7B) - 🐟 EvoVLM-JP-v1-7B🤗 Models | 📚 Paper | 📝 Blog | 🐦
 * 📥 2070 [mmnga/aibuncho-japanese-novel-gpt-j-6b-gguf](https://huggingface.co/mmnga/aibuncho-japanese-novel-gpt-j-6b-gguf) - AIBunCho/japanese-novel-gpt-j-6bAI BunChoさんが公開しているjapanese-novel-gpt-j-6bのgguf変換版です。注意:こちらはブランチで試用になります。llama.cpp本家にgptneox, gpt2が実装された時に、このggufファイルが使用できない可能性があります。GitHubリポジトリの readme はこちらUsage (試用)git clone --branch
 * 📥 2031 [TFMC/Japanese-Starling-ChatV-7B-GGUF](https://huggingface.co/TFMC/Japanese-Starling-ChatV-7B-GGUF) - Japanese-Starling-ChatV-7B-GGUFGGUF conversion of "Japanese-Starling-ChatV-7B""Japanese-Starling-ChatV-7B" is a Japanese chat model built on top of "chatntq-ja-7b-v1.0", originally based on Mistral-7B-v0.1.I applied the chat vector acquired by subtracting the weights of Mistral-7B-v0.1 from the weights of "Starling-LM-7B-beta" to this model.
 * 📥 2019 [TKU410410103/wav2vec2-base-japanese-asr](https://huggingface.co/TKU410410103/wav2vec2-base-japanese-asr) - wav2vec2-base-asrThis model is a fine-tuned version of rinna/japanese-wav2vec2-base on the common_voice_11_0 dataset for ASR tasks.
 * 📥 2007 [TKU410410103/hubert-large-japanese-asr](https://huggingface.co/TKU410410103/hubert-large-japanese-asr) - hubert-large-asrThis model is a fine-tuned version of rinna/japanese-hubert-large ASR.
 * 📥 1905 [stabilityai/japanese-stablelm-base-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-base-beta-70b) - Japanese-StableLM-Base-Beta-70BA cute robot wearing a kimono writes calligraphy with one single brush — Stable Diffusion XLModel Descriptionjapanese-stablelm-base-beta-70b is a 70B-parameter decoder-only language model based on Llama-2-70b that has been fine-tuned on a diverse collection of Japanese data, with the intent of maximizing downstream performance on Japanese language tasks.
 * 📥 1813 [cyberagent/open-calm-1b](https://huggingface.co/cyberagent/open-calm-1b) - OpenCALM-1BModel DescriptionOpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by CyberAgent, Inc.
 * 📥 1780 [rinna/japanese-cloob-vit-b-16](https://huggingface.co/rinna/japanese-cloob-vit-b-16) - rinna/japanese-cloob-vit-b-16This is a Japanese CLOOB (Contrastive Leave One Out Boost) model trained by rinna Co., Ltd..Please see japanese-clip for the other available models.
 * 📥 1753 [stockmark/stockmark-13b](https://huggingface.co/stockmark/stockmark-13b) - stockmark/stockmark-13bStockmark-13b is a 13 billion parameter LLM pretrained from scratch based on Japanese corpus of about 220B tokens.
 * 📥 1749 [jarvisx17/japanese-sentiment-analysis](https://huggingface.co/jarvisx17/japanese-sentiment-analysis) - japanese-sentiment-analysisThis model was trained from scratch on the chABSA dataset.
 * 📥 1696 [Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime) - このモデルはLuke-japanese-large-liteをファインチューニングしたものです。このモデルは８つの感情（喜び、悲しみ、期待、驚き、怒り、恐れ、嫌悪、信頼）の内、どの感情が文章に含まれているのか分析することができます。このモデルはwrimeデータセット（https://huggingface.co/datasets/shunk031/wrime）を用いて学習を行いました。This model is based on Luke-japanese-large-liteThis model is fine-tuned model which besed on studio-ousia/Luke-japanese-large-lite.
 * 📥 1689 [OrionStarAI/Orion-14B-Base](https://huggingface.co/OrionStarAI/Orion-14B-Base) - Orion-14B🌐English | 🇨🇳中文 | 🇯🇵日本語 |🇰🇷한국어🤗 HuggingFace Mainpage | 🤖 ModelScope Mainpage🎬 HuggingFace Demo | 🎫 ModelScope Demo😺 GitHub📖 Tech ReportTable of Contents📖 Model Introduction🔗 Model Download🔖 Model Benchmark📊 Model Inference📜 Declarations &amp; License🥇 Company Introduction1.
 * 📥 1591 [mmnga/stockmark-gpt-neox-japanese-1.4b-gguf](https://huggingface.co/mmnga/stockmark-gpt-neox-japanese-1.4b-gguf) - stockmark-gpt-neox-japanese-1.4b-ggufstockmarkさんが公開しているgpt-neox-japanese-1.4bのggufフォーマット変換版です。注意:こちらはブランチで試用になります。llama.cpp本家にgptneoxが実装された時に、このggufファイルが使用できない可能性があります。GitHubリポジトリの readme はこちらUsage (試用)git clone --branch
 * 📥 1573 [stabilityai/japanese-stablelm-instruct-alpha-7b-v2](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b-v2) - Japanese-StableLM-Instruct-Alpha-7B-v2"A parrot able to speak Japanese, ukiyoe, edo period" — Stable Diffusion XLModel Descriptionjapanese-stablelm-instruct-alpha-7b-v2 is a 7B parameter decoder-only language models pre-trained built on top of the Japanese-StableLM-Base-Alpha-7B model and further fine-tuned on various instruction-following datasets.
 * 📥 1551 [stabilityai/japanese-stablelm-3b-4e1t-instruct](https://huggingface.co/stabilityai/japanese-stablelm-3b-4e1t-instruct) - Japanese StableLM-3B-4E1T InstructModel DescriptionThis is a 3B-parameter decoder-only Japanese language model fine-tuned on instruction-following datasets, built on top of the base model Japanese StableLM-3B-4E1T Base.
 * 📥 1549 [Lasorco/lametta_old](https://huggingface.co/Lasorco/lametta_old) - old？
 * 📥 1542 [cyberagent/open-calm-small](https://huggingface.co/cyberagent/open-calm-small) - OpenCALM-SmallModel DescriptionOpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by
 * 📥 1527 [rinna/japanese-gpt-1b](https://huggingface.co/rinna/japanese-gpt-1b) - japanese-gpt-1bThis repository provides a 1.3B-parameter Japanese GPT model.
 * 📥 1453 [jurabi/bert-ner-japanese](https://huggingface.co/jurabi/bert-ner-japanese) - BERTによる日本語固有表現抽出のモデルBertForTokenClassificationを用いて、日本語の文から固有表現を抽出します。抽出される固有表現のタイプは、以下の8種類です。人名法人名（法人または法人に類する組織）政治的組織名（政治的組織名、政党名、政府組織名、行政組織名、軍隊名、国際組織名）その他の組織名	（競技組織名、公演組織名、その他）地名施設名製品名（商品名、番組名、映画名、書籍名、歌名、ブランド名等）イベント名使用方法必要なライブラリ（transformers、unidic_lite、fugashi）をpipなどでインストールして、下記のコードを実行するだけです。from transformers import BertJapaneseTokenizer, BertForTokenClassificationfrom transformers import pipelinemodel = BertForTokenClassification.from_pretrained("jurabi/bert-ner-japanese")tokenizer = BertJapaneseTokeniz
 * 📥 1427 [tohoku-nlp/bert-base-japanese-char-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v3) - BERT base Japanese (character-level tokenization with whole word masking, CC-100 and jawiki-20230102)This is a BERT model pretrained on texts in the Japanese language.
 * 📥 1400 [dahara1/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ](https://huggingface.co/dahara1/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ) - Model Card for Model IDOriginal model elyza/ELYZA-japanese-Llama-2-7b-fast-instruct which is based on Meta's "Llama 2" and has undergone additional pre-training in Japanese, and thier original post-training and speed up tuning.
 * 📥 1332 [christian-phu/bert-finetuned-japanese-sentiment](https://huggingface.co/christian-phu/bert-finetuned-japanese-sentiment) - bert-finetuned-japanese-sentimentThis model is a fine-tuned version of cl-tohoku/bert-base-japanese-v2 on product amazon reviews japanese dataset.
 * 📥 1332 [line-corporation/japanese-large-lm-3.6b-instruction-sft](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft) - japanese-large-lm-3.6b-instruction-sftThis repository provides a 3.6B parameters Japanese language model, fine-tuned and trained by LINE Corporation.
 * 📥 1331 [karakuri-ai/karakuri-lm-70b-v0.1](https://huggingface.co/karakuri-ai/karakuri-lm-70b-v0.1) - KARAKURI LMKARAKURI LM is a pretrained language model that builds upon Llama 2.Our model enhances Llama 2's capabilities by incorporating additional Japanese vocabulary and further pretraining on a mixture of Japanese and multilingual corpora.
 * 📥 1259 [KBlueLeaf/guanaco-7b-leh-v2](https://huggingface.co/KBlueLeaf/guanaco-7b-leh-v2) - Guanaco-leh-V2: A Multilingual Instruction-Following Language Model Based on LLaMA
 * 📥 1214 [retrieva-jp/t5-small-short](https://huggingface.co/retrieva-jp/t5-small-short) - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus.
 * 📥 1175 [nlp-waseda/roberta-large-japanese-seq512](https://huggingface.co/nlp-waseda/roberta-large-japanese-seq512) - nlp-waseda/roberta-large-japanese-seq512Model descriptionThis is a Japanese RoBERTa large model pretrained on Japanese Wikipedia and the Japanese portion of CC-100 with the maximum sequence length of 512.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained("nlp-waseda/roberta-large-japanese-seq512")model = AutoModelForMaskedLM.from_pretrained("nlp-waseda/roberta-large-japanese-seq512")se
 * 📥 1165 [llm-book/bert-base-japanese-v3-marc_ja](https://huggingface.co/llm-book/bert-base-japanese-v3-marc_ja) - bert-base-japanese-v3-marc_ja「大規模言語モデル入門」の第5章で紹介している(感情分析)のモデルです。cl-tohoku/bert-base-japanese-v3をJGLUEのMARC-jaデータセットでファインチューニングして構築されています。関連リンクGitHubリポジトリColabノートブック（訓練）Colabノートブック（推論）データセット大規模言語モデル入門（Amazon.co.jp）大規模言語モデル入門（gihyo.jp）使い方from transformers import pipelinetext_classification_pipeline
 * 📥 1142 [stabilityai/japanese-stablelm-instruct-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-beta-7b) - Japanese-StableLM-Instruct-Beta-7BA cute robot wearing a kimono writes calligraphy with one single brush — Stable Diffusion XLModel Descriptionjapanese-stablelm-instruct-beta-7b is a 7B-parameter decoder-only language model based on
 * 📥 1142 [OrionStarAI/Orion-14B-Chat-RAG](https://huggingface.co/OrionStarAI/Orion-14B-Chat-RAG) - Orion-14B🌐English | 🇨🇳中文 | 🇯🇵日本語 | 🇰🇷한국어🤗 HuggingFace Mainpage | 🤖 ModelScope Mainpage🎬 HuggingFace Demo | 🎫 ModelScope Demo😺 GitHub📖 Tech ReportTable of Contents📖 Model Introduction🔗 Model Download🔖 Model Benchmark📊 Model Inference📜 Declarations &amp; License🥇 Company Introduction1.
 * 📥 1127 [tokyotech-llm/Swallow-13b-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
 * 📥 1115 [umiyuki/Japanese-WizardLM2-ChatV-7B-GGUF](https://huggingface.co/umiyuki/Japanese-WizardLM2-ChatV-7B-GGUF) - Japanese-WizardLM2-ChatV-7B-GGUFGGUF conversion of "Japanese-WizardLM2-ChatV-7B"This model, Japanese-WizardLM2-ChatV-7B, is based on "chatntq-ja-7b-v1.0 ", and was created by subtracting "Mistral-7B-v0.1" from "WizardLM-2-7b" ChatVector was added by a factor of 1.0.We aimed to add the high performance of WizardLM-2 to the Japanese language capability of ChatNTQ.このモデル、Japanese-WizardLM2-ChatV-7Bは、”chatntq-ja-7b-v1.0”をベースに、"WizardLM-2-7b"から"Mistral-7B-v0.1"を差し引いて作ったChatVectorを1.0倍で足しました。ChatNTQの日本語能力にWizardLM
 * 📥 1095 [tokyotech-llm/Swallow-MX-8x7b-NVE-v0.1](https://huggingface.co/tokyotech-llm/Swallow-MX-8x7b-NVE-v0.1) - Swallow-MX-8x7b-NVE-v0.1Our Swallow-MX-8x7b-NVE-v0.1 model has undergone continuous pre-training from the Mixtral-8x7B-Instruct-v0.1, primarily with the addition of Japanese language data.
 * 📥 1049 [rinna/japanese-gpt-neox-3.6b-instruction-sft](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft) - japanese-gpt-neox-3.6b-instruction-sftOverviewThis repository provides a Japanese GPT-NeoX model of 3.6 billion parameters.
 * 📥 1039 [stabilityai/japanese-stablelm-base-alpha-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-alpha-7b) - Japanese-StableLM-Base-Alpha-7B"A parrot able to speak Japanese, ukiyoe, edo period" — Stable Diffusion XLModel Descriptionjapanese-stablelm-base-alpha-7b is a 7B-parameter decoder-only language model pre-trained on a diverse collection of Japanese and English datasets which focus on maximizing Japanese language modeling performance and Japanese downstream task performance.
 * 📥 1037 [rinna/japanese-gpt-neox-small](https://huggingface.co/rinna/japanese-gpt-neox-small) - japanese-gpt-neox-smallThis repository provides a small-sized Japanese GPT-NeoX model.
 * 📥 1029 [haqishen/Llama-3-8B-Japanese-Instruct](https://huggingface.co/haqishen/Llama-3-8B-Japanese-Instruct) - IntroductionWho am I: Qishen Ha
 * 📥 1021 [izumi-lab/bert-base-japanese-fin-additional](https://huggingface.co/izumi-lab/bert-base-japanese-fin-additional) - Additional pretrained BERT base Japanese financeThis is a BERT model pretrained on texts in the Japanese language.
 * 📥 1020 [abeja/gpt2-large-japanese](https://huggingface.co/abeja/gpt2-large-japanese) - gpt2-large-japaneseThis repository provides a large sized Japanese GPT-2 model.
 * 📥 1012 [ken11/albert-base-japanese-v1](https://huggingface.co/ken11/albert-base-japanese-v1) - albert-base-japanese-v1日本語事前学習済みALBERTモデルですHow to useファインチューニングこのモデルはPreTrainedモデルです基本的には各種タスク用にファインチューニングして使用されることを想定していますFill-MaskこのモデルではTokenizerにSentencepieceを利用していますそのままでは[MASK]トークンのあとに余計なトークンが混入する問題があるので、利用する際には以下のようにする必要がありますfor PyTorchfrom transformers import (AlbertForMaskedLM, AlbertTokenizerFast)import torchtokenizer = AlbertTokenizerFast.from_pretrained("ken11/albert-base-japanese-v1")
 * 📥 1006 [mmnga/ELYZA-japanese-Llama-2-7b-fast-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-gguf) - ELYZA-japanese-Llama-2-7b-fast-ggufELYZAさんが公開しているELYZA-japanese-Llama-2-7b-fastのggufフォーマット変換版です。他のモデルはこちら通常版: llama2に日本語のデータセットで学習したモデルmmnga/ELYZA-japanese-Llama-2-7b-ggufmmnga/ELYZA-japanese-Llama-2-7b-instruct-ggufFast版 日本語の語彙を追加してトークンコストを減らし、1.8倍高速化したモデルmmnga/ELYZA-japanese-Llama-2-7b-fast-ggufmmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-ggufCodellama版 GGUFmmnga/ELYZA-japanese-CodeLlama-7b-ggufmmnga/ELYZA-japanese-CodeLlama-7b-instruct-ggufCodellama版 GPTQmmnga/ELYZA-japanese-CodeLlama-7b-instruct-GPTQ-c
 * 📥 1004 [pfnet/plamo-13b-instruct](https://huggingface.co/pfnet/plamo-13b-instruct) - PLaMo-13B-InstructModel DescriptionPLaMo-13B-Instruct is an instruct fine-tuned model built upon the 8192 context length version of PLaMo-13B text generation model.
 * 📥 1003 [nlp-waseda/bigbird-base-japanese](https://huggingface.co/nlp-waseda/bigbird-base-japanese) - nlp-waseda/bigbird-base-japaneseModel descriptionThis is a Japanese BigBird base model pretrained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained("nlp-waseda/bigbird-base-japanese")model = AutoModelForMaskedLM.from_pretrained("nlp-waseda/bigbird-base-japanese")sentence = '[MASK] 大学 で 自然 言語 処理 を
 * 📥 992 [ThePioneer/CoolerWaifuDiffusion](https://huggingface.co/ThePioneer/CoolerWaifuDiffusion) - モデル説明 (model explanation)CoolJapanDiffusion 2.1.1とWaifuDiffusion 1.4 anime epoch2のマージ。
 * 📥 972 [mmnga/karakuri-lm-70b-chat-v0.1-gguf](https://huggingface.co/mmnga/karakuri-lm-70b-chat-v0.1-gguf) - karakuri-lm-70b-chat-v0.1-ggufkarakuri-aiさんが公開しているkarakuri-lm-70b-chat-v0.1のggufフォーマット変換版です。
 * 📥 941 [stockmark/gpt-neox-japanese-1.4b](https://huggingface.co/stockmark/gpt-neox-japanese-1.4b) - stockmark/gpt-neox-japanese-1.4bThis repository provides a GPT-NeoX based model with 1.4B parameters pre-trained on Japanese corpus of about 20B tokens.
 * 📥 938 [TFMC/Japanese-Starling-ChatV-7B](https://huggingface.co/TFMC/Japanese-Starling-ChatV-7B) - Japanese-Starling-ChatV-7Bこのモデルは"chatntq-ja-7b-v1.0"をベースにした7Bパラメータの日本語チャットモデルです。"Japanese-Starling-ChatV-7B" is a Japanese chat model built on top of "chatntq-ja-7b-v1.0", originally based on Mistral-7B-v0.1.詳細とGGUF版はこちら。Details and GGUFs are here.
 * 📥 934 [elyza/ELYZA-japanese-CodeLlama-7b-instruct](https://huggingface.co/elyza/ELYZA-japanese-CodeLlama-7b-instruct) - ELYZA-japanese-CodeLlama-7bModel DescriptionELYZA-japanese-CodeLlama-7b は、 Code Llamaをベースとして日本語能力を拡張するために追加事前学習を行ったモデルです。詳細は
 * 📥 924 [line-corporation/japanese-large-lm-3.6b](https://huggingface.co/line-corporation/japanese-large-lm-3.6b) - japanese-large-lm-3.6bThis repository provides a 3.6B parameters Japanese language model, trained by LINE Corporation.
 * 📥 922 [sazyou-roukaku/LittleStepMix](https://huggingface.co/sazyou-roukaku/LittleStepMix) - License:CreativeML Open RAIL-MAdditional Copyright: sazyou_roukaku (TwitterID @sazyou_roukaku) as of June 25, 2023このモデルは『CreativeML Open RAIL-M』でLicenseそのものに変更はありません。
 * 📥 921 [NTQAI/chatntq-ja-7b-v1.0](https://huggingface.co/NTQAI/chatntq-ja-7b-v1.0) - ChatNTQ JA 7B V1.0Model
 * 📥 908 [tokyotech-llm/Swallow-7b-plus-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-plus-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
 * 📥 904 [TKU410410103/uniTKU-hubert-japanese-asr](https://huggingface.co/TKU410410103/uniTKU-hubert-japanese-asr) - uniTKU-hubert-japanese-asrThis model was fine-tuned on a dataset provided by uniTKU, and it has maintained the original performance metrics on the common_voice_11_0 dataset.
 * 📥 899 [studio-ousia/luke-japanese-large](https://huggingface.co/studio-ousia/luke-japanese-large) - luke-japanese-largeluke-japanese is the Japanese version of LUKE (LanguageUnderstanding with Knowledge-based Embeddings), a pre-trainedknowledge-enhanced contextualized representation of words and entities.
 * 📥 878 [TheBloke/japanese-stablelm-instruct-gamma-7B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-GGUF) - Chat &amp; support: TheBloke's Discord serverWant to contribute?
 * 📥 856 [rinna/nekomata-14b](https://huggingface.co/rinna/nekomata-14b) - rinna/nekomata-14bOverviewWe conduct continual pre-training of qwen-14b on 66B tokens from a mixture of Japanese and English datasets.
 * 📥 851 [rinna/llama-3-youko-8b](https://huggingface.co/rinna/llama-3-youko-8b) - Llama 3 Youko 8B (rinna/llama-3-youko-8b)OverviewWe conduct continual pre-training of meta-llama/Meta-Llama-3-8B on 22B tokens from a mixture of Japanese and English datasets.
 * 📥 835 [rinna/nekomata-7b](https://huggingface.co/rinna/nekomata-7b) - rinna/nekomata-7bOverviewWe conduct continual pre-training of qwen-7b on 30B tokens from a mixture of Japanese and English datasets.
 * 📥 822 [mmnga/ELYZA-japanese-CodeLlama-7b-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-CodeLlama-7b-instruct-gguf) - ELYZA-japanese-CodeLlama-7b-instruct-ggufELYZAさんが公開しているELYZA-japanese-CodeLlama-7b-instructのggufフォーマット変換版です。他のモデルはこちら通常版: llama2に日本語のデータセットで学習したモデルmmnga/ELYZA-japanese-Llama-2-7b-ggufmmnga/ELYZA-japanese-Llama-2-7b-instruct-ggufFast版 日本語の語彙を追加してトークンコストを減らし、1.8倍高速化したモデルmmnga/ELYZA-japanese-Llama-2-7b-fast-ggufmmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-ggufCodellama版 GGUFmmnga/ELYZA-japanese-CodeLlama-7b-ggufmmnga/ELYZA-japanese-CodeLlama-7b-instruct-ggufCodellama版 GPTQmmnga/ELYZA-japanese-CodeLlama-7b-ins
 * 📥 811 [KoichiYasuoka/roberta-small-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-luw-upos) - roberta-small-japanese-luw-uposModel
 * 📥 805 [Aratako/c4ai-command-r-v01-japanese-instruct-GGUF](https://huggingface.co/Aratako/c4ai-command-r-v01-japanese-instruct-GGUF) - c4ai-command-r-v01-japanese-instruct-GGUF概要Aratako/c4ai-command-r-v01-japanese-instructの量子化済みGGUF版です。ライセンス等詳細は元モデルをご確認ください。
 * 📥 795 [pfnet/plamo-13b-instruct-nc](https://huggingface.co/pfnet/plamo-13b-instruct-nc) - PLaMo-13B-Instruct-NCModel DescriptionPLaMo-13B-Instruct-NC is a noncommercial instruct fine-tuned model built upon the 8192 context length version of PLaMo-13B text generation model.
 * 📥 783 [stabilityai/japanese-stablelm-3b-4e1t-base](https://huggingface.co/stabilityai/japanese-stablelm-3b-4e1t-base) - Japanese StableLM-3B-4E1T BaseModel DescriptionThis is a 3B-parameter decoder-only language model with a focus on maximizing Japanese language modeling performance and Japanese downstream task performance.
 * 📥 767 [stabilityai/japanese-stablelm-base-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-beta-7b) - Japanese-StableLM-Base-Beta-7BA cute robot wearing a kimono writes calligraphy with one single brush — Stable Diffusion XLModel Descriptionjapanese-stablelm-base-beta-7b is a 7B-parameter decoder-only language model based on Llama-2-7b that has been fine-tuned on a diverse collection of Japanese data, with the intent of maximizing downstream performance on Japanese language tasks.
 * 📥 764 [tokyotech-llm/Swallow-7b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-7b-instruct-v0.1) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
 * 📥 751 [llm-book/bert-base-japanese-v3-jnli](https://huggingface.co/llm-book/bert-base-japanese-v3-jnli) - bert-base-japanese-v3-jnli「大規模言語モデル入門」の第5章で紹介している(自然言語推論)のモデルです。cl-tohoku/bert-base-japanese-v3をJGLUEのMARC-jaデータセットでファインチューニングして構築されています。関連リンクGitHubリポジトリColabノートブック（訓練）Colabノートブック（推論）データセット大規模言語モデル入門（Amazon.co.jp）大規模言語モデル入門（gihyo.jp）使い方from transformers import pipelinenli_pipeline = pipeline(model="llm-book/bert-base-japanese-v3-jnli")text = "二人の男性がジェット機を見ています"entailment_text = "ジェット機を見ている人が二人います"# textとentailment_textの論理関係を予測print(nli_pipeline({"text": text, "text_pair": entailment_text}))# {'label': 'enta
 * 📥 721 [mmnga/c4ai-command-r-plus-gguf](https://huggingface.co/mmnga/c4ai-command-r-plus-gguf) - c4ai-command-r-plus-ggufCohereForAIさんが公開しているc4ai-command-r-plusのggufフォーマット変換版です。imatrixのデータはTFMC/imatrix-dataset-for-japanese-llmを使用して作成しました。分割されたファイルについてq6_kやq8_0のファイルはサイズが大きく分割されているので結合する必要があります。cat c4ai-command-r-plus-Q5_K_M.gguf.* &gt; c4ai-command-r-plus-Q5_K_M.ggufUsagegit clone https://github.com/ggerganov/llama.cpp.gitcd llama.cppmake -j./main
 * 📥 703 [studio-ousia/luke-japanese-base-lite](https://huggingface.co/studio-ousia/luke-japanese-base-lite) - luke-japaneseluke-japanese is the Japanese version of LUKE (LanguageUnderstanding with Knowledge-based Embeddings), a pre-trainedknowledge-enhanced contextualized representation of words and entities.
 * 📥 687 [line-corporation/japanese-large-lm-1.7b-instruction-sft](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft) - japanese-large-lm-1.7b-instruction-sftThis repository provides a 1.7B parameters Japanese language model, fine-tuned and trained by LINE Corporation.
 * 📥 659 [llm-book/bert-base-japanese-v3-jsts](https://huggingface.co/llm-book/bert-base-japanese-v3-jsts) - bert-base-japanese-v3-jsts「大規模言語モデル入門」の第5章で紹介している(意味類似度計算)のモデルです。cl-tohoku/bert-base-japanese-v3をJGLUEのJSTSデータセットでファインチューニングして構築されています。関連リンクGitHubリポジトリColabノートブック（訓練）Colabノートブック（推論）データセット大規模言語モデル入門（Amazon.co.jp）大規模言語モデル入門（gihyo.jp）使い方from transformers import pipelinetext_sim_pipeline = pipeline(model="llm-book/bert-base-japanese-v3-jsts",function_to_apply="none",)text = "川べりでサーフボードを持った人たちがいます"sim_text = "サーファーたちが川べりに立っています"# textとsim_textの類似度を計算result = text_sim_pipeline({"text": text, "text_pair": sim_text
 * 📥 658 [retrieva-jp/t5-base-long](https://huggingface.co/retrieva-jp/t5-base-long) - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus.
 * 📥 654 [Tanrei/GPTSAN-japanese](https://huggingface.co/Tanrei/GPTSAN-japanese) - Model Card for Tanrei/GPTSAN-japaneseGeneral-purpose Swich transformer based Japanese language modelGPTSAN has some unique features.
 * 📥 652 [nlp-waseda/roberta-base-japanese](https://huggingface.co/nlp-waseda/roberta-base-japanese) - nlp-waseda/roberta-base-japaneseModel descriptionThis is a Japanese RoBERTa base model pretrained on Japanese Wikipedia and the Japanese portion of CC-100.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained("nlp-waseda/roberta-base-japanese")model = AutoModelForMaskedLM.from_pretrained("nlp-waseda/roberta-base-japanese")sentence = '早稲田 大学 で 自然 言語 処理 を
 * 📥 643 [TheBloke/japanese-stablelm-instruct-beta-70B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-70B-GGUF) - Chat &amp; support: TheBloke's Discord serverWant to contribute?
 * 📥 637 [line-corporation/japanese-large-lm-1.7b](https://huggingface.co/line-corporation/japanese-large-lm-1.7b) - japanese-large-lm-1.7bThis repository provides a 1.7B parameters Japanese language model, trained by LINE Corporation.
 * 📥 611 [aken12/splade-japanese-v3](https://huggingface.co/aken12/splade-japanese-v3) - Evaluation on MIRACL japaneseThese models don't train on the MIRACL training data.
 * 📥 605 [mmnga/ELYZA-japanese-Llama-2-13b-fast-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-13b-fast-gguf) - ELYZA-japanese-Llama-2-13b-fast-ggufELYZAさんが公開しているELYZA-japanese-Llama-2-13b-fastのggufフォーマット変換版です。他のモデルはこちら通常版: llama2に日本語のデータセットで学習したモデルmmnga/ELYZA-japanese-Llama-2-7b-ggufmmnga/ELYZA-japanese-Llama-2-7b-instruct-ggufFast版 日本語の語彙を追加してトークンコストを減らし、1.8倍高速化したモデルmmnga/ELYZA-japanese-Llama-2-7b-fast-ggufmmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-ggufmmnga/ELYZA-japanese-Llama-2-13b-fast-ggufmmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-ggufCodellama版 GGUFmmnga/ELYZA-japanese-CodeLlama-7b-ggufmmnga/ELYZA-japa
 * 📥 588 [rinna/nekomata-14b-instruction-gguf](https://huggingface.co/rinna/nekomata-14b-instruction-gguf) - rinna/nekomata-14b-instruction-ggufOverviewThe model is the GGUF version of rinna/nekomata-14b-instruction.
 * 📥 585 [mmnga/ELYZA-japanese-Llama-2-7b-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-gguf) - ELYZA-japanese-Llama-2-7b-ggufELYZAさんが公開しているELYZA-japanese-Llama-2-7bのggufフォーマット変換版です。他のモデルはこちら通常版: llama2に日本語のデータセットで学習したモデルmmnga/ELYZA-japanese-Llama-2-7b-ggufmmnga/ELYZA-japanese-Llama-2-7b-instruct-ggufFast版 日本語の語彙を追加してトークンコストを減らし、1.8倍高速化したモデルmmnga/ELYZA-japanese-Llama-2-7b-fast-ggufmmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-ggufCodellama版 GGUFmmnga/ELYZA-japanese-CodeLlama-7b-ggufmmnga/ELYZA-japanese-CodeLlama-7b-instruct-ggufCodellama版 GPTQmmnga/ELYZA-japanese-CodeLlama-7b-instruct-GPTQ-calib-ja-1k
 * 📥 582 [ku-nlp/deberta-v2-base-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-base-japanese-char-wwm) - Model Card for Japanese character-level DeBERTa V2 baseModel
 * 📥 579 [TareHimself/manga-ocr-base](https://huggingface.co/TareHimself/manga-ocr-base) - Original ModelOptical character recognition for Japanese text, with the main focus being Japanese manga.
 * 📥 543 [mmnga/rinna-japanese-gpt-neox-3.6b-instruction-ppo-gguf](https://huggingface.co/mmnga/rinna-japanese-gpt-neox-3.6b-instruction-ppo-gguf) - rinna/japanese-gpt-neox-3.6b-instruction-pporinnaさんが公開しているjapanese-gpt-neox-3.6b-instruction-ppoのgguf変換版です。他モデルはこちらmmnga/rinna-bilingual-gpt-neox-4b-ggufmmnga/rinna-bilingual-gpt-neox-4b-8k-ggufmmnga/rinna-bilingual-gpt-neox-4b-instruction-ppo-ggufmmnga/rinna-japanese-gpt-neox-3.6b-ggufmmnga/rinna-japanese-gpt-neox-3.6b-instruction-ppo-gguf注意:こちらはブランチで試用になります。llama.cpp本家にgptneoxが実装された時に、このggufファイルが使用できない可能性があります。GitHubリポジトリの readme はこちらUsage (試用)git clone --branch
 * 📥 542 [mmnga/rinna-japanese-gpt-neox-3.6b-gguf](https://huggingface.co/mmnga/rinna-japanese-gpt-neox-3.6b-gguf) - rinna/japanese-gpt-neox-3.6brinnaさんが公開しているjapanese-gpt-neox-3.6bのgguf変換版です。他モデルはこちらmmnga/rinna-bilingual-gpt-neox-4b-ggufmmnga/rinna-bilingual-gpt-neox-4b-8k-ggufmmnga/rinna-bilingual-gpt-neox-4b-instruction-ppo-ggufmmnga/rinna-japanese-gpt-neox-3.6b-ggufmmnga/rinna-japanese-gpt-neox-3.6b-instruction-ppo-gguf注意:こちらはブランチで試用になります。llama.cpp本家にgptneoxが実装された時に、このggufファイルが使用できない可能性があります。GitHubリポジトリの readme はこちらUsage (試用)git clone --branch
 * 📥 536 [nlp-waseda/comet-t5-base-japanese](https://huggingface.co/nlp-waseda/comet-t5-base-japanese) - COMET-T5 jaFinetuned T5 on ATOMIC ja using a text-to-text language modeling objective.
 * 📥 536 [mmnga/gemma-1.1-7b-it-gguf](https://huggingface.co/mmnga/gemma-1.1-7b-it-gguf) - gemma-1.1-7b-it-ggufgoogleさんが公開しているgemma-1.1-7b-itのggufフォーマット変換版です。Licencegemma-terms-of-use 利用規約をご利用前に必ずご確認ください。他のモデルmmnga/codegemma-1.1-7b-it-ggufmmnga/codegemma-1.1-2b-ggufmmnga/gemma-2b-it-ggufmmnga/gemma-7b-it-ggufmmnga/gemma-1.1-7b-it-ggufmmnga/codegemma-7b-it-ggufUsagegit clone https://github.com/ggerganov/llama.cpp.gitcd llama.cppmake -j./main
 * 📥 525 [hotchpotch/japanese-reranker-cross-encoder-small-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-small-v1) - hotchpotch/japanese-reranker-cross-encoder-small-v1日本語で学習させた Reranker (CrossEncoder) シリーズです。モデル名layershidden_sizehotchpotch/japanese-reranker-cross-encoder-xsmall-v16384hotchpotch/japanese-reranker-cross-encoder-small-v112384hotchpotch/japanese-reranker-cross-encoder-base-v112768hotchpotch/japanese-reranker-cross-encoder-large-v1241024hotchpotch/japanese-bge-reranker-v2-m3-v1241024Reranker についてや、技術レポート・評価等は以下を参考ください。日本語最高性能のRerankerをリリース / そもそも Reranker とは?日本語 Reranker 作成のテクニカルレポート使い方SentenceTransformersfro
 * 📥 505 [tohoku-nlp/bert-large-japanese-char-v2](https://huggingface.co/tohoku-nlp/bert-large-japanese-char-v2) - BERT large Japanese (character-level tokenization with whole word masking, CC-100 and jawiki-20230102)This is a BERT model pretrained on texts in the Japanese language.
 * 📥 481 [tokyotech-llm/Swallow-7b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-NVE-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
 * 📥 479 [rinna/japanese-gpt-neox-3.6b-instruction-sft-v2](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft-v2) - japanese-gpt-neox-3.6b-instruction-sft-v2OverviewThis repository provides a Japanese GPT-NeoX model of 3.6 billion parameters.
 * 📥 475 [rinna/japanese-wav2vec2-base](https://huggingface.co/rinna/japanese-wav2vec2-base) - rinna/japanese-wav2vec2-baseOverviewThis is a Japanese wav2vec 2.0 Base model trained by rinna Co., Ltd.Model summaryThe model architecture is the same as the original wav2vec 2.0 Base model, which contains 12 transformer layers with 12 attention heads.
 * 📥 457 [second-state/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF](https://huggingface.co/second-state/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF) - ELYZA-japanese-Llama-2-13b-fast-instruct-GGUFOriginal Modelelyza/ELYZA-japanese-Llama-2-13b-fast-instructRun with LlamaEdgeLlamaEdge version: v0.2.8 and abovePrompt templatePrompt type: llama-2-chatPrompt string&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;{{ system_prompt }}&lt;&lt;/SYS&gt;&gt;{{ user_msg_1 }}
 * 📥 456 [retrieva-jp/t5-large-long](https://huggingface.co/retrieva-jp/t5-large-long) - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus.
 * 📥 450 [kotoba-tech/kotoba-whisper-v1.1](https://huggingface.co/kotoba-tech/kotoba-whisper-v1.1) - Kotoba-Whisper-v1.1Kotoba-Whisper-v1.1 is a Japanese ASR model based on kotoba-tech/kotoba-whisper-v1.0, withadditional postprocessing stacks integrated as pipeline.
 * 📥 438 [tsmatz/mt5_summarize_japanese](https://huggingface.co/tsmatz/mt5_summarize_japanese) - mt5_summarize_japanese(Japanese caption : 日本語の要約のモデル)This model is a fine-tuned version of google/mt5-small trained for Japanese summarization.
 * 📥 431 [retrieva-jp/t5-small-long](https://huggingface.co/retrieva-jp/t5-small-long) - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus.
 * 📥 431 [Local-Novel-LLM-project/Vecteus-v1](https://huggingface.co/Local-Novel-LLM-project/Vecteus-v1) - Our ModelsVecteusNinja-v1Ninja-v1-NSFWNinja-v1-128kNinja-v1-NSFW-128kModel Card for VecTeus-v1.0The Mistral-7B--based Large Language Model (LLM) is an noveldataset fine-tuned version of the Mistral-7B-v0.1VecTeus has the following changes compared to Mistral-7B-v0.1.128k context window (8k context in v0.1)Achieving both high quality Japanese and English generationCan be generated NSFWMemory ability that does not forget even after long-context generationThis model was created with the help of GPUs from the f
 * 📥 426 [llm-book/bert-base-japanese-v3-unsup-simcse-jawiki](https://huggingface.co/llm-book/bert-base-japanese-v3-unsup-simcse-jawiki) - bert-base-japanese-v3-unsup-simcse-jawiki「大規模言語モデル入門」の第8章で紹介している教師なしSimCSEのモデルです。cl-tohoku/bert-base-japanese-v3 を
 * 📥 424 [megagonlabs/t5-base-japanese-web](https://huggingface.co/megagonlabs/t5-base-japanese-web) - t5-base-japanese-web (with Byte-fallback, 32K)Descriptionmegagonlabs/t5-base-japanese-web is a T5 (Text-to-Text Transfer Transformer) model pre-trained on Japanese web texts.
 * 📥 415 [ku-nlp/deberta-v3-base-japanese](https://huggingface.co/ku-nlp/deberta-v3-base-japanese) - Model Card for Japanese DeBERTa V3 baseModel
 * 📥 414 [maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-instruct-gguf](https://huggingface.co/maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-instruct-gguf) - I'm constantly enhancing these model descriptions to provide you with the most relevant and comprehensive informationjapanese-stablelm-3b-4e1t-instruct - GGUFModel creator: stabilityaiOriginal model: japanese-stablelm-3b-4e1t-instructStableLMThis is a Model based on StableLM.Stablelm is a familiy of Language Models by Stability AI.Note:Current (as of 2023-11-15) implementations of Llama.cpp only support GPU offloading up to 34 Layers with these StableLM Models.
 * 📥 414 [TheBloke/japanese-stablelm-base-beta-70B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-base-beta-70B-GGUF) - Chat &amp; support: TheBloke's Discord serverWant to contribute?
 * 📥 413 [alfredplpl/Llama-3-8B-Instruct-Ja](https://huggingface.co/alfredplpl/Llama-3-8B-Instruct-Ja) - 日本語向け Llama 3 8BはじめにこのリポジトリはLlama 3を日本語化しようとしたモデルのリポジトリです。
 * 📥 408 [stabilityai/japanese-stable-diffusion-xl](https://huggingface.co/stabilityai/japanese-stable-diffusion-xl) - By downloading, using, or distributing any portion or element of this model, you agree to be bound by the agreement described in the LICENSE file.
 * 📥 403 [mmnga/ELYZA-japanese-CodeLlama-7b-gguf](https://huggingface.co/mmnga/ELYZA-japanese-CodeLlama-7b-gguf) - ELYZA-japanese-CodeLlama-7b-ggufELYZAさんが公開しているELYZA-japanese-CodeLlama-7b-instructのggufフォーマット変換版です。他のモデルはこちら通常版: llama2に日本語のデータセットで学習したモデルmmnga/ELYZA-japanese-Llama-2-7b-ggufmmnga/ELYZA-japanese-Llama-2-7b-instruct-ggufFast版 日本語の語彙を追加してトークンコストを減らし、1.8倍高速化したモデルmmnga/ELYZA-japanese-Llama-2-7b-fast-ggufmmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-ggufCodellama版 GGUFmmnga/ELYZA-japanese-CodeLlama-7b-ggufmmnga/ELYZA-japanese-CodeLlama-7b-instruct-ggufCodellama版 GPTQmmnga/ELYZA-japanese-CodeLlama-7b-instruct-GPT
 * 📥 402 [sarulab-speech/hubert-base-jtube](https://huggingface.co/sarulab-speech/hubert-base-jtube) - hubert-base-jtubeThis repo provides model weights for the hubert-base model trained on the JTubeSpeech corpus.Scroll down for the model usageFAQQ. 
 * 📥 401 [MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF) - MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUFModel creator: MaziyarPanahiOriginal model: MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1DescriptionMaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF contains GGUF format model files for MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1.How to useThanks to TheBloke for preparing an amazing README on how to use GGUF models:About GGUFGGUF is a new format introduced
 * 📥 397 [ku-nlp/gpt2-small-japanese-char](https://huggingface.co/ku-nlp/gpt2-small-japanese-char) - Model Card for Japanese character-level GPT-2 SmallModel descriptionThis is a Japanese character-level GPT-2 Small (90M parameters) language model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.How to useYou can use this model directly with a pipeline for text generation.&gt;&gt;&gt; from transformers import pipeline, set_seed&gt;&gt;&gt; generator = pipeline('text-generation', model='ku-nlp/gpt2-small-japanese-char')&gt;&gt;&gt; set_seed(5)&gt;&gt;&gt;
 * 📥 390 [maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-base-gguf](https://huggingface.co/maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-base-gguf) - I'm constantly enhancing these model descriptions to provide you with the most relevant and comprehensive informationjapanese-stablelm-3b-4e1t-base - GGUFModel creator: stabilityaiOriginal model: japanese-stablelm-3b-4e1t-baseStableLMThis is a Model based on StableLM.Stablelm is a familiy of Language Models by Stability AI.Note:Current (as of 2023-11-15) implementations of Llama.cpp only support GPU offloading up to 34 Layers with these StableLM Models.
 * 📥 375 [vumichien/wav2vec2-large-xlsr-japanese-hiragana](https://huggingface.co/vumichien/wav2vec2-large-xlsr-japanese-hiragana) - Wav2Vec2-Large-XLSR-53-JapaneseFine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the Common Voice and Japanese speech corpus of Saruwatari-lab, University of Tokyo JSUT.When using this model, make sure that your speech input is sampled at 16kHz.
 * 📥 367 [hotchpotch/japanese-bge-reranker-v2-m3-v1](https://huggingface.co/hotchpotch/japanese-bge-reranker-v2-m3-v1) - hotchpotch/japanese-bge-reranker-v2-m3-v1日本語で学習させた Reranker (CrossEncoder) シリーズです。モデル名layershidden_sizehotchpotch/japanese-reranker-cross-encoder-xsmall-v16384hotchpotch/japanese-reranker-cross-encoder-small-v112384hotchpotch/japanese-reranker-cross-encoder-base-v112768hotchpotch/japanese-reranker-cross-encoder-large-v1241024hotchpotch/japanese-bge-reranker-v2-m3-v1241024Reranker についてや、技術レポート・評価等は以下を参考ください。日本語最高性能のRerankerをリリース / そもそも Reranker とは?日本語 Reranker 作成のテクニカルレポート使い方SentenceTransformersfrom sentence
 * 📥 363 [bclavie/fio-base-japanese-v0.1](https://huggingface.co/bclavie/fio-base-japanese-v0.1) - fio-base-japanese-v0.1日本語版は近日公開予定です（日本語を勉強中なので、間違いはご容赦ください！）fio-base-japanese-v0.1 is a proof of concept, and the first release of the Fio family of Japanese embeddings.
 * 📥 359 [mmnga/SakanaAI-EvoLLM-JP-A-v1-7B-gguf](https://huggingface.co/mmnga/SakanaAI-EvoLLM-JP-A-v1-7B-gguf) - SakanaAI-EvoLLM-JP-A-v1-7B-ggufSakanaAIさんが公開しているEvoLLM-JP-A-v1-7Bのggufフォーマット変換版です。
 * 📥 354 [tsmatz/roberta_qa_japanese](https://huggingface.co/tsmatz/roberta_qa_japanese) - roberta_qa_japanese(Japanese caption : 日本語の (抽出型) 質問応答のモデル)This model is a fine-tuned version of rinna/japanese-roberta-base (pre-trained RoBERTa model provided by rinna Co., Ltd.) trained for extractive question answering.
 * 📥 348 [cyberagent/xlm-roberta-large-jnli-jsick](https://huggingface.co/cyberagent/xlm-roberta-large-jnli-jsick) - Japanese Natural Language Inference ModelThis model was trained using SentenceTransformers Cross-Encoder class, gradient accumulation PR, and the code from CyberAgentAILab/japanese-nli-model.
 * 📥 344 [Mizuiro-sakura/luke-japanese-base-finetuned-QA](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-QA) - このモデルはluke-japanese-base-liteをファインチューニングして、Question-Answeringに用いれるようにしたものです。このモデルはluke-japanese-base-liteを運転ドメインQAデータセット（DDQA）（　https://nlp.ist.i.kyoto-u.ac.jp/index.php?Driving%20domain%20QA%20datasets　）を用いてファインチューニングしたものです。Question-Answeringタスク（SQuAD）に用いることができます。This model is fine-tuned model for Question-Answering which is based on luke-japanese-base-liteThis model is fine-tuned by using DDQA dataset.
 * 📥 343 [retrieva-jp/t5-xl](https://huggingface.co/retrieva-jp/t5-xl) - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus.
 * 📥 341 [izumi-lab/bert-small-japanese](https://huggingface.co/izumi-lab/bert-small-japanese) - BERT small Japanese financeThis is a BERT model pretrained on texts in the Japanese language.
 * 📥 333 [OrionStarAI/Orion-14B-LongChat](https://huggingface.co/OrionStarAI/Orion-14B-LongChat) - Orion-14B🌐English | 🇨🇳中文 | 🇯🇵日本語 | 🇰🇷한국어🤗 HuggingFace Mainpage | 🤖 ModelScope Mainpage🎬 HuggingFace Demo | 🎫 ModelScope Demo😺 GitHub📖 Tech ReportTable of Contents📖 Model Introduction🔗 Model Download🔖 Model Benchmark📊 Model Inference📜 Declarations &amp; License🥇 Company Introduction1.
 * 📥 331 [KoichiYasuoka/bert-base-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/bert-base-japanese-wikipedia-ud-head) - bert-base-japanese-wikipedia-ud-headModel
 * 📥 329 [turing-motors/heron-chat-git-ja-stablelm-base-7b-v1](https://huggingface.co/turing-motors/heron-chat-git-ja-stablelm-base-7b-v1) - Heron GIT Japanese StableLM
 * 📥 329 [rinna/nekomata-14b-gguf](https://huggingface.co/rinna/nekomata-14b-gguf) - rinna/nekomata-14b-ggufOverviewThe model is the GGUF version of rinna/nekomata-14b.
 * 📥 328 [ku-nlp/deberta-v2-large-japanese](https://huggingface.co/ku-nlp/deberta-v2-large-japanese) - Model Card for Japanese DeBERTa V2 largeModel descriptionThis is a Japanese DeBERTa V2 large model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and theJapanese portion of OSCAR.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained('ku-nlp/deberta-v2-large-japanese')model = AutoModelForMaskedLM.from_pretrained('ku-nlp/deberta-v2-large-japanese')sentence = '京都 大学 で 自然
 * 📥 327 [MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF) - MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUFModel creator: MaziyarPanahiOriginal model: MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1DescriptionMaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF contains GGUF format model files for MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1.How to useThanks to TheBloke for preparing an amazing README on how to use GGUF models:About GGUFGGUF is a new f
 * 📥 317 [mmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-gguf) - line-corporation/japanese-large-lm-3.6b-instruction-sftline-corporationさんが公開しているjapanese-large-lm-3.6b-instruction-sftのgguf変換版です。他モデルはこちらGPT-NEOXmmnga/line-corp-japanese-large-lm-3.6b-ggufmmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-ggufGPT-2mmnga/line-corp-japanese-large-lm-1.7b-ggufmmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-gguf注意:こちらはブランチで試用になります。llama.cpp本家にgptneox, gpt2が実装された時に、このggufファイルが使用できない可能性があります。GitHubリポジトリの readme はこちらUsage (試用)git clone --branch
 * 📥 315 [hajime9652/xlnet-japanese](https://huggingface.co/hajime9652/xlnet-japanese) - XLNet-japaneseModel descriptionThis model require Mecab and senetencepiece with XLNetTokenizer.
 * 📥 314 [TheBloke/japanese-stablelm-instruct-beta-7B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-7B-GGUF) - Chat &amp; support: TheBloke's Discord serverWant to contribute?
 * 📥 310 [webbigdata/C3TR-Adapter](https://huggingface.co/webbigdata/C3TR-Adapter) - モデルカード(Model Card for Model ID)C3TR-AdapterはGoogleが発表したLLMであるgemma-7bの日英・英日翻訳性能を向上させるQLoRA Adapterです。C3TR-Adapter is a QLoRA Adapter that improves the Japanese-English and English-Japanese translation performance of gemma-7b released by Google.
 * 📥 306 [llm-book/t5-base-long-livedoor-news-corpus](https://huggingface.co/llm-book/t5-base-long-livedoor-news-corpus) - llm-book/t5-base-long-livedoor-news-corpus「大規模言語モデル入門」の第7章で紹介している要約生成のモデルです。
 * 📥 303 [eepj/wstcg-mt-ja-en](https://huggingface.co/eepj/wstcg-mt-ja-en) - WS TCG Card Text TranslatorA Japanese-English machine translation model specifically trained for translating card text from the Weiss Schwarz (WS) Trading Card Game, fine-tuned on Helsinki-NLP/opus-mt-ja-en.
 * 📥 297 [nlp-waseda/roberta-large-japanese](https://huggingface.co/nlp-waseda/roberta-large-japanese) - nlp-waseda/roberta-large-japaneseModel descriptionThis is a Japanese RoBERTa large model pretrained on Japanese Wikipedia and the Japanese portion of CC-100.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained("nlp-waseda/roberta-large-japanese")model = AutoModelForMaskedLM.from_pretrained("nlp-waseda/roberta-large-japanese")sentence = '早稲田 大学 で 自然 言語 処理 を
 * 📥 293 [stanfordnlp/stanza-ja](https://huggingface.co/stanfordnlp/stanza-ja) - Stanza model for Japanese (ja)Stanza is a collection of accurate and efficient tools for the linguistic analysis of many human languages.
 * 📥 288 [SakanaAI/EvoLLM-JP-v1-10B](https://huggingface.co/SakanaAI/EvoLLM-JP-v1-10B) - 🐟 EvoLLM-JP-v1-10B🤗 Models | 📚 Paper | 📝 Blog | 🐦
 * 📥 286 [SakanaAI/EvoLLM-JP-A-v1-7B](https://huggingface.co/SakanaAI/EvoLLM-JP-A-v1-7B) - 🐟 EvoLLM-JP-A-v1-7B🤗 Models | 📚 Paper | 📝 Blog | 🐦
 * 📥 283 [clu-ling/whisper-large-v2-japanese-5k-steps](https://huggingface.co/clu-ling/whisper-large-v2-japanese-5k-steps) - whisper-large-v2-japanese-5k-stepsThis model is a fine-tuned version of openai/whisper-large-v2 on the Japanese CommonVoice dataset (v11)..
 * 📥 280 [mmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-gguf) - line-corporation/japanese-large-lm-1.7b-instruction-sftline-corporationさんが公開しているjapanese-large-lm-1.7b-instruction-sftのgguf変換版です。他モデルはこちらGPT-NEOXmmnga/line-corp-japanese-large-lm-3.6b-ggufmmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-ggufGPT-2mmnga/line-corp-japanese-large-lm-1.7b-ggufmmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-gguf変換スクリプトline-gpt2_convert-hf-to-gguf.pyUsage (試用)git clone https://github.com/ggerganov/llama.cpp.gitcd llama.cppmake
 * 📥 276 [mmnga/line-corp-japanese-large-lm-1.7b-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-1.7b-gguf) - line-corporation/japanese-large-lm-1.7bline-corporationさんが公開しているjapanese-large-lm-1.7bのgguf変換版です。他モデルはこちらGPT-NEOXmmnga/line-corp-japanese-large-lm-3.6b-ggufmmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-ggufGPT-2mmnga/line-corp-japanese-large-lm-1.7b-ggufmmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-gguf変換スクリプトline-gpt2_convert-hf-to-gguf.pyUsagegit clone --branch
 * 📥 270 [KoichiYasuoka/deberta-base-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-aozora-ud-head) - deberta-base-japanese-aozora-ud-headModel
 * 📥 266 [ku-nlp/gpt2-large-japanese-char](https://huggingface.co/ku-nlp/gpt2-large-japanese-char) - Model Card for Japanese character-level GPT-2 LargeModel descriptionThis is a Japanese character-level GPT-2 Large (717M parameters) language model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.How to useYou can use this model directly with a pipeline for text generation.&gt;&gt;&gt; from transformers import pipeline, set_seed&gt;&gt;&gt; generator = pipeline('text-generation', model='ku-nlp/gpt2-large-japanese-char')&gt;&gt;&gt; set_seed(5)&gt;&gt;&gt;
 * 📥 260 [stabilityai/japanese-stablelm-base-ja_vocab-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-ja_vocab-beta-7b) - Japanese-StableLM-Base-JAVocab-Beta-7BA cute robot wearing a kimono writes calligraphy with one single brush — Stable Diffusion XLModel Descriptionjapanese-stablelm-base-ja_vocab-beta-7b is a 7B-parameter decoder-only language model based on Llama-2-7b that has been fine-tuned on a diverse collection of Japanese data, with the intent of maximizing downstream performance on Japanese language tasks.
 * 📥 257 [mmnga/shisa-7b-v1-gguf](https://huggingface.co/mmnga/shisa-7b-v1-gguf) - shisa-7b-v1-ggufaugmxntさんが公開しているshisa-7b-v1のggufフォーマット変換版です。
 * 📥 245 [stabilityai/japanese-stablelm-instruct-ja_vocab-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-ja_vocab-beta-7b) - Japanese-StableLM-Instruct-JAVocab-Beta-7BA cute robot wearing a kimono writes calligraphy with one single brush — Stable Diffusion XLModel Descriptionjapanese-stablelm-instruct-ja_vocab-beta-7b is a 7B-parameter decoder-only language model based on japanese-stablelm-ja_vocab-beta-7b and further fine tuned on Databricks Dolly-15k, Anthropic HH, and other public data.
 * 📥 235 [OrionStarAI/Orion-14B-Chat-Int4](https://huggingface.co/OrionStarAI/Orion-14B-Chat-Int4) - Orion-14B🌐English | 🇨🇳中文 | 🇯🇵日本語 | 🇰🇷한국어🤗 HuggingFace Mainpage | 🤖 ModelScope Mainpage🎬 HuggingFace Demo | 🎫 ModelScope Demo😺 GitHub📖 Tech ReportTable of Contents📖 Model Introduction🔗 Model Download🔖 Model Benchmark📊 Model Inference📜 Declarations &amp; License🥇 Company Introduction1.
 * 📥 229 [TFMC/ChatNTQ-JA-7b-v1.0-GGUF](https://huggingface.co/TFMC/ChatNTQ-JA-7b-v1.0-GGUF) - GGUF conversion of NTQAI/chatntq-ja-7b-v1.0ChatNTQ-JA-7b-v1.0 is a Japanese chat fine-tuned model built on top of the stabilityai/japanese-stablelm-base-gamma-7b, which is originally based on Mistral 7B v0.1.
 * 📥 228 [cameltech/japanese-gpt-1b-PII-masking](https://huggingface.co/cameltech/japanese-gpt-1b-PII-masking) - japanese-gpt-1b-PII-maskingModel Descriptionjapanese-gpt-1b-PII-masking は、 日本語事前学習済み1B GPTモデルをベースとして、日本語の文章から個人情報をマスキングするように学習したモデルです。
 * 📥 227 [Mizuiro-sakura/luke-japanese-base-finetuned-ner](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-ner) - このモデルはluke-japanese-baseをファインチューニングして、固有表現抽出（NER）に用いれるようにしたものです。このモデルはluke-japanese-baseをWikipediaを用いた日本語の固有表現抽出データセット(ストックマーク社、https://github.com/stockmarkteam/ner-wikipedia-dataset )を用いてファインチューニングしたものです。固有表現抽出（NER）タスクに用いることができます。This model is fine-tuned model for Named-Entity-Recognition(NER) which is based on luke-japanese-baseThis model is fine-tuned by using Wikipedia dataset.
 * 📥 223 [tokyotech-llm/Swallow-13b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-NVE-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
 * 📥 215 [mmnga/SakanaAI-EvoLLM-JP-v1-7B-gguf](https://huggingface.co/mmnga/SakanaAI-EvoLLM-JP-v1-7B-gguf) - SakanaAI-EvoLLM-JP-v1-7B-ggufSakanaAIさんが公開しているEvoLLM-JP-v1-7Bのggufフォーマット変換版です。
 * 📥 214 [sonoisa/t5-base-japanese-v1.1](https://huggingface.co/sonoisa/t5-base-japanese-v1.1) - 日本語T5事前学習済みモデルThis is a T5 (Text-to-Text Transfer Transformer) model pretrained on Japanese corpus.
 * 📥 211 [NTQAI/wav2vec2-large-japanese](https://huggingface.co/NTQAI/wav2vec2-large-japanese) - Wav2Vec2-Large-JapaneseFine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the Common Voice, JSUT, TEDxJP and some other data.
 * 📥 206 [minutillamolinara/bert-japanese_finetuned-sentiment-analysis](https://huggingface.co/minutillamolinara/bert-japanese_finetuned-sentiment-analysis) - bert-japanese_finetuned-sentiment-analysisThis model was trained from scratch on the Japanese Sentiment Polarity Dictionary dataset.
 * 📥 203 [sonoisa/clip-vit-b-32-japanese-v1](https://huggingface.co/sonoisa/clip-vit-b-32-japanese-v1) - 日本語版CLIPモデルThis is a CLIP text/image encoder model for Japanese.
 * 📥 203 [mmnga/Deepreneur-blue-lizard-gguf](https://huggingface.co/mmnga/Deepreneur-blue-lizard-gguf) - Deepreneur-blue-lizard-ggufDeepreneurさんが公開しているblue-lizardのggufフォーマット変換版です。
 * 📥 202 [studio-ousia/luke-japanese-large-lite](https://huggingface.co/studio-ousia/luke-japanese-large-lite) - luke-japanese-large-liteluke-japanese is the Japanese version of LUKE (LanguageUnderstanding with Knowledge-based Embeddings), a pre-trainedknowledge-enhanced contextualized representation of words and entities.
 * 📥 202 [tokyotech-llm/Swallow-70b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-70b-instruct-v0.1) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
 * 📥 201 [tokyotech-llm/Swallow-13b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-13b-instruct-v0.1) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
 * 📥 200 [stabilityai/japanese-stable-vlm](https://huggingface.co/stabilityai/japanese-stable-vlm) - By downloading, using, or distributing any portion or element of this model, you agree to be bound by the agreement described in the LICENSE file.
 * 📥 199 [rinna/japanese-stable-diffusion](https://huggingface.co/rinna/japanese-stable-diffusion) - One more step before getting this model.
 * 📥 190 [reazon-research/reazonspeech-espnet-next](https://huggingface.co/reazon-research/reazonspeech-espnet-next) - reazonspeech-espnet-nextReazonSpeech is a project to maintain freely-available Japanese audiodatasets and ML models.reazonspeech-espnet-next is a "bleeding-edge" repository that containslatest ASR models trained by ReazonSpeech team.
 * 📥 186 [tokyotech-llm/Swallow-70b-NVE-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-NVE-instruct-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
 * 📥 185 [hotchpotch/japanese-reranker-cross-encoder-base-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-base-v1) - hotchpotch/japanese-reranker-cross-encoder-base-v1日本語で学習させた Reranker (CrossEncoder) シリーズです。モデル名layershidden_sizehotchpotch/japanese-reranker-cross-encoder-xsmall-v16384hotchpotch/japanese-reranker-cross-encoder-small-v112384hotchpotch/japanese-reranker-cross-encoder-base-v112768hotchpotch/japanese-reranker-cross-encoder-large-v1241024hotchpotch/japanese-bge-reranker-v2-m3-v1241024Reranker についてや、技術レポート・評価等は以下を参考ください。日本語最高性能のRerankerをリリース / そもそも Reranker とは?日本語 Reranker 作成のテクニカルレポート使い方SentenceTransformersfrom
 * 📥 183 [Mizuiro-sakura/deberta-v2-base-japanese-finetuned-QAe](https://huggingface.co/Mizuiro-sakura/deberta-v2-base-japanese-finetuned-QAe) - このモデルはdeberta-v2-base-japaneseをファインチューニングしてQAタスクに用いれるようにしたものです。このモデルはdeberta-v2-base-japaneseを運転ドメインQAデータセット（DDQA）（　https://nlp.ist.i.kyoto-u.ac.jp/index.php?Driving%20domain%20QA%20datasets　）を用いてファインチューニングしたものです。Question-Answeringタスク（SQuAD）に用いることができます。This model is fine-tuned model for Question-Answering which is based on deberta-v2-base-japaneseThis model is fine-tuned by using DDQA dataset.
 * 📥 173 [mmnga/line-corp-japanese-large-lm-3.6b-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-3.6b-gguf) - line-corporation/japanese-large-lm-3.6bline-corporationさんが公開しているjapanese-large-lm-3.6bのgguf変換版です。他モデルはこちらGPT-NEOXmmnga/line-corp-japanese-large-lm-3.6b-ggufmmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-ggufGPT-2mmnga/line-corp-japanese-large-lm-1.7b-ggufmmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-gguf注意:こちらはブランチで試用になります。llama.cpp本家にgptneox, gpt2が実装された時に、このggufファイルが使用できない可能性があります。GitHubリポジトリの readme はこちらUsage (試用)git clone --branch
 * 📥 171 [retrieva-jp/t5-large-short](https://huggingface.co/retrieva-jp/t5-large-short) - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus.
 * 📥 166 [studio-ousia/luke-japanese-base](https://huggingface.co/studio-ousia/luke-japanese-base) - luke-japaneseluke-japanese is the Japanese version of LUKE (Language Understanding with Knowledge-based Embeddings), a pre-trained knowledge-enhanced contextualized representation of words and entities.
 * 📥 165 [Mizuiro-sakura/luke-japanese-base-finetuned-jsts](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-jsts) - このモデルはluke-japanese-baseをファインチューニングして、JSTS(文章の類似度計算)に用いれるようにしたものです。このモデルはluke-japanese-baseをyahoo japan/JGLUEのJSTS( https://github.com/yahoojapan/JGLUE )を用いてファインチューニングしたものです。文章の類似度(5が最高値)を計算するタスクに用いることができます。This model is fine-tuned model for JSTS which is based on luke-japanese-baseThis model is fine-tuned by using yahoo japan JGLUE JSTS dataset.
 * 📥 161 [Local-Novel-LLM-project/Ninja-v1](https://huggingface.co/Local-Novel-LLM-project/Ninja-v1) - Our ModelsVecteusNinja-v1Ninja-v1-NSFWNinja-v1-128kNinja-v1-NSFW-128kModel Card for Ninja-v1.0The Mistral-7B--based Large Language Model (LLM) is an noveldataset fine-tuned version of the Mistral-7B-v0.1Ninja has the following changes compared to Mistral-7B-v0.1.Achieving both high quality Japanese and English generationMemory ability that does not forget even after long-context generationThis model was created with the help of GPUs from the first LocalAI hackathon.
 * 📥 161 [nlp-waseda/roberta-large-japanese-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-large-japanese-with-auto-jumanpp) - nlp-waseda/roberta-large-japanese-with-auto-jumanppModel descriptionThis is a Japanese RoBERTa large model pretrained on Japanese Wikipedia and the Japanese portion of CC-100.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained("nlp-waseda/roberta-large-japanese-with-auto-jumanpp")model = AutoModelForMaskedLM.from_pretrained("nlp-waseda/roberta-large-japanese-with-auto-jumanpp")sentence
 * 📥 161 [ThePioneer/MoeDiffusionPlusPlus](https://huggingface.co/ThePioneer/MoeDiffusionPlusPlus) - モデル説明 (model explanation)V1 = MoeDiffusion 1.0 + (HassanBlend 1.5 - VMix03) * 0.2V2 = MoeDiffusion 0.6 : HassanBlend 1.5 0.2 : VMix03 : 0.2マージ元のルーツにNAIリークやInsta系モデルが含まれるという噂があるので、NAIリークアンチ・Insta系モデルアンチには非推奨理想の黒髪ポニテ顔が出せるYaguruMagikuを、ある程度顔が近くて制御しやすいAbyssOrangeMix2と混ぜてみた。
 * 📥 160 [rinna/nue-asr](https://huggingface.co/rinna/nue-asr) - rinna/nue-asrOverview[Paper][GitHub]We propose a novel end-to-end speech recognition model, Nue ASR, which integrates pre-trained speech and language models.
 * 📥 155 [owner203/japanese-llama-2-13b-gguf](https://huggingface.co/owner203/japanese-llama-2-13b-gguf) - Japanese-LLaMA-2-13B-GGUFJapanese-LLaMA-2-13B-GGUFはJapanese-LLaMA-2-13BのGGUF形式です。モデルURL：https://huggingface.co/owner203/japanese-llama-2-13b
 * 📥 155 [DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUF](https://huggingface.co/DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUF) - DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUFThis model was converted to GGUF format from inu-ai/alpaca-guanaco-japanese-gpt-1b using llama.cpp via the ggml.ai's GGUF-my-repo space.
 * 📥 153 [sappho192/jesc-ja-en-translator](https://huggingface.co/sappho192/jesc-ja-en-translator) - Japanese to English translatorJapanese to English translator model based on EncoderDecoderModel(bert-japanese+GPT2)UsageDemoPlease visit https://huggingface.co/spaces/sappho192/jesc-ja-en-translator-demoDependencies (PyPI)torchtransformersfugashiunidic-liteInferenceimport transformersimport torchencoder_model_name = "cl-tohoku/bert-base-japanese-v2"decoder_model_name = "openai-community/gpt2"src_tokenizer = transformers.
 * 📥 153 [Vsukiyaki/Yaki-Dofu-Mix](https://huggingface.co/Vsukiyaki/Yaki-Dofu-Mix) - Yaki-Dofu-Mix概要 / OverviewYaki-Dofu-Mixは、アニメ風の画風に特化したマージモデルです。 / Yaki-Dofu-Mix is a merge model that specializes in an anime-like painting style.
 * 📥 145 [Deepreneur/blue-lizard](https://huggingface.co/Deepreneur/blue-lizard) - Deepreneur-blue-lizardModel DescriptionDeepreneur-blue-lizardは、MetaのLlama-2-7bに対して、Wikipediaや書籍等の日本語の学習データを用いて追加事前学習と独自データによるファインチューニングを実施したモデルです。
 * 📥 138 [rinna/nekomata-7b-instruction-gguf](https://huggingface.co/rinna/nekomata-7b-instruction-gguf) - rinna/nekomata-7b-instruction-ggufOverviewThe model is the GGUF version of rinna/nekomata-7b-instruction.
 * 📥 138 [tokyotech-llm/Swallow-70b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-NVE-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data.
 * 📥 137 [sonoisa/t5-base-japanese-title-generation](https://huggingface.co/sonoisa/t5-base-japanese-title-generation) - 記事本文からタイトルを生成するモデルSEE: https://qiita.com/sonoisa/items/a9af64ff641f0bbfed44
 * 📥 137 [izumi-lab/deberta-v2-base-japanese](https://huggingface.co/izumi-lab/deberta-v2-base-japanese) - DeBERTa V2 base JapaneseThis is a DeBERTaV2 model pretrained on Japanese texts.
 * 📥 136 [nold/Orion-14B-Base-GGUF](https://huggingface.co/nold/Orion-14B-Base-GGUF) - Orion-14B🌐English | 🇨🇳中文 | 🇯🇵日本語 |🇰🇷한국어🤗 HuggingFace Mainpage | 🤖 ModelScope Mainpage🎬 HuggingFace Demo | 🎫 ModelScope Demo😺 GitHub📖 Tech ReportTable of Contents📖 Model Introduction🔗 Model Download🔖 Model Benchmark📊 Model Inference📜 Declarations &amp; License🥇 Company Introduction1.
 * 📥 134 [stabilityai/japanese-stablelm-instruct-alpha-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b) - This repository is publicly accessible, but you have to accept the conditions to access its files and content.
 * 📥 130 [toshi456/llava-jp-1.3b-v1.0](https://huggingface.co/toshi456/llava-jp-1.3b-v1.0) - LLaVA-JP Model CardModel detailModel type:LLaVA-JP is a vision-language model that can converse about input images.
 * 📥 128 [retrieva-jp/t5-small-medium](https://huggingface.co/retrieva-jp/t5-small-medium) - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus.
 * 📥 127 [izumi-lab/deberta-v2-small-japanese](https://huggingface.co/izumi-lab/deberta-v2-small-japanese) - DeBERTa V2 small JapaneseThis is a DeBERTaV2 model pretrained on Japanese texts.
 * 📥 121 [skytnt/gpt2-japanese-lyric-small](https://huggingface.co/skytnt/gpt2-japanese-lyric-small) - Japanese GPT2 Lyric ModelModel descriptionThe model is used to generate Japanese lyrics.
 * 📥 120 [nu-dialogue/sfc2022-stable-diffusion](https://huggingface.co/nu-dialogue/sfc2022-stable-diffusion) - SFCOCO Stable Diffusion Model CardSFCOCO Stable Diffusion is a Japanese-specific latent text-to-image diffusion model capable of generating photo-realistic images given any text input.
 * 📥 120 [Local-Novel-LLM-project/Ninja-v1-128k](https://huggingface.co/Local-Novel-LLM-project/Ninja-v1-128k) - Our ModelsVecteusNinja-v1Ninja-v1-NSFWNinja-v1-128kNinja-v1-NSFW-128kModel Card for Ninja-v1-128kThe Mistral-7B--based Large Language Model (LLM) is an noveldataset fine-tuned version of the Mistral-7B-v0.1Ninja-128k has the following changes compared to Mistral-7B-v0.1.128k context window (8k context in v0.1)Achieving both high quality Japanese and English generationMemory ability that does not forget even after long-context generationThis model was created with the help of GPUs from the first LocalAI hack
 * 📥 119 [ku-nlp/bart-base-japanese](https://huggingface.co/ku-nlp/bart-base-japanese) - Model Card for Japanese BART baseModel
 * 📥 116 [Local-Novel-LLM-project/Ninja-v1-GGUF](https://huggingface.co/Local-Novel-LLM-project/Ninja-v1-GGUF) - Ninja-v1 のGGUF版Our Models for GGUFVecteus-GGUFNinja-v1-GGUFNinja-v1-NSFW-GGUFNinja-v1-128k-GGUFNinja-v1-NSFW-128k-GGUF
 * 📥 116 [aerner/lm-v2](https://huggingface.co/aerner/lm-v2) - Aerner LM-v2事前学習から全部日本語で学習させたモデルのバージョン2です。
 * 📥 115 [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1) - Heron BLIP Japanese StableLM
 * 📥 115 [Spiral-AI/Spiral-RetNet-3b-base](https://huggingface.co/Spiral-AI/Spiral-RetNet-3b-base) - SpiralAI Spiral-RetNet-3b-baseWe have conducted pre-training from scratch on the RetNet (https://arxiv.org/abs/2307.08621) architecture model 3b using a mixed dataset of Japanese and English.
 * 📥 114 [alfredplpl/suzume-poc](https://huggingface.co/alfredplpl/suzume-poc) - はじめにGoogleのGemma-2Bを日本語で使えるように継続事前学習を施した、商用利用可能なベースモデルです。小型なのでスマホや家電などに向いています。ただし、Instruction tuningが困難な可能性があります。Colabで試すmmngaさんが作った軽量版をColabで試すUsagefrom transformers import AutoTokenizer, AutoModelForCausalLMtokenizer = AutoTokenizer.from_pretrained("alfredplpl/suzume-poc")model = AutoModelForCausalLM.from_pretrained("alfredplpl/suzume-poc")input_text = """人工知能とは"""input_ids = tokenizer(input_text, return_tensors="pt")outputs = model.generate(**input_ids,max_new_tokens=64)print(tokenizer.decode(outputs[0])
 * 📥 112 [kit-nlp/bert-base-japanese-sentiment-cyberbullying](https://huggingface.co/kit-nlp/bert-base-japanese-sentiment-cyberbullying) - electra-base-cyberbullyingThis is a BERT Base model for the Japanese language finetuned for automatic cyberbullying detection.
 * 📥 108 [stockmark/stockmark-13b-instruct](https://huggingface.co/stockmark/stockmark-13b-instruct) - Stockmark-13b-instructStockmark-13b-instruct is an instruction-tuned version of Stockmark-13b, a 13 billion parameter Japanese LLM.
 * 📥 107 [inu-ai/dolly-japanese-gpt-1b](https://huggingface.co/inu-ai/dolly-japanese-gpt-1b) - 更新履歴2023年5月7日「oasst1-89k-ja」データセットを追加して対話システムに対応しました。
 * 📥 106 [mmnga/Llama-3-70B-japanese-suzume-vector-v0.1](https://huggingface.co/mmnga/Llama-3-70B-japanese-suzume-vector-v0.1) - Model Card for Model ID実験モデルです /
 * 📥 100 [Lasorco/spekulatius](https://huggingface.co/Lasorco/spekulatius) - spekulatiusマージしているとたまに出てくる「目的の意図とは違うのだけどなんだか消すにはもったいないモデル」をおすそ分けするシリーズです。
 * 📥 100 [recruit-jp/japanese-clip-vit-b-32-roberta-base](https://huggingface.co/recruit-jp/japanese-clip-vit-b-32-roberta-base) - recruit-jp/japanese-clip-vit-b-32-roberta-baseOverviewDeveloped by: Recruit Co., Ltd.Model type: Contrastive Language-Image Pretrained ModelLanguage(s): JapaneseLICENSE: CC-BY-4.0More details are described in our tech blog post.日本語CLIP学習済みモデルとその評価用データセットの公開Model
 * 📥 99 [Lasorco/Kokuwa](https://huggingface.co/Lasorco/Kokuwa) - Kokuwalamettaの改良でマージさせるモデル探しをしていたらKiwiMixという面白そうなモデルを見つけました。
 * 📥 97 [ku-nlp/gpt2-medium-japanese-char](https://huggingface.co/ku-nlp/gpt2-medium-japanese-char) - Model Card for Japanese character-level
 * 📥 97 [oshizo/japanese-sexual-moderation-v2](https://huggingface.co/oshizo/japanese-sexual-moderation-v2) - japanese-sexual-moderation-v2は、studio-ousia/luke-japanese-large-liteをファインチューニングしたモデルです。短文が性的かどうかをスコアリングします。regressionで学習しており、出力するスコアはおおむね0-1の範囲を取りますが負の値や1を超える値が出る場合があります。長い文章は学習しておらず、入力は改行単位で分割することを想定しています。0.0-0.2: 全く性的ではない0.2-0.4: ほとんど性的な内容を含まない0.4-0.6: 性的な内容を含む可能性がある0.6-0.8: 性的な内容を含んでいる0.8-1.0: 非常に性的な内容であるUsageimport torchfrom transformers import AutoModelForSequenceClassification, AutoTokenizermodel_id
 * 📥 96 [sazyou-roukaku/AfterRealXL](https://huggingface.co/sazyou-roukaku/AfterRealXL) - こちらでアップロードできないので、civitaiにて先に公開しています。
 * 📥 94 [ysakuramoto/mobilebert-ja](https://huggingface.co/ysakuramoto/mobilebert-ja) - MobileBERT 日本語事前学習済みモデル爆誕！！
 * 📥 93 [retrieva-jp/t5-base-medium](https://huggingface.co/retrieva-jp/t5-base-medium) - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus.
 * 📥 92 [aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2](https://huggingface.co/aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2) - Swallow-MX-8x7b-NVE-v0.1に対し、Mixtral-8x7B-Instruct-v0.1とMixtral-8x7B-v0.1の差分をマージしたモデルです。Swallow-MX-8x7b-NVE-v0.1 + 0.8*(Mixtral-8x7B-Instruct-v0.1 - Mixtral-8x7B-v0.1)aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct のアウトプットの語彙がおかしい場合に使用してください。日本語がより自然になりますcontext size 32k token使用可能な日本語対応ローカル用LLMとしては2024年3月時点では最高クラスの性能です
 * 📥 88 [esnya/japanese_speecht5_tts](https://huggingface.co/esnya/japanese_speecht5_tts) - SpeechT5 (TTS task) for JapaneseSpeechT5 model fine-tuned for Japanese speech synthesis (text-to-speech) on JVS.This model utilizes the JVS dataset which encompasses 100 speakers.
 * 📥 84 [kotoba-tech/kotoba-speech-v0.1](https://huggingface.co/kotoba-tech/kotoba-speech-v0.1) - Kotoba-Speech-v0.1Kotoba-Speech v0.1 is a 1.2B Transformer-based speech generative model.
 * 📥 84 [ThePioneer/MoeSharpV1](https://huggingface.co/ThePioneer/MoeSharpV1) - モデル説明 (model explanation)MoeDiffusionPlusPlus 0.7 : DreamShaper 3.3 (full) 0.3。
 * 📥 83 [ThePioneer/MoeDiffusion](https://huggingface.co/ThePioneer/MoeDiffusion) - モデル説明 (model explanation)YaguruMagiku 0.6 : AbyssOrangeMix2_sfw 0.4マージ元のルーツにNAIリークが含まれるという噂があるので、NAIリークアンチには非推奨理想の黒髪ポニテ顔が出せるYaguruMagikuを、ある程度顔が近くて制御しやすいAbyssOrangeMix2と混ぜてみた。
 * 📥 83 [watashiha/watashiha-gpt-6b](https://huggingface.co/watashiha/watashiha-gpt-6b) - モデル概要AWSのtrn1インスタンスを用いて開発した大喜利言語モデルです。事前学習後に大喜利データでFine-tuningしています。Architecture: GPT2Vocab size: 44880Model size: 6B paramsLicense: Apache License 2.0Library: aws-neuron-reference-for-megatron-lm学習データ以下のコーパスを使用して、事前学習を行いました。その際のトークン数は477億トークンでした。C4の日本語データCC-100の日本語データOSCARの日本語データWikipediaの日本語ダンプデータ自社データFine-tuningは、693万件の大喜利データを用いて行いました。使用方法import torchfrom transformers import AutoModelForCausalLM, AutoTokenizermodel_name = "watashiha/watashiha-gpt-6b"tokenizer = AutoTokenizer.from_pretrained(model_name, u
 * 📥 81 [oshizo/donut-base-japanese-visual-novel](https://huggingface.co/oshizo/donut-base-japanese-visual-novel) - Donut (base-sized model, fine-tuned on visual novel like synthetic dataset )ビジュアルノベル風画像の合成データセットでnaver-clova-ix/donut-baseを訓練したモデルです。
 * 📥 80 [sappho192/aihub-ja-ko-translator](https://huggingface.co/sappho192/aihub-ja-ko-translator) - Japanese to Korean translatorJapanese to Korean translator model based on EncoderDecoderModel(bert-japanese+kogpt2)UsageDemoPlease visit https://huggingface.co/spaces/sappho192/aihub-ja-ko-translator-demoDependencies (PyPI)torchtransformersfugashiunidic-liteInferencefrom transformers import(EncoderDecoderModel,PreTrainedTokenizerFast,BertJapaneseTokenizer,)import torchencoder_model_name = "cl-tohoku/bert-base-japanese-v2"decoder_model_name = "skt/kogpt2-base-v2"src_tokenizer = BertJapaneseTokenizer.from_pre
 * 📥 79 [stabilityai/japanese-instructblip-alpha](https://huggingface.co/stabilityai/japanese-instructblip-alpha) - Japanese InstructBLIP AlphaModel DetailsJapanese InstructBLIP Alpha is a vision-language instruction-following model that enables to generate Japanese descriptions for input images and optionally input texts such as questions.
 * 📥 79 [ttop324/wav2vec2-live-japanese](https://huggingface.co/ttop324/wav2vec2-live-japanese) - wav2vec2-live-japanesehttps://github.com/ttop32/wav2vec2-live-japanese-translatorFine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese hiragana using thecommon_voiceJSUTCSS10TEDxJP-10KJVSJSSSInference#usageimport torchimport torchaudiofrom datasets import load_datasetfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processormodel = Wav2Vec2ForCTC.from_pretrained("ttop324/wav2vec2-live-japanese")processor = Wav2Vec2Processor.from_pretrained("ttop324/wav2vec2-live-japanese")test_dataset = load_dataset("commo
 * 📥 79 [spow12/Visual-novel-transcriptor](https://huggingface.co/spow12/Visual-novel-transcriptor) - Model Card for Model IDFine tunned ASR model from distil-whisper/distil-large-v2.This model aimed to transcribe japanese audio especially visual novel.
 * 📥 78 [ushikado/yuyuyui-chatbot](https://huggingface.co/ushikado/yuyuyui-chatbot) - yuyuyui-chatbotThis model is based on rinna/japanese-gpt2-medium and finetuned on Yuyuyui scenario corpus.
 * 📥 77 [abeja/Mixtral-8x7B-Instruct-v0.1-japanese](https://huggingface.co/abeja/Mixtral-8x7B-Instruct-v0.1-japanese) - Mixtral-8x7B-Instruct-v0.1-japaneseMixtral-8x7B-Instruct-v0.1-japaneseはMixtral-8x7B-Instruct-v0.1をベースに日本語の語彙拡張継続事前学習を実施したモデルです。詳細はABEJAのテックブログを参照してください。学習を実施したMetagton-LMのレポジトリはこちらです。使い方import torchfrom transformers import AutoModelForCausalLM,
 * 📥 73 [owner203/japanese-alpaca-2-13b-gguf](https://huggingface.co/owner203/japanese-alpaca-2-13b-gguf) - Japanese-Alpaca-2-13B-GGUFJapanese-Alpaca-2-13B-GGUFはJapanese-Alpaca-2-13BのGGUF形式です。モデルURL：https://huggingface.co/owner203/japanese-alpaca-2-13b
 * 📥 73 [alabnii/jmedroberta-base-manbyo-wordpiece](https://huggingface.co/alabnii/jmedroberta-base-manbyo-wordpiece) - alabnii/jmedroberta-base-manbyo-wordpieceModel descriptionThis is a Japanese RoBERTa base model pre-trained on academic articles in medical sciences collected by Japan Science and Technology Agency (JST).This model is released under the Creative Commons 4.0 International License (CC BY-NC-SA
 * 📥 72 [sambanovasystems/SambaLingo-Japanese-Chat](https://huggingface.co/sambanovasystems/SambaLingo-Japanese-Chat) - SambaLingo-Japanese-ChatSambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English.
 * 📥 72 [alfredplpl/gemma-2b-it-ja-poc-3](https://huggingface.co/alfredplpl/gemma-2b-it-ja-poc-3) - はじめになんか日本語が話せる商用利用可能なAIです。小型なのでスマホや家電などに向いています。Usagefrom transformers import AutoTokenizer, AutoModelForCausalLMimport torchfrom peft import PeftModel# トークナイザーとモデルの準備tokenizer = AutoTokenizer.from_pretrained("alfredplpl/ja-aozora-wikipedia-gemmba-2b")model = AutoModelForCausalLM.from_pretrained("alfredplpl/ja-aozora-wikipedia-gemmba-2b")model = PeftModel.from_pretrained(model = model, model_id = "alfredplpl/gemma-2b-it-ja-poc-3")# プロンプトの準備prompt="""あなたは親切なアシスタントです。英語は喋らず、日本語だけ喋ってください。&lt;start_of_turn&gt;us
 * 📥 70 [kit-nlp/bert-base-japanese-sentiment-irony](https://huggingface.co/kit-nlp/bert-base-japanese-sentiment-irony) - BERT Base Japanese for IronyThis is a BERT Base model for sentiment analysis in Japanese additionally finetuned for automatic irony detection.
 * 📥 69 [ken11/albert-base-japanese-v1-with-japanese-tokenizer](https://huggingface.co/ken11/albert-base-japanese-v1-with-japanese-tokenizer) - albert-base-japanese-v1-with-japanese日本語事前学習済みALBERTモデルですこのモデルではTokenizerにBertJapaneseTokenizerクラスを利用していますalbert-base-japanese-v1よりトークナイズ処理が楽になっていますHow to useファインチューニングこのモデルはPreTrainedモデルです基本的には各種タスク用にファインチューニングして使用されることを想定していますFill-Maskfor PyTorchfrom transformers import (AutoModelForMaskedLM, AutoTokenizer)tokenizer = AutoTokenizer.from_pretrained("ken11/albert-base-japanese-v1-with-japanese-tokenizer")model = AutoModelForMaskedLM.from_pretrained("ken11/albert-base-japanese-v1-with-japanese-tokenizer")tex
 * 📥 67 [sonoisa/sentence-bert-base-ja-en-mean-tokens](https://huggingface.co/sonoisa/sentence-bert-base-ja-en-mean-tokens) - This is a Japanese+English sentence-BERT model.
 * 📥 65 [rinna/nekomata-7b-gguf](https://huggingface.co/rinna/nekomata-7b-gguf) - rinna/nekomata-7b-ggufOverviewThe model is the GGUF version of rinna/nekomata-7b.
 * 📥 65 [nlp-waseda/gpt2-small-japanese](https://huggingface.co/nlp-waseda/gpt2-small-japanese) - nlp-waseda/gpt2-small-japaneseThis model is Japanese GPT-2 pretrained on Japanese Wikipedia and CC-100.Intended uses &amp; limitationsYou can use the raw model for text generation or fine-tune it to a downstream task.
 * 📥 64 [AIBunCho/japanese-novel-gpt-j-6b](https://huggingface.co/AIBunCho/japanese-novel-gpt-j-6b) - AIBunCho/japanese-novel-gpt-j-6bAI BunChoで利用しているモデルです。2021年に作った小説用言語モデルです。Model DetailsGPT-J-6BをTPUで2週間日本語tokenizerを用いて日本語データで事前学習し、その後2週間小説データで転移学習したものです。UsesGoogle colabのT4 High-RAMで動作確認しています。pip install transformers sentencepiece acceleratefrom transformers import GPTJForCausalLM, AlbertTokenizerimport torchtokenizer = AlbertTokenizer.from_pretrained('AIBunCho/japanese-novel-gpt-j-6b', keep_accents=True, remove_space=False)model = GPTJForCausalLM.from_pretrained("AIBunCho/japanese-novel-gpt-j-6b", torch_
 * 📥 62 [offtoung/tsukuyomi-chan-calm2-7b](https://huggingface.co/offtoung/tsukuyomi-chan-calm2-7b) - つくよみちゃんデータセットを用いて calm-2-7b-chat をファインチューニングしたモデルです。
 * 📥 61 [sonoisa/sentence-t5-base-ja-mean-tokens](https://huggingface.co/sonoisa/sentence-t5-base-ja-mean-tokens) - This is a Japanese sentence-T5 model.
 * 📥 61 [sociocom/MedNERN-CR-JA](https://huggingface.co/sociocom/MedNERN-CR-JA) - This is a model for named entity recognition of Japanese medical documents.
 * 📥 60 [Helsinki-NLP/opus-mt-ja-it](https://huggingface.co/Helsinki-NLP/opus-mt-ja-it) - jpn-itasource group: Japanesetarget group: ItalianOPUS readme: jpn-itamodel: transformer-alignsource language(s): jpn jpn_Hani jpn_Hira jpn_Kana jpn_Latn jpn_Yiiitarget language(s): itamodel: transformer-alignpre-processing: normalization + SentencePiece (spm32k,spm32k)download original weights: opus-2020-06-17.ziptest set translations: opus-2020-06-17.test.txttest set scores: opus-2020-06-17.eval.txtBenchmarkstestsetBLEUchr-FTatoeba-test.jpn.ita22.80.460System Info:hf_name: jpn-itasource_languages: jpntarg
 * 📥 59 [mr4/bert-base-jp-sentiment-analysis](https://huggingface.co/mr4/bert-base-jp-sentiment-analysis) - Sentiment Analysis in Japanese - Phân tích cảm xúc trong tiếng NhậtBert phân tích cảm xúcModel descriptionMô hình có tác dụng xác định cảm xúc của đoạn văn.
 * 📥 57 [wolf4032/bert-japanese-token-classification-search-local-cuisine](https://huggingface.co/wolf4032/bert-japanese-token-classification-search-local-cuisine) - Model Card for Model ID料理を検索するための質問文から、検索検索用キーワードである固有表現を抽出しますModel DetailsModel Description例えば、「東京の肉料理で、春に食べられる、鶏肉を使った料理を教えてください」という文章を入力すると、「東京　→　都道府県/地方(AREA)」　「肉料理　→　種類(TYPE)」　「春　→　季節(SZN)」　「鶏肉　→　食材(INGR)」のように、固有表現を抽出します抽出対象は、AREA、TYPE、SZN、INGRの４つですLanguage(s) (NLP): 日本語License: mitFinetuned from model: tohoku-nlp/bert-base-japanese-v2Model SourcesRepository: wolf4032/nlp-token-classificationデータセット、言語モデル、アプリの作成に使ったコードが掲載されていますPaper: [More Information Needed]Demo: wolf4032/japanese-token-classification-s
 * 📥 56 [MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1](https://huggingface.co/MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1) - japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1 is a merge of the following models:mistralai/Mistral-7B-Instruct-v0.1stabilityai/japanese-stablelm-base-gamma-7b🧩 Configurationslices:- sources:-
 * 📥 56 [MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1](https://huggingface.co/MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1) - japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1 is a merge of the following models:mistralai/Mistral-7B-Instruct-v0.1stabilityai/japanese-stablelm-instruct-gamma-7b🧩 Configurationslices:- sources:-
 * 📥 56 [LoneStriker/SambaLingo-Japanese-Chat-GGUF](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-GGUF) - SambaLingo-Japanese-ChatSambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English.
 * 📥 55 [patrickramos/bert-base-japanese-v2-wrime-fine-tune](https://huggingface.co/patrickramos/bert-base-japanese-v2-wrime-fine-tune) - WRIME-fine-tuned BERT base JapaneseThis model is a Japanese BERTBASE fine-tuned on the WRIME dataset.
 * 📥 55 [SkelterLabsInc/bert-base-japanese-jaquad](https://huggingface.co/SkelterLabsInc/bert-base-japanese-jaquad) - BERT base Japanese - JaQuADDescriptionA Japanese Question Answering model fine-tuned on JaQuAD.Please refer BERT base Japanese for details about the pre-training model.
 * 📥 52 [ryota39/Phi-3-mini-4k-instruct-dpo](https://huggingface.co/ryota39/Phi-3-mini-4k-instruct-dpo) - モデルベースモデル：microsoft/Phi-3-mini-4k-instruct学習データセット：llm-jp/hh-rlhf-12k-ja学習方式：フルパラメータチューニングサンプルimport torchfrom transformers import AutoTokenizer, AutoModelForCausalLMtokenizer = AutoTokenizer.from_pretrained("ryota39/Phi-3-mini-4k-instruct-dpo",trust_remote_code=True,)model = AutoModelForCausalLM.from_pretrained("ryota39/Phi-3-mini-4k-instruct-dpo",device_map="auto",torch_dtype='auto',trust_remote_code=True,)text = "&lt;|user|&gt;\n与えられた質問に対して英語で思考し、日本語で答えてください。東京の観光地を教えてください。\n&lt;|end|&gt;\n&lt;|assistant
 * 📥 52 [megagonlabs/roberta-long-japanese](https://huggingface.co/megagonlabs/roberta-long-japanese) - roberta-long-japanese (jumanpp + sentencepiece, mC4 Japanese)This is the longer input version of RoBERTa Japanese model pretrained on approximately 200
 * 📥 52 [sonoisa/t5-base-english-japanese](https://huggingface.co/sonoisa/t5-base-english-japanese) - 英語+日本語T5事前学習済みモデルThis is a T5 (Text-to-Text Transfer Transformer) model pretrained on English and Japanese balanced corpus.
 * 📥 52 [TeamFnord/manga-ocr](https://huggingface.co/TeamFnord/manga-ocr) - Manga OCROptical character recognition for Japanese text, with the main focus being Japanese manga.
 * 📥 51 [colorfulscoop/gpt2-small-ja](https://huggingface.co/colorfulscoop/gpt2-small-ja) - GPT-2 small Japanese modelThis repository contains a GPT2-small model trained on Japanese Wikipedia dataset.
 * 📥 50 [Ivydata/whisper-base-japanese](https://huggingface.co/Ivydata/whisper-base-japanese) - Fine-tuned Japanese Whisper model for speech recognition using whisper-baseFine-tuned openai/whisper-base on Japanese using Common Voice, JVS and JSUT.When using this model, make sure that your speech input is sampled at 16kHz.
 * 📥 49 [alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000](https://huggingface.co/alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000) - alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000Model descriptionThis is a Japanese RoBERTa base model pre-trained on academic articles in medical sciences collected by Japan Science and Technology Agency (JST).This model is released under the Creative Commons 4.0 International License (CC BY-NC-SA
 * 📥 45 [KoichiYasuoka/bert-base-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-unidic-luw-upos) - bert-base-japanese-unidic-luw-uposModel
 * 📥 44 [nlp-waseda/comet-gpt2-small-japanese](https://huggingface.co/nlp-waseda/comet-gpt2-small-japanese) - COMET-GPT2 jaFinetuned GPT-2 on ATOMIC ja using a causal language modeling (CLM) objective.
 * 📥 43 [kkuramitsu/mt5-mini9L](https://huggingface.co/kkuramitsu/mt5-mini9L) - Model Card for Model IDThis is a small T5 (Text-to-Text Transfer Transformer) model pretrained on Japanese and English corpus.
 * 📥 42 [drewschaub/whisper-large-v3-japanese-4k-steps](https://huggingface.co/drewschaub/whisper-large-v3-japanese-4k-steps) - whisper-large-v3-japanese-4k-stepsThis model is a fine-tuned version of openai/whisper-large-v3 on the Common Voice 16.1 dataset.
 * 📥 42 [furnqse/elyza-fork2](https://huggingface.co/furnqse/elyza-fork2) - ELYZA-japanese-Llama-2-7bModel DescriptionELYZA-japanese-Llama-2-7b
 * 📥 41 [megagonlabs/t5-base-japanese-web-8k](https://huggingface.co/megagonlabs/t5-base-japanese-web-8k) - t5-base-japanese-web-8k (with Byte-fallback, 8K)Descriptionmegagonlabs/t5-base-japanese-web-8k is a T5 (Text-to-Text Transfer Transformer) model pre-trained on Japanese web texts.
 * 📥 41 [alfredplpl/gemma-2b-it-ja-poc](https://huggingface.co/alfredplpl/gemma-2b-it-ja-poc) - Noteこのモデルはマージに失敗してバグっているため、こちらをおすすめします。Google ColabUsagefrom transformers import AutoTokenizer, AutoModelForCausalLMimport torch# トークナイザーとモデルの準備tokenizer = AutoTokenizer.from_pretrained("alfredplpl/gemma-2b-it-ja-poc")model = AutoModelForCausalLM.from_pretrained("alfredplpl/gemma-2b-it-ja-poc")# プロンプトの準備prompt="""あなたは親切なアシスタントです。英語は喋らず、日本語だけ喋ってください。&lt;start_of_turn&gt;user人生で大切なことはなんですか？&lt;end_of_turn&gt;&lt;start_of_turn&gt;model"""# 推論の実行input_ids = tokenizer(prompt, return_tensors="pt").to(model.device
 * 📥 40 [sonoisa/byt5-small-japanese](https://huggingface.co/sonoisa/byt5-small-japanese) - 日本語ByT5事前学習済みモデルThis is a ByT5 (a tokenizer-free extension of the Text-to-Text Transfer Transformer) model pretrained on Japanese corpus.
 * 📥 40 [Formzu/bert-base-japanese-jsnli](https://huggingface.co/Formzu/bert-base-japanese-jsnli) - bert-base-japanese-jsnliThis model is a fine-tuned version of cl-tohoku/bert-base-japanese-v2 on the JSNLI dataset.
 * 📥 40 [abhishek/autonlp-japanese-sentiment-59363](https://huggingface.co/abhishek/autonlp-japanese-sentiment-59363) - Model Trained Using AutoNLPProblem type: Binary ClassificationModel ID: 59363Validation MetricsLoss: 0.12651239335536957Accuracy: 0.9532079853817648Precision: 0.9729688278823665Recall: 0.9744633462616643AUC: 0.9717333684823413F1: 0.9737155136027014UsageYou can use cURL to access this model:$ curl -X POST -H "Authorization: Bearer YOUR_API_KEY" -H "Content-Type: application/json" -d '{"inputs": "I love AutoNLP"}'
 * 📥 39 [uzabase/luke-japanese-wordpiece-base](https://huggingface.co/uzabase/luke-japanese-wordpiece-base) - studio-ousia/luke-japanese-baseに対して次の変更を加えたモデルです。ベースのモデルをRoBERTaから日本語BERTに切り替え、それに伴ってトークナイザがSentencepieceからWordPieceになりました2023年7月1日時点の日本語Wikipediaのデータで事前学習をおこないました[UNK] (unknown) エンティティを扱えるようにしました詳細はブログ記事をご参照ください。使用方法from transformers import AutoTokenizer, AutoModel# 本モデル用のトークナイザのコードを使用するため、trust_remote_code=True の指定が必要ですtokenizer = AutoTokenizer.from_pretrained("uzabase/luke-japanese-wordpiece-base", trust_remote_code=True)model = AutoModel.from_pretrained("uzabase/luke-japanese-wordpiece-base")更新情報2023/11
 * 📥 38 [daisaku-s/medtxt_ner_roberta](https://huggingface.co/daisaku-s/medtxt_ner_roberta) - 日本語医療固有表現抽出モデル概要ソーシャル・コンピューティング研究室さまより公開されているMedTxt-CRを用いて、alabniiさまより公開されているRoBERTaをfine-tuningした固有表現抽出モデルです。
 * 📥 37 [recruit-jp/japanese-typo-detector-roberta-base](https://huggingface.co/recruit-jp/japanese-typo-detector-roberta-base) - recruit-jp/japanese-typo-detector-roberta-baseモデルの概要日本語の文章を入力すると各文字ごとに誤字脱字である確率を出力します各ラベルの意味は以下の通りですidlabelmeaning0OK誤字なし1deletion1文字の抜け2insertion_a余分な1文字の挿入3insertion_b直前の文字列と一致する２文字以上の余分な文字の挿入4kanji-conversion_a同一の読みを持つ漢字の入れ替え（誤変換）5kanji-conversion_b近い読みを持つ漢字の入れ替え（誤変換）6substitution1文字の入れ替え7transposition隣接する２文字間の転置8othersその他の入力誤り誤り種類の詳細については学習データセットの元論文をご参照ください日本語 Wikipedia の編集履歴に基づく 入力誤りデータセットと訂正システムの改良その他、モデルの詳細については当社ブログ記事をご参照ください誤字脱字検出モデルをHugging Face Hubに公開しました (Recruit Data Blog)学習データ京都大学大学院情報学研究科知能情
 * 📥 36 [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1-llava-620k](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1-llava-620k) - Heron BLIP Japanese StableLM
 * 📥 36 [rinna/japanese-hubert-large](https://huggingface.co/rinna/japanese-hubert-large) - rinna/japanese-hubert-largeOverviewThis is a Japanese HuBERT Large model trained by rinna Co., Ltd.Model summaryThe model architecture is the same as the original HuBERT Large model, which contains 24 transformer layers with 16 attention heads.
 * 📥 36 [taishi-i/awesome-japanese-nlp-classification-model](https://huggingface.co/taishi-i/awesome-japanese-nlp-classification-model) - Model overviewThis model is the baseline model for awesome-japanese-nlp-classification-dataset.
 * 📥 36 [ku-nlp/roberta-large-japanese-char-wwm](https://huggingface.co/ku-nlp/roberta-large-japanese-char-wwm) - ku-nlp/roberta-large-japanese-char-wwmModel descriptionThis is a Japanese RoBERTa large model pre-trained on Japanese Wikipedia and the Japanese portion of CC-100.This model is trained with character-level tokenization and whole word masking.
 * 📥 36 [zh-plus/faster-whisper-large-v2-japanese-5k-steps](https://huggingface.co/zh-plus/faster-whisper-large-v2-japanese-5k-steps) - Converted from clu-ling/whisper-large-v2-japanese-5k-steps using CTranslate2.Usage:Install pip install faster-whisper (Check faster-whisper for detailed instructions.)from faster_whisper import WhisperModelmodel = WhisperModel('zh-plus/faster-whisper-large-v2-japanese-5k-steps', device="cuda", compute_type="float16")segments, info = model.transcribe("audio.mp3", beam_size=5)print("Detected language '%s' with probability
 * 📥 35 [if001/tiny_mixtral_ja](https://huggingface.co/if001/tiny_mixtral_ja) - 275.86Mのmixtralを日本語データセットでpretrainingしたものですsamplefrom transformers import AutoTokenizer, AutoModelForCausalLMmodel = AutoModelForCausalLM.from_pretrained("if001/tiny_mixtral_ja")tokenizer = AutoTokenizer.from_pretrained("if001/sentencepiece_ja", trust_remote_code=True)prompt = "それは九月初旬のある蒸し暑い晩のことであった。私は、Ｄ坂の"inputs = tokenizer(prompt, return_tensors="pt")generate_ids = model.generate(inputs.input_ids,max_length=30,top_k=30,top_p=0.95,temperature=0.6,repetition_penalty=1.2,do_sample=True,)tokenizer.decode(gen
 * 📥 34 [napopoa32/swallow-hermes-st-v1](https://huggingface.co/napopoa32/swallow-hermes-st-v1) - swallow-hermes-st-v1物語作成に強めなモデルが出来ないかと考えて作ったモデルです。
 * 📥 34 [ku-nlp/bart-large-japanese](https://huggingface.co/ku-nlp/bart-large-japanese) - Model Card for Japanese BART largeModel descriptionThis is a Japanese BART large model pre-trained on Japanese Wikipedia.
 * 📥 33 [Hemlok/REV-Mix](https://huggingface.co/Hemlok/REV-Mix) - ◆REV-Mix"レボリューション"なモデルです。
 * 📥 32 [cinmodel/electra-small-japanese-generator](https://huggingface.co/cinmodel/electra-small-japanese-generator) - Japanese ELECTRA-smallWe provide a Japanese ELECTRA-Small model, as described in ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators.
 * 📥 32 [ken11/bert-japanese-ner](https://huggingface.co/ken11/bert-japanese-ner) - bert-japanese-nerこのモデルは日本語の固有表現抽出タスクを目的として、京都大学 黒橋・褚・村脇研究室が公開しているBERT日本語Pretrainedモデルをベースにストックマーク株式会社が公開しているner-wikipedia-datasetでファインチューニングしたものです。
 * 📥 32 [aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct](https://huggingface.co/aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct) - 更新情報日本語機能とinstructベクトルのバランス調整したver.2をアップロードしましたSwallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2モデル概要Swallow-MX-8x7b-NVE-v0.1に対し、Mixtral-8x7B-Instruct-v0.1とMixtral-8x7B-v0.1の差分をマージしたモデルです。
 * 📥 31 [sonoisa/t5-base-japanese-adapt](https://huggingface.co/sonoisa/t5-base-japanese-adapt) - 日本語T5 Prefix Language ModelThis is a T5 (Text-to-Text Transfer Transformer)
 * 📥 31 [elyza/ELYZA-japanese-CodeLlama-7b](https://huggingface.co/elyza/ELYZA-japanese-CodeLlama-7b) - ELYZA-japanese-CodeLlama-7bModel DescriptionELYZA-japanese-CodeLlama-7b は、 Code Llamaをベースとして日本語能力を拡張するために追加事前学習を行ったモデルです。詳細は
 * 📥 31 [nlp-waseda/gpt2-xl-japanese](https://huggingface.co/nlp-waseda/gpt2-xl-japanese) - nlp-waseda/gpt2-xl-japaneseThis is Japanese GPT2 with approximately　1.5B parameters pretrained on Japanese Wikipedia and CC-100The model architecture of the model are based on Radford+ 2019.Intended uses &amp; limitationsYou can use the raw model for text generation or fine-tune it to a downstream task.
 * 📥 30 [watashiha/Watashiha-Llama-2-13B-Ogiri-sft](https://huggingface.co/watashiha/Watashiha-Llama-2-13B-Ogiri-sft) - The English document is here.
 * 📥 30 [OrionStarAI/Orion-14B-Base-Int4](https://huggingface.co/OrionStarAI/Orion-14B-Base-Int4) - Orion-14B🌐English | 🇨🇳中文 | 🇯🇵日本語 |🇰🇷한국어🤗 HuggingFace Mainpage | 🤖 ModelScope Mainpage🎬 HuggingFace Demo | 🎫 ModelScope Demo😺 GitHub📖 Tech ReportTable of Contents📖 Model Introduction🔗 Model Download🔖 Model Benchmark📊 Model Inference📜 Declarations &amp; License🥇 Company Introduction1.
 * 📥 29 [llm-book/bert-base-japanese-v3-crf-ner-wikipedia-dataset](https://huggingface.co/llm-book/bert-base-japanese-v3-crf-ner-wikipedia-dataset) - llm-book/bert-base-japanese-v3-crf-ner-wikipedia-dataset「大規模言語モデル入門」の第6章で紹介している固有表現認識のモデルです。cl-tohoku/bert-base-japanese-v3の出力層にCRF層を組み合わせたモデルをllm-book/ner-wikipedia-datasetでファインチューニングして構築されています。関連リンクGitHubリポジトリColabノートブックデータセット大規模言語モデル入門（Amazon.co.jp）大規模言語モデル入門（gihyo.jp）使い方from transformers import pipelinefrom pprint import pprintner_pipeline = pipeline(model="llm-book/bert-base-japanese-v3-crf-ner-wikipedia-dataset",aggregation_strategy="simple",)text = "大谷翔平は岩手県水沢市出身のプロ野球選手"# text中の固有表現を抽出pprint(ner_pipe
 * 📥 29 [OrionStarAI/Orion-14B-Chat-Plugin](https://huggingface.co/OrionStarAI/Orion-14B-Chat-Plugin) - Orion-14B🌐English | 🇨🇳中文 | 🇯🇵日本語 | 🇰🇷한국어🤗 HuggingFace Mainpage | 🤖 ModelScope Mainpage🎬 HuggingFace Demo | 🎫 ModelScope Demo😺 GitHub📖 Tech ReportTable of Contents📖 Model Introduction🔗 Model Download🔖 Model Benchmark📊 Model Inference📜 Declarations &amp; License🥇 Company Introduction1.
 * 📥 29 [ybelkada/japanese-roberta-question-answering](https://huggingface.co/ybelkada/japanese-roberta-question-answering) - RoBERTa base Japanese - JaQuADDescriptionA Japanese Question Answering model fine-tuned on JaQuAD.Please refer RoBERTa base Japanese for details about the pre-training model.
 * 📥 29 [sonoisa/t5-qiita-title-generation](https://huggingface.co/sonoisa/t5-qiita-title-generation) - 記事本文からタイトルを生成するモデルSEE: https://qiita.com/sonoisa/items/30876467ad5a8a81821f
 * 📥 28 [llm-book/bert-base-japanese-v3-jcommonsenseqa](https://huggingface.co/llm-book/bert-base-japanese-v3-jcommonsenseqa) - bert-base-japanese-v3-jcommonsenseqa「大規模言語モデル入門」の第5章で紹介している(多肢選択式質問応答)のモデルです。
 * 📥 28 [Formzu/bart-base-japanese](https://huggingface.co/Formzu/bart-base-japanese) - bart-base-japaneseThis model is converted from the original Japanese BART Pretrained model released by Kyoto University.
 * 📥 26 [yellowback/gpt-neo-japanese-1.3B](https://huggingface.co/yellowback/gpt-neo-japanese-1.3B) - GPT-Neo 1.3B pre-trained model for JapaneseModel DescriptionGPT2/GPT3 like model trained on Japanese.corpus.
 * 📥 26 [sambanovasystems/SambaLingo-Japanese-Base](https://huggingface.co/sambanovasystems/SambaLingo-Japanese-Base) - SambaLingo-Japanese-BaseSambaLingo-Japanese-Base is a pretrained Bi-lingual Japanese and English model that adapts Llama-2-7b to Japanese by training on 42 billion tokens from the Japanese split of the Cultura-X dataset.
 * 📥 25 [Mizuiro-sakura/luke-japanese-base-marcja](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-marcja) - このモデルはluke-japanese-baseをファインチューニングして、MARC-ja(positive or negativeの二値分類)に用いれるようにしたものです。このモデルはluke-japanese-baseをyahoo japan/JGLUEのMARC-ja( https://github.com/yahoojapan/JGLUE )を用いてファインチューニングしたものです。positive or negativeの二値分類タスクに用いることができます。This model is fine-tuned model for MARC-ja which is based on luke-japanese-baseThis model is fine-tuned by using yahoo japan JGLUE MARC-ja dataset.
 * 📥 25 [jovyan/Swallow-MS-7b-v0.1-ChatVector](https://huggingface.co/jovyan/Swallow-MS-7b-v0.1-ChatVector) - Swallow-MS-7b-v0.1-ChatVectorJapanese "instruction tuned" model made by the technique of Chat VectorThe weights of this model are obtained not by any instruction tuning but by the following arithmetic:Swallow-MS-7b-v0.1 + Mistral-7B-Instruct-v0.2 - Mistral-7B-v0.1Chat Vectorの手法を使って、学習済み重みの足し引きのみでSwallow-MS-7b-v0.1モデルにチャット形式の対話能力を与えたモデルです。詳細はこちらの日本語記事で解説しています。Instruction formatThe promot format should be the same as Mistral-7B-Instruct-v0.2.E.g.text = "&lt;s&gt;[INST]
 * 📥 25 [Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1-GGUF) - ELYZA-japanese-Llama-2-MoE-2x13B-v0.1-GGUF概要Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1の量子化済みGGUF版です。ライセンス等詳細は元モデルをご確認ください。現在はQ4_K_Mのみです。需要ありそうであれば他のものも用意します。DescriptionThis is the quantized GGUF version of Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1.
 * 📥 24 [AndrewMcDowell/wav2vec2-xls-r-300m-japanese](https://huggingface.co/AndrewMcDowell/wav2vec2-xls-r-300m-japanese) - This model is a fine-tuned version of facebook/wav2vec2-xls-r-300m on the MOZILLA-FOUNDATION/COMMON_VOICE_8_0 - JA dataset.
 * 📥 24 [hakuhodo-tech/japanese-clip-vit-h-14-bert-wider](https://huggingface.co/hakuhodo-tech/japanese-clip-vit-h-14-bert-wider) - Japanese CLIP ViT-H/14 (Wider)Table of ContentsOverviewUsageModel DetailsEvaluationLimitations and BiasesCitationSee AlsoContact InformationOverviewDeveloped by: HAKUHODO Technologies Inc.Model type: Contrastive Language-Image Pre-trained ModelLanguage(s): JapaneseLICENSE: CC BY-NC-SA 4.0Presented here is a Japanese CLIP (Contrastive Language-Image Pre-training) model,mapping Japanese texts and images to a unified embedding space.
 * 📥 24 [abeja/Mixtral-8x7B-v0.1-japanese](https://huggingface.co/abeja/Mixtral-8x7B-v0.1-japanese) - Mixtral-8x7B-v0.1-japaneseMixtral-8x7B-v0.1-japaneseはMixtral-8x7B-v0.1をベースに日本語の語彙拡張継続事前学習を実施したモデルです。詳細はABEJAのテックブログを参照してください。学習を実施したMetagton-LMのレポジトリはこちらです。使い方import torchfrom transformers import AutoModelForCausalLM,
 * 📥 23 [Bagus/wav2vec2-xlsr-japanese-speech-emotion-recognition](https://huggingface.co/Bagus/wav2vec2-xlsr-japanese-speech-emotion-recognition) - This is for (private) DEMO only.
 * 📥 23 [bardsai/finance-sentiment-ja-base](https://huggingface.co/bardsai/finance-sentiment-ja-base) - Finance Sentiment JA (base)Finance Sentiment JA (base) is a model based on bert-base-japanese for analyzing sentiment of Japanese financial news.
 * 📥 23 [akiFQC/bert-base-japanese-v3_nli-jsnli](https://huggingface.co/akiFQC/bert-base-japanese-v3_nli-jsnli) - Cross-Encoder for Natural Language Inference(NLI) for JapaneseConsidering the results of the JNLI evaluation result, we recommend using akiFQC/bert-base-japanese-v3_nli-jsnli-jnli-jsick for natural language inference in Japanese.
 * 📥 22 [sonoisa/t5-base-japanese-question-generation](https://huggingface.co/sonoisa/t5-base-japanese-question-generation) - 回答と回答が出てくるパラグラフを与えると質問文を生成するモデルSEE: https://github.com/sonoisa/deep-question-generation本モデルの作成ステップ概要SQuAD 1.1を日本語に機械翻訳し、不正なデータをクレンジング（有効なデータは約半分）。
 * 📥 22 [retrieva-jp/t5-large-medium](https://huggingface.co/retrieva-jp/t5-large-medium) - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus.
 * 📥 22 [kubota/luke-large-defamation-detection-japanese](https://huggingface.co/kubota/luke-large-defamation-detection-japanese) - luke-large-defamation-detection-japanese日本語誹謗中傷検出器This model is a fine-tuned version of studio-ousia/luke-japanese-large for the Japanese language finetuned for automatic defamation detection.
 * 📥 21 [haqishen/h2o-Llama-3-8B-Japanese-Instruct](https://huggingface.co/haqishen/h2o-Llama-3-8B-Japanese-Instruct) - IntroductionWho am I: Qishen Ha
 * 📥 21 [kit-nlp/bert-base-japanese-basic-char-v2-cyberbullying](https://huggingface.co/kit-nlp/bert-base-japanese-basic-char-v2-cyberbullying) - electra-base-cyberbullyingThis is a BERT Base model for the Japanese language finetuned for automatic cyberbullying detection.
 * 📥 21 [Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUF) - ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUF概要Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1の量子化済みGGUF版です。ライセンス等詳細は元モデルをご確認ください。現在はQ4_K_Mのみです。需要ありそうであれば他のものも用意します。DescriptionThis is the quantized GGUF version of Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1.
 * 📥 21 [ku-nlp/roberta-base-japanese-char-wwm](https://huggingface.co/ku-nlp/roberta-base-japanese-char-wwm) - ku-nlp/roberta-base-japanese-char-wwmModel descriptionThis is a Japanese RoBERTa base model pre-trained on Japanese Wikipedia and the Japanese portion of CC-100.This model is trained with character-level tokenization and whole word masking.
 * 📥 20 [kz/mt5base-finetuned-patentsum-japanese-small](https://huggingface.co/kz/mt5base-finetuned-patentsum-japanese-small) - Google's mt5-base fine-tuned in Japanese to summarize patent claims in a limited Pharmaceutical domain.
 * 📥 20 [snu-nia-12/wav2vec2-xls-r-300m_nia12_phone-hiragana_japanese](https://huggingface.co/snu-nia-12/wav2vec2-xls-r-300m_nia12_phone-hiragana_japanese) - Wav2Vec2-XLS-R-300M-Japanese-HiraganaFine-tuned facebook/wav2vec2-xls-r-300m on Japanese Hiragana characters using JSUT, JVS, Common Voice, and in-house dataset.
 * 📥 20 [akiFQC/japanese-dialogpt-small-aozora](https://huggingface.co/akiFQC/japanese-dialogpt-small-aozora) - Japanese DialoGPT trained with Aozora(ja) 青空文庫のセリフで学習した日本語のDialoGPT Smallです(en) Japanese DialoGPT Small trained on Aozora Bunko.
 * 📥 19 [Mizuiro-sakura/bert-large-japanese-v2-finetuned-ner](https://huggingface.co/Mizuiro-sakura/bert-large-japanese-v2-finetuned-ner) - このモデルはcl-tohoku/bert-large-japanese-v2をファインチューニングして、固有表現抽出（NER）に用いれるようにしたものです。このモデルはcl-tohoku/bert-large-japanese-v2をWikipediaを用いた日本語の固有表現抽出データセット(ストックマーク社、https://github.com/stockmarkteam/ner-wikipedia-dataset )
 * 📥 19 [turing-motors/heron-chat-git-ja-stablelm-base-7b-v0](https://huggingface.co/turing-motors/heron-chat-git-ja-stablelm-base-7b-v0) - Heron GIT Japanese StableLM
 * 📥 19 [Mizuiro-sakura/deberta-v2-base-japanese-finetuned-ner](https://huggingface.co/Mizuiro-sakura/deberta-v2-base-japanese-finetuned-ner) - このモデルはdeberta-v2-base-japaneseをファインチューニングして固有表現抽出（NER）に用いれるようにしたものです。このモデルはdeberta-v2-base-japaneseを Wikipediaを用いた日本語の固有表現抽出データセット(ストックマーク社、https://github.com/stockmarkteam/ner-wikipedia-dataset )を用いてファインチューニングしたものです。This model is fine-tuned model for Named Entity Recognition (NER) which is based on deberta-v2-base-japaneseThis model is fine-tuned by using Wikipedia dataset.
 * 📥 19 [stockmark/bart-base-japanese-news](https://huggingface.co/stockmark/bart-base-japanese-news) - bart-base-japanese-news(base-sized model)This repository provides a Japanese BART model.
 * 📥 18 [KoichiYasuoka/bert-large-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/bert-large-japanese-luw-upos) - bert-large-japanese-luw-uposModel
 * 📥 18 [if001/llama2_ja_ss](https://huggingface.co/if001/llama2_ja_ss) - 日本語でtrainingしたllama2model size:  130.78Mtrainingは以下のscript参照https://github.com/Lightning-AI/lit-gpt/tree/mainusefrom transformers import AutoTokenizer, AutoModelForCausalLMtokenizer = AutoTokenizer.from_pretrained("if001/sentencepiece_ja", trust_remote_code=True)model = AutoModelForCausalLM.from_pretrained("if001/llama2_ja_ss")import torchfrom transformers import GenerationConfigprompt="あのイーハトーヴォのすきとおった風、"inputs = tokenizer(prompt, return_tensors="pt")input_ids = inputs["input_ids"]generation_config = Gener
 * 📥 18 [izumi-lab/electra-base-japanese-discriminator](https://huggingface.co/izumi-lab/electra-base-japanese-discriminator) - ELECTRA base Japanese discriminatorThis is a ELECTRA model pretrained on texts in the Japanese language.
 * 📥 17 [KoichiYasuoka/deberta-large-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-unidic-luw-upos) - deberta-large-japanese-unidic-luw-uposModel
 * 📥 17 [NilanE/tinyllama-en_ja-translation-v2](https://huggingface.co/NilanE/tinyllama-en_ja-translation-v2) - In-progess long-context Japanese-English translation model based on tinyllama.
 * 📥 17 [taishi-i/nagisa_bert](https://huggingface.co/taishi-i/nagisa_bert) - nagisa_bertA BERT model for nagisa.
 * 📥 17 [hiroshi-matsuda-rit/ja_gsd_bert_wwm_unidic_lite](https://huggingface.co/hiroshi-matsuda-rit/ja_gsd_bert_wwm_unidic_lite) - Japanese transformer pipeline (bert-base).
 * 📥 16 [colorfulscoop/bert-base-ja](https://huggingface.co/colorfulscoop/bert-base-ja) - BERT base Japanese modelThis repository contains a BERT base model trained on Japanese Wikipedia dataset.
 * 📥 16 [vumichien/wav2vec2-large-xlsr-japanese](https://huggingface.co/vumichien/wav2vec2-large-xlsr-japanese) - Wav2Vec2-Large-XLSR-53-JapaneseFine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the Common Voice and Japanese speech corpus of Saruwatari-lab, University of Tokyo JSUT.When using this model, make sure that your speech input is sampled at 16kHz.
 * 📥 16 [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v0](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v0) - Heron BLIP Japanese StableLM
 * 📥 16 [izumi-lab/electra-small-japanese-discriminator](https://huggingface.co/izumi-lab/electra-small-japanese-discriminator) - ELECTRA small Japanese discriminatorThis is a ELECTRA model pretrained on texts in the Japanese language.
 * 📥 16 [hiroshi-matsuda-rit/bert-base-japanese-basic-char-v2](https://huggingface.co/hiroshi-matsuda-rit/bert-base-japanese-basic-char-v2) - BERT base Japanese (character-level tokenization with whole word masking, jawiki-20200831)This pretrained model is almost the same as cl-tohoku/bert-base-japanese-char-v2 but do not need fugashi or unidic_lite.
 * 📥 15 [Jumtra/mpt-7b-base](https://huggingface.co/Jumtra/mpt-7b-base) - MPT-7B-baseこのモデルは、MosaicMLのllm-foundryリポジトリを使用してmosaicml/mpt-7bをファインチューニングしたモデルです。
 * 📥 15 [Jumtra/mpt-7b-inst](https://huggingface.co/Jumtra/mpt-7b-inst) - MPT-7B-instこのモデルは、MosaicMLのllm-foundryリポジトリを使用してmosaicml/mpt-7b-instructをファインチューニングしたモデルです。
 * 📥 15 [Mizuiro-sakura/deberta-v2-japanese-tiny-finetuned-commonsenseqa](https://huggingface.co/Mizuiro-sakura/deberta-v2-japanese-tiny-finetuned-commonsenseqa) - このモデルはdeberta-v2-tiny-japaneseをファインチューニングしてCommonsenseQA(選択式の質問)に用いれるようにしたものです。このモデルはdeberta-v2-tiny-japaneseをyahoo japan/JGLUEのJCommonsenseQA( https://github.com/yahoojapan/JGLUE ) を用いてファインチューニングしたものです。This model is fine-tuned model for CommonsenseQA which is based on deberta-v2-tiny-japaneseThis model is fine-tuned by using JGLUE/JCommonsenseQA dataset.
 * 📥 15 [ptaszynski/yacis-electra-small-japanese-cyberbullying](https://huggingface.co/ptaszynski/yacis-electra-small-japanese-cyberbullying) - yacis-electra-small-cyberbullyingThis is an ELECTRA Small model for the Japanese language finetuned for automatic cyberbullying detection.
 * 📥 14 [nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanpp) - nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanppModel descriptionThis is a Japanese RoBERTa large model pretrained on Japanese Wikipedia and the Japanese portion of CC-100 with the maximum sequence length of 512.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained("nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanpp")model = AutoModelForMaskedLM.from_pretrained("nlp-wase
 * 📥 14 [retrieva-jp/t5-base-short](https://huggingface.co/retrieva-jp/t5-base-short) - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus.
 * 📥 14 [line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-128g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-128g-actorder_False) - japanese-large-lm-1.7b-instruction-sft-4bit-128g-actorder_FalseThis repository provides a 1.7B parameters Japanese language quantized model, fine-tuned and trained by LINE Corporation.
 * 📥 14 [mpasila/calm2-7b-safetensors](https://huggingface.co/mpasila/calm2-7b-safetensors) - This is a conversion of cyberagent/calm2-7b  to safetensors so you don't have to worry about getting hacked by downloading dirty pickled files.
 * 📥 14 [leia-llm/Leia-Swallow-7b](https://huggingface.co/leia-llm/Leia-Swallow-7b) - Leia-Swallow-7BLEIA is a training technique for autoregressive LLMs that effectively improves their performance in languages other than English by enhancing cross-lingual knowledge transfer from English to a target language.
 * 📥 14 [minkhantycc/translation-en-ja](https://huggingface.co/minkhantycc/translation-en-ja) - This model is the fine-tuned version of Helsinki-NLP/opus-mt-ja-en on bsd_ja_en dataset.
 * 📥 14 [KoichiYasuoka/deberta-base-japanese-wikipedia-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia-luw-upos) - deberta-base-japanese-wikipedia-luw-uposModel
 * 📥 14 [NovelAI/genji-jp](https://huggingface.co/NovelAI/genji-jp) - Genji-JP 6BPlease check our blog post for more details, samples, evaluations and more:BlogpostModel DescriptionGenji-JP 6B is a model finetuned on our Japanese storytelling dataset based on EleutherAI's GPT-J 6B model.
 * 📥 13 [Ivydata/whisper-small-japanese](https://huggingface.co/Ivydata/whisper-small-japanese) - Fine-tuned Japanese Whisper model for speech recognition using whisper-smallFine-tuned openai/whisper-small on Japanese using Common Voice, JVS and JSUT.When using this model, make sure that your speech input is sampled at 16kHz.
 * 📥 13 [hitachi-nlp/bert-base-japanese_nothing-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_nothing-unigram) - Japanese BERT-base (Nothing + Unigram)How to load the tokenizerPlease download the dictionary file for Nothing + Unigram from our GitHub repository.
 * 📥 13 [liwii/line-distilbert-base-japanese-fork](https://huggingface.co/liwii/line-distilbert-base-japanese-fork) - LINE DistilBERT Japanese (forked by liwii)This is a forked version of DistilBERT model pre-trained on 131 GB of Japanese web text.
 * 📥 13 [hakuhodo-tech/japanese-clip-vit-h-14-bert-base](https://huggingface.co/hakuhodo-tech/japanese-clip-vit-h-14-bert-base) - Japanese CLIP ViT-H/14 (Base)Table of ContentsOverviewUsageModel DetailsEvaluationLimitations and BiasesCitationSee AlsoContact InformationOverviewDeveloped by: HAKUHODO Technologies Inc.Model type: Contrastive Language-Image Pre-trained ModelLanguage(s): JapaneseLICENSE: CC BY-NC-SA 4.0Presented here is a Japanese CLIP (Contrastive Language-Image Pre-training) model,mapping Japanese texts and images to a unified embedding space.
 * 📥 13 [aken12/splade-japanese-efficient](https://huggingface.co/aken12/splade-japanese-efficient) - output筑波 2.0035860538482666つくば 1.6586617231369019研究 1.6227693557739258大学 1.3798155784606934実験 0.5522942543029785学生 0.42351895570755005分析 0.37844282388687134国立 0.3685397505760193キャンパス 0.36495038866996765茨城 0.3056415021419525科学 0.2876652181148529関東 0.24301066994667053地域 0.21340851485729218実施 0.1976248174905777先端 0.192025288939476サイト 0.11629197001457214調査 0.09159307181835175プロジェクト 0.08552580326795578議論 0.07484486699104309検討 0.007034890353679657
 * 📥 13 [tohoku-nlp/stable-diffusion-xl-jp-refiner-1.0](https://huggingface.co/tohoku-nlp/stable-diffusion-xl-jp-refiner-1.0) - (English part follows Japanese one.
 * 📥 13 [hitachi-nlp/bert-base-japanese_jumanpp-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_jumanpp-unigram) - Japanese BERT-base (Juman++ + Unigram)How to load the tokenizerPlease download the dictionary file for Juman++ + Unigram from our GitHub repository.
 * 📥 13 [Ivydata/wav2vec2-large-xlsr-53-japanese](https://huggingface.co/Ivydata/wav2vec2-large-xlsr-53-japanese) - Fine-tuned Japanese Wav2Vec2 model for speech recognition using XLSR-53 largeFine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using Common Voice, JVS and JSUT.When using this model, make sure that your speech input is sampled at 16kHz.
 * 📥 13 [qqpann/w2v_hf_jsut_xlsr53](https://huggingface.co/qqpann/w2v_hf_jsut_xlsr53) - Wav2Vec2-Large-XLSR-53-JapaneseFine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the Common Voice, and JSUT
 * 📥 13 [Momerio/meigen_generate_Japanese](https://huggingface.co/Momerio/meigen_generate_Japanese) - 名言推論モデル
 * 📥 12 [kanhatakeyama/Tanuki-ZeRo](https://huggingface.co/kanhatakeyama/Tanuki-ZeRo) - Tanuki-ZeroBase model: llm-jp/llm-jp-13b-v1.0Instruction data: Randomly sampled, 15k Jaster dataset (train)Code is here.
 * 📥 12 [turing-motors/heron-chat-git-ELYZA-fast-7b-v0](https://huggingface.co/turing-motors/heron-chat-git-ELYZA-fast-7b-v0) - Heron GIT Japanese ELYZA Llama 2 Fast 7BModel
 * 📥 12 [Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF) - ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF概要Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1の量子化済みGGUF版です。ライセンス等詳細は元モデルをご確認ください。現在はQ4_K_Mのみです。需要ありそうであれば他のものも用意します。DescriptionThis is the quantized GGUF version of Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1.
 * 📥 12 [dddump/Japanese-TextGen-Kage-v0.1-2x7B-gguf](https://huggingface.co/dddump/Japanese-TextGen-Kage-v0.1-2x7B-gguf) - Japanese-TextGen-Kage-v0.1-2x7BThis is a merge model using Mergekit-Evolve.We merged our model and Ninja-v1 with Mergekit-Evolve and then franken MoE.This model has been made more powerful by merging Ninja-v1!
 * 📥 12 [schroneko/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf](https://huggingface.co/schroneko/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf) - ELYZA-japanese-Llama-2-13b-fast-instruct-ggufELYZA-japanese-Llama-2-13b-fast-instructの GGUF
 * 📥 12 [oshizo/japanese-e5-mistral-7b_slerp](https://huggingface.co/oshizo/japanese-e5-mistral-7b_slerp) - This model was created by merging intfloat/e5-mistral-7b-instruct and stabilityai/japanese-stablelm-base-gamma-7b.
 * 📥 12 [dahara1/ELYZA-japanese-Llama-2-7b-instruct-AWQ](https://huggingface.co/dahara1/ELYZA-japanese-Llama-2-7b-instruct-AWQ) - Model Card for Model IDOriginal model elyza/ELYZA-japanese-Llama-2-7b-instruct which is based on Meta's "Llama 2" and has undergone additional pre-training in Japanese instruction.
 * 📥 12 [ku-nlp/deberta-v2-base-japanese-with-auto-jumanpp](https://huggingface.co/ku-nlp/deberta-v2-base-japanese-with-auto-jumanpp) - Model Card for Japanese DeBERTa V2 baseModel
 * 📥 12 [arc-r/faster-whisper-large-v2-mix-jp](https://huggingface.co/arc-r/faster-whisper-large-v2-mix-jp) - whisper-large-v2-mix-jp model for CTranslate2This repository contains the conversion of vumichien/whisper-large-v2-mix-jp to the CTranslate2 model format.
 * 📥 12 [KoichiYasuoka/deberta-large-japanese-aozora](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-aozora) - deberta-large-japanese-aozoraModel DescriptionThis is a DeBERTa(V2) model pre-trained on 青空文庫 texts.
 * 📥 12 [ptaszynski/yacis-electra-small-japanese](https://huggingface.co/ptaszynski/yacis-electra-small-japanese) - yacis-electra-smallThis is ELECTRA Small model for Japanese pretrained on 354 million sentences / 5.6 billion words of YACIS blog corpus.
 * 📥 11 [tohoku-nlp/bert-large-japanese-char](https://huggingface.co/tohoku-nlp/bert-large-japanese-char) - BERT large Japanese (character-level tokenization with whole word masking, jawiki-20200831)This is a BERT model pretrained on texts in the Japanese language.
 * 📥 11 [astremo/friendly_JA](https://huggingface.co/astremo/friendly_JA) - friendly_JA-Model　(T5 fine-tuned model)MT model trained using the friendly_JA Corpus attempting to make Japanese easier/more accessible to occidental people by using the Latin/English derived katakana lexicon instead of the standard Sino-Japanese lexiconExamplesinputoutput最適化を応用した機械翻訳モデルは高精度だオプティマイゼーションを応用したマシントランスレーションモデルは高いアキュラシーだ彼は架空の世界に住んでいる彼はイマジナリー世界に住んでいる新型コロナウイルスに感染してしまったコロナウイルスにかかってしまった深層学習は難しいディープラーニングはむずかしい新たな概念を紹介する新しいコンセプトを紹介する津波の警報が流れたツナミのアラートが流れた南海トラフの災害は震源地による南海トラフのディザスターはエピセンターによる息子は際どい内容の本を
 * 📥 11 [kit-nlp/electra-small-japanese-discriminator-cyberbullying](https://huggingface.co/kit-nlp/electra-small-japanese-discriminator-cyberbullying) - electra-base-cyberbullyingThis is an ELECTRA Small model for the Japanese language finetuned for automatic cyberbullying detection.
 * 📥 11 [alabnii/jmedroberta-base-sentencepiece](https://huggingface.co/alabnii/jmedroberta-base-sentencepiece) - alabnii/jmedroberta-base-sentencepieceModel descriptionThis is a Japanese RoBERTa base model pre-trained on academic articles in medical sciences collected by Japan Science and Technology Agency (JST).This model is released under the Creative Commons 4.0 International License (CC BY-NC-SA
 * 📥 11 [Mizuiro-sakura/deberta-v2-japanese-base-finetuned-commonsenseqa](https://huggingface.co/Mizuiro-sakura/deberta-v2-japanese-base-finetuned-commonsenseqa) - このモデルはdeberta-v2-base-japaneseをファインチューニングしてCommonsenseQA(選択式の質問)に用いれるようにしたものです。このモデルはdeberta-v2-base-japaneseをyahoo japan/JGLUEのJCommonsenseQA( https://github.com/yahoojapan/JGLUE ) を用いてファインチューニングしたものです。This model is fine-tuned model for CommonsenseQA which is based on deberta-v2-base-japaneseThis model is fine-tuned by using JGLUE/JCommonsenseQA dataset.
 * 📥 11 [TheBloke/japanese-stablelm-instruct-beta-70B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-70B-GPTQ) - Chat &amp; support: TheBloke's Discord serverWant to contribute?
 * 📥 11 [oshizo/japanese-e5-mistral-1.9b](https://huggingface.co/oshizo/japanese-e5-mistral-1.9b) - Model trained on 800,000 Japanese sentences after reducing oshizo/japanese-e5-mistral-7b_slerp to 8 layers.
 * 📥 11 [leia-llm/Leia-Swallow-13b](https://huggingface.co/leia-llm/Leia-Swallow-13b) - Leia-Swallow-13BLEIA is a training technique for autoregressive LLMs that effectively improves their performance in languages other than English by enhancing cross-lingual knowledge transfer from English to a target language.
 * 📥 11 [AdapterHub/bert-base-multilingual-cased-ja-wiki_pfeiffer](https://huggingface.co/AdapterHub/bert-base-multilingual-cased-ja-wiki_pfeiffer) - Adapter bert-base-multilingual-cased-ja-wiki_pfeiffer for bert-base-multilingual-casedPfeiffer Adapter trained with Masked Language Modelling on Japanese Wikipedia Articles for 250k steps and a batch size of 64.This adapter was created for usage with the Adapters library.
 * 📥 11 [akiFQC/bert-base-japanese-v3_nli-jsnli-jnli-jsick](https://huggingface.co/akiFQC/bert-base-japanese-v3_nli-jsnli-jnli-jsick) - Cross-Encoder for Natural Language Inference(NLI) for JapaneseThis model was trained using SentenceTransformers Cross-Encoder class.
 * 📥 11 [thefrigidliquidation/nllb-jaen-1.3B-lightnovels](https://huggingface.co/thefrigidliquidation/nllb-jaen-1.3B-lightnovels) - NLLB 1.3B fine-tuned on Japanese to English Light Novel translationThis model was fine-tuned on light and web novel for Japanese to English translation.
 * 📥 11 [qqpann/wav2vec2-large-xlsr-japanese-0325-1200](https://huggingface.co/qqpann/wav2vec2-large-xlsr-japanese-0325-1200) - Wav2Vec2-Large-XLSR-53-{language} #TODO: replace language with your {language}, e.g. FrenchFine-tuned facebook/wav2vec2-large-xlsr-53 on {language} using the Common Voice, ... and ... dataset{s}.

## Datasets

This list is sorted by downloads as of May 13, 2024.

 * 📥 27586 [shunk031/JGLUE](https://huggingface.co/datasets/shunk031/JGLUE) - Please feel free to open an issue or pull request.
 * 📥 11209 [sbintuitions/JMTEB](https://huggingface.co/datasets/sbintuitions/JMTEB) - JMTEB:
 * 📥 2884 [kunishou/databricks-dolly-15k-ja](https://huggingface.co/datasets/kunishou/databricks-dolly-15k-ja) - This dataset was created by automatically translating "databricks-dolly-15k" into Japanese.
 * 📥 1805 [llm-book/wrime-sentiment](https://huggingface.co/datasets/llm-book/wrime-sentiment) - GitHub リポジトリ ids-cv/wrime で公開されているデータセットを利用しています。
 * 📥 1684 [cl-nagoya/auto-wiki-qa](https://huggingface.co/datasets/cl-nagoya/auto-wiki-qa) - AutoWikiQA東工大が公開しているSwallow-MXを用いて、Wikipedia中のテキストを入力として「質問(query)」と「回答(answer)」を生成し、生成された質問と回答についてフィルタリングを行ったデータセットです。
 * 📥 1392 [range3/wiki40b-ja](https://huggingface.co/datasets/range3/wiki40b-ja) - range3/wiki40b-jaThis dataset consists of three parquet files from the wiki40b dataset with only Japanese data extracted.
 * 📥 1259 [elyza/ELYZA-tasks-100](https://huggingface.co/datasets/elyza/ELYZA-tasks-100) - ELYZA-tasks-100: 日本語instructionモデル評価データセットData Description本データセットはinstruction-tuningを行ったモデルの評価用データセットです。
 * 📥 1186 [fujiki/japanese_hh-rlhf-49k](https://huggingface.co/datasets/fujiki/japanese_hh-rlhf-49k) - This is a little bit different version of kunishou/hh-rlhf-49k-ja without ng_translation == 1 examples.
 * 📥 861 [kumapo/JAQKET](https://huggingface.co/datasets/kumapo/JAQKET) - Please feel free to open an issue or pull request.
 * 📥 813 [mkshing/xlsum_ja](https://huggingface.co/datasets/mkshing/xlsum_ja) - This is the filtered Japanese subset of XL-Sum followed by PaLM 2filters15-gram overlap* code: https://gist.github.com/mkshing/d6371cbfdd50d4f352cee247fd4dd86anumber of examplestrain: 4215 (before: 7113)validation: 758 (before: 889)test: 766 (before: 889)
 * 📥 779 [llm-jp/oasst1-21k-ja](https://huggingface.co/datasets/llm-jp/oasst1-21k-ja) - oasst1-21k-jaThis repository provides an instruction tuning dataset developed by LLM-jp, a collaborative project launched in Japan.
 * 📥 729 [llm-jp/databricks-dolly-15k-ja](https://huggingface.co/datasets/llm-jp/databricks-dolly-15k-ja) - databricks-dolly-15k-jaThis repository provides an instruction tuning dataset developed by LLM-jp, a collaborative project launched in Japan.
 * 📥 728 [datasets/bsd_ja_en](https://huggingface.co/datasets/bsd_ja_en) - The dataset was constructed in 3 steps:selecting business scenes,writing monolingual conversation scenarios according to the selected scenes, andtranslating the scenarios into the other language.
 * 📥 539 [joujiboi/japanese-anime-speech](https://huggingface.co/datasets/joujiboi/japanese-anime-speech) - Japanese Anime Speech Dataset日本語はこちらjapanese-anime-speech is an audio-text dataset designed for the training of automatic speech recognition models.
 * 📥 496 [nlp-waseda/JMMLU](https://huggingface.co/datasets/nlp-waseda/JMMLU) - JMMLUJapanese Massive Multitask Language Understanding BenchmarkJMMLU is a four-choice question set consisting of Japanese-translated questions of a portion of MMLU (Paper, Github) (Translated questions) and questions based on unique Japanese cultural context (Japanese questions).
 * 📥 453 [turing-motors/LLaVA-Instruct-150K-JA](https://huggingface.co/datasets/turing-motors/LLaVA-Instruct-150K-JA) - Dataset DetailsDataset Type:Japanese LLaVA Instruct 150K is a localized version of the original LLaVA Visual Instruct 150K dataset.
 * 📥 450 [kogi-jwu/jhumaneval](https://huggingface.co/datasets/kogi-jwu/jhumaneval) - LLM のコード生成能力の標準ベンチマーク HumanEval の日本語翻訳版です。機械翻訳(DeepL, GPT-4)の翻訳結果を全て人手によって再修正し、 訳文を日本人のプログラマが読んで理解し、コードが書ける内容かチェックしました。ただし、英語版 HumanEval の間違いは、修正せずに残して、 HumanEval 同様に不完全なドキュメントからの生成能力を見るようになっています。日本語LLM のベンチマークとしてお使いください。LanguagesThe programmin
 * 📥 373 [nyanko7/danbooru2023](https://huggingface.co/datasets/nyanko7/danbooru2023) - Danbooru2023:
 * 📥 353 [llm-jp/hh-rlhf-12k-ja](https://huggingface.co/datasets/llm-jp/hh-rlhf-12k-ja) - hh-rlhf-12k-jaThis repository provides a human preference dataset developed by LLM-jp, a collaborative project launched in Japan.
 * 📥 351 [llm-book/livedoor-news-corpus](https://huggingface.co/datasets/llm-book/livedoor-news-corpus) - オリジナルのサイトと同じものを使用しています。
 * 📥 340 [yuzuai/rakuda-questions](https://huggingface.co/datasets/yuzuai/rakuda-questions) - Rakuda - Questions for Japanese modelsRepository:
 * 📥 320 [globis-university/aozorabunko-clean](https://huggingface.co/datasets/globis-university/aozorabunko-clean) - OverviewThis dataset provides a convenient and user-friendly format of data from Aozora Bunko (青空文庫), a website that compiles public-domain books in Japan, ideal for Machine Learning applications.[For Japanese] 日本語での概要説明を Qiita に記載しました: https://qiita.com/akeyhero/items/b53eae1c0bc4d54e321fMethodologyThe code to reproduce this dataset is made available on GitHub: globis-org/aozorabunko-exctractor.1.
 * 📥 284 [llm-book/ner-wikipedia-dataset](https://huggingface.co/datasets/llm-book/ner-wikipedia-dataset) - Githubリポジトリstockmarkteam/ner-wikipedia-datasetで公開されているデータセットを利用しています。
 * 📥 279 [SkelterLabsInc/JaQuAD](https://huggingface.co/datasets/SkelterLabsInc/JaQuAD) - JaQuAD is developed to provide a SQuAD-like QA dataset in Japanese.
 * 📥 276 [neulab/odex](https://huggingface.co/datasets/neulab/odex) - ODEX is an Open-Domain EXecution-based NL-to-Code generation data benchmark.
 * 📥 258 [p1atdev/ichikara-instruction](https://huggingface.co/datasets/p1atdev/ichikara-instruction) - ichikara-instruction (Non Commercial)LLMのための日本語インストラクションデータ 公開ページ公開ページより、本データに関して、言語処理学会第３０回年次大会において発表を行います。
 * 📥 239 [kunishou/OpenMathInstruct-1-1.8m-ja](https://huggingface.co/datasets/kunishou/OpenMathInstruct-1-1.8m-ja) - OpenMathInstruct-1 を日本語に自動翻訳した商用利用可能な180万件の指示チューニングデータセットになります。
 * 📥 225 [if001/aozorabunko-clean-sin](https://huggingface.co/datasets/if001/aozorabunko-clean-sin) - this is forkhttps://huggingface.co/datasets/globis-university/aozorabunko-cleanfilteredrow["meta"]["文字遣い種別"] == "新字新仮名"
 * 📥 222 [datasets/snow_simplified_japanese_corpus](https://huggingface.co/datasets/snow_simplified_japanese_corpus) - The corpus has 50,000 manually simplified and aligned sentences.
 * 📥 192 [shunk031/jsnli](https://huggingface.co/datasets/shunk031/jsnli) - Dataset PreprocessingSupported Tasks and LeaderboardsLanguages注釈はすべて日本語を主要言語としています。
 * 📥 189 [izumi-lab/llm-japanese-dataset](https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset) - llm-japanese-datasetLLM構築用の日本語インストラクション(チャット)データセット主に，英語で構築されたLLMモデルなどに対して，チャット(Instruction)応答タスクに関してLoRAなどでチューニングするために使用できます．
 * 📥 187 [transformersegmentation/CHILDES](https://huggingface.co/datasets/transformersegmentation/CHILDES) - Phonemized Child Directed Speech DatasetThis dataset contains utterance downloaded from CHILDES which have been pre-processed and converted to phonemic transcriptions by this processing script.
 * 📥 185 [hotchpotch/JQaRA](https://huggingface.co/datasets/hotchpotch/JQaRA) - JQaRA : Japanese Question Answering with Retrieval Augmentation - 検索拡張(RAG)評価のための日本語 Q&amp;A データセット高性能な LLM の台頭に伴い、LLM を用いた質疑応答のユースケースが増加しています。
 * 📥 182 [hatakeyama-llm-team/AutoGeneratedJapaneseQA](https://huggingface.co/datasets/hatakeyama-llm-team/AutoGeneratedJapaneseQA) - 自動生成Q&amp;A種々のデータソースから､MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUFを使ってQ&amp;Aを自動生成したものです｡二種類の自動生成された回答が存在しますCommonCrawlまたは、CC-BY系のデータソースから生成しています。
 * 📥 174 [kunishou/oasst1-89k-ja](https://huggingface.co/datasets/kunishou/oasst1-89k-ja) - This dataset was created by automatically translating "OpenAssistant/oasst1" into Japanese.
 * 📥 166 [range3/cc100-ja](https://huggingface.co/datasets/range3/cc100-ja) - range3/cc100-jaThis dataset consists of parquet files from the cc100 dataset with only the Japanese language extracted and sharded.
 * 📥 163 [taishi-i/awesome-japanese-nlp-classification-dataset](https://huggingface.co/datasets/taishi-i/awesome-japanese-nlp-classification-dataset) - Dataset overviewThis dataset identifies whether a GitHub repository description pertains to Japanese natural language processing (NLP).The labels are categorized as "Relevant (1)" and "Not Relevant (0)".
 * 📥 160 [kunishou/oasst2-135k-ja](https://huggingface.co/datasets/kunishou/oasst2-135k-ja) - Update:2023/12/25oasst2-135k-jaをチャット形式に変換したoasst2-chat-68k-jaを公開しました。This dataset was created by automatically translating "OpenAssistant/oasst2" into Japanese by DeepL."OpenAssistant/oasst2" を DeepL翻訳を用いて日本語に自動翻訳したデータセットになります。以下のコードを用いることで、 Instruction と Output （prompterの命令とassistantの回答）の形式に変換することができます。ファインチューニングで使用する場合はこちらのコードで変換して下さい（変換には5分程度かかります）。変換コード参考https://github.com/h2oai/h2o-llmstudio/blob/5ebfd3879e226b4e1afd0a0b45eb632e60412129/app_utils/utils.py#L1888pip install datasetsfrom datasets import l
 * 📥 158 [llm-book/jawiki-sentences](https://huggingface.co/datasets/llm-book/jawiki-sentences) - GitHub リポジトリ singletongue/wikipedia-utils で公開されているデータセットを利用しています。
 * 📥 154 [hatakeyama-llm-team/japanese2010](https://huggingface.co/datasets/hatakeyama-llm-team/japanese2010) - 日本語ウェブコーパス2010こちらのデータをhuggingfaceにアップロードしたものです｡2009 年度における著作権法の改正（平成21年通常国会　著作権法改正等について | 文化庁）に基づき，情報解析研究への利用に限って利用可能です｡形態素解析を用いて､自動で句点をつけました｡変換コード変換スクリプト形態素解析など
 * 📥 131 [fujiki/japanese_alpaca_data](https://huggingface.co/datasets/fujiki/japanese_alpaca_data) - [github].
 * 📥 111 [taishi-i/nagisa_stopwords](https://huggingface.co/datasets/taishi-i/nagisa_stopwords) - Japanese stopwords for nagisaThis is a stopword list of frequently used words in the Japanese language, created according to the tokenization rules of the Japanese text analysis library, nagisa.
 * 📥 107 [Nexdata/multi_language](https://huggingface.co/datasets/Nexdata/multi_language) - SummaryThe dataset contains 25,000 hours of multi-language reading speech data.
 * 📥 107 [NilanE/ParallelFiction-Ja_En-100k](https://huggingface.co/datasets/NilanE/ParallelFiction-Ja_En-100k) - Dataset detailsEach entry in this dataset is a sentence-aligned Japanese web novel chapter and English fan translation.
 * 📥 106 [datasets/covid_tweets_japanese](https://huggingface.co/datasets/covid_tweets_japanese) - The annotation is by majority decision by 5 - 10 crowd workers.
 * 📥 106 [Atsushi/fungi_indexed_mycological_papers_japanese](https://huggingface.co/datasets/Atsushi/fungi_indexed_mycological_papers_japanese) - fungi_indexed_mycological_papers_japanese大菌輪「論文3行まとめ」データセット最終更新日：2024/2/23（R3-11457まで）====LanguagesJapaneseThis dataset is available in Japanese only.
 * 📥 106 [Atsushi/fungi_diagnostic_chars_comparison_japanese](https://huggingface.co/datasets/Atsushi/fungi_diagnostic_chars_comparison_japanese) - fungi_diagnostic_chars_comparison_japanese大菌輪「識別形質まとめ」データセット最終更新日：2024/2/23（R3-11457まで）====LanguagesJapaneseThis dataset is available in Japanese only.
 * 📥 105 [matsuxr/JaGovFaqs-22k](https://huggingface.co/datasets/matsuxr/JaGovFaqs-22k) - このデータセットについてこのデータは、日本の官公庁のWebサイトに掲載されている「よくある質問」を手作業で抽出し、インストラクション用のデータセットとしたものです。
 * 📥 105 [Atsushi/fungi_trait_circus_database](https://huggingface.co/datasets/Atsushi/fungi_trait_circus_database) - fungi_trait_circus_database大菌輪「Trait Circus」データセット（統制形質）最終更新日：2023/12/29====LanguagesJapanese and EnglishPlease do not use this dataset for academic purposes for the time being. 
 * 📥 103 [kunishou/J-ResearchCorpus](https://huggingface.co/datasets/kunishou/J-ResearchCorpus) - J-ResearchCorpusUpdate:2024/3/16言語処理学会第30回年次大会(NLP2024)を含む、論文 1,343 本のデータを追加2024/2/25言語処理学会誌「自然言語処理」のうち CC-BY-4.0 で公開されている論文 360 本のデータを追加概要CC-BY-* ライセンスで公開されている日本語論文や学会誌等から抜粋した高品質なテキストのデータセットです。
 * 📥 81 [izumi-lab/llm-japanese-dataset-vanilla](https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset-vanilla) - llm-japanese-dataset-vanillaLLM構築用の日本語チャットデータセットizumi-lab/llm-japanese-dataset から，日英翻訳のデータセット等を抜いたものです．
 * 📥 81 [Verah/JParaCrawl-Filtered-English-Japanese-Parallel-Corpus](https://huggingface.co/datasets/Verah/JParaCrawl-Filtered-English-Japanese-Parallel-Corpus) - IntroductionThis is a LLM-filtered set of the first 1M rows from ntt's JParaCrawl v3 large English-Japanese parallel corpus.
 * 📥 74 [SakanaAI/JA-VG-VQA-500](https://huggingface.co/datasets/SakanaAI/JA-VG-VQA-500) - JA-VG-VQA-500Dataset DescriptionJA-VG-VQA-500 is a 500-sample subset of Japanese Visual Genome VQA dataset.
 * 📥 57 [range3/wikipedia-ja-20230101](https://huggingface.co/datasets/range3/wikipedia-ja-20230101) - range3/wikipedia-ja-20230101This dataset consists of a parquet file from the wikipedia dataset with only Japanese data extracted.
 * 📥 56 [kunishou/jp-effective-instructions](https://huggingface.co/datasets/kunishou/jp-effective-instructions) - oasst1-89k-ja , databricks-dolly-15k-ja , hh-rlhf-49k-ja の中から JGLUE（ JcommonsenseQA , MARC-ja , JSQuAD ）の観点で高品質なデータセットに絞り込んだデータセットです。
 * 📥 55 [llm-book/jsnli](https://huggingface.co/datasets/llm-book/jsnli) - JSNLI Version 1.1 のデータセットのうち、フィルタリング後の訓練セット (train_w_filtering)
 * 📥 53 [hpprc/jsick](https://huggingface.co/datasets/hpprc/jsick) - Dataset.
 * 📥 52 [svjack/pokemon-blip-captions-en-ja](https://huggingface.co/datasets/svjack/pokemon-blip-captions-en-ja) - Dataset used to train Pokémon text to image model, add a Japanese Column of Pokémon BLIP captionsBLIP generated captions for Pokémon images from Few Shot Pokémon dataset introduced by Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis (FastGAN).
 * 📥 52 [GENIAC-Team-Ozaki/chatbot-arena-ja-calm2-7b-chat-experimental_deduped](https://huggingface.co/datasets/GENIAC-Team-Ozaki/chatbot-arena-ja-calm2-7b-chat-experimental_deduped) - chatbot-arena-ja-calm2-7b-chatからpromptが一致するデータを削除したデータセットです。
 * 📥 51 [hpprc/jawiki](https://huggingface.co/datasets/hpprc/jawiki) - JaWikiWikipediaのHTML形式のダンプファイルから抽出したテキストデータセットです。
 * 📥 51 [hpprc/mmarco-ja](https://huggingface.co/datasets/hpprc/mmarco-ja) - mmarcoデータセットのquery--passageのペアについて、queryをkeyとして重複を削除したデータセットです。
 * 📥 46 [llm-book/aio-retriever](https://huggingface.co/datasets/llm-book/aio-retriever) - GitHub リポジトリ cl-tohoku/quiz-datasets で公開されているデータセットを利用しています。
 * 📥 46 [globis-university/aozorabunko-chats](https://huggingface.co/datasets/globis-university/aozorabunko-chats) - OverviewThis dataset is of conversations extracted from Aozora Bunko (青空文庫), which collects public-domain books in Japan, using a simple heuristic approach.[For Japanese] 日本語での概要説明を Qiita に記載しました: https://qiita.com/akeyhero/items/b53eae1c0bc4d54e321fMethodFirst, lines surrounded by quotation mark pairs (「」) are extracted as utterances from the text field of globis-university/aozorabunko-clean.
 * 📥 45 [kunishou/oasst1-chat-44k-ja](https://huggingface.co/datasets/kunishou/oasst1-chat-44k-ja) - oasst1-89k-jaをチャット形式に変換したデータセットになります。
 * 📥 42 [stockmark/ner-wikipedia-dataset](https://huggingface.co/datasets/stockmark/ner-wikipedia-dataset) - Wikipediaを用いた日本語の固有表現抽出データセットGitHub: https://github.com/stockmarkteam/ner-wikipedia-dataset/LICENSE: CC-BY-SA 3.0Developed by Stockmark Inc.
 * 📥 42 [dichmau/ja_vi_translation](https://huggingface.co/datasets/dichmau/ja_vi_translation) - Japanese-Vietnamese Translated Sentence Pairs.
 * 📥 38 [Fhrozen/CABankSakuraCHJP](https://huggingface.co/datasets/Fhrozen/CABankSakuraCHJP) - CABank Japanese CallHome CorpusParticipants:   120Type of Study:  phone callLocation:   United StatesMedia type: audioDOI:    doi:10.21415/T5H59VWeb: https://ca.talkbank.org/access/CallHome/jpn.htmlCitation informationSome citation here.
 * 📥 33 [y2lan/japan-law](https://huggingface.co/datasets/y2lan/japan-law) - Japanese LawsThis dataset comprises 8.75K law records retrieved from the official Japanese government website e-Gov. Each entry furnishes comprehensive details about a particular law, encapsulating its number, title, unique ID, the date it came into effect, and its complete text.
 * 📥 32 [kunishou/amenokaku-code-instruct](https://huggingface.co/datasets/kunishou/amenokaku-code-instruct) - Amenokaku-Code-InstructUpdate:2023/12/27データセットに JaxTon , プロになるJava のコードデータ 180 レコードを追加しました。
 * 📥 32 [hotchpotch/JaCWIR](https://huggingface.co/datasets/hotchpotch/JaCWIR) - JaCWIR: Japanese Casual Web IR - 日本語情報検索評価のための小規模でカジュアルなWebタイトルと概要のデータセット近年、大規模言語モデル（LLM）の台頭により、一般的な日本語を用いた自然な検索クエリで質問するユースケースが増えています。
 * 📥 29 [sudy-super/CoTangent](https://huggingface.co/datasets/sudy-super/CoTangent) - CoTangentは人手で作成された高品質でクリーンな100セットの日本語CoT用データセットです。
 * 📥 28 [llm-book/ner-wikinews-dataset](https://huggingface.co/datasets/llm-book/ner-wikinews-dataset) - 固有表現ラベルはllm-book/ner-wikipedia-datasetと同様のものを採用しており、全部で8種類 (人名、法人名、地名、製品名、政治的組織名、施設名、その他の組織名、イベント名)あります。
 * 📥 28 [turing-motors/Japanese-Heron-Bench](https://huggingface.co/datasets/turing-motors/Japanese-Heron-Bench) - Japanese-Heron-BenchDataset DescriptionJapanese-Heron-Bench is a benchmark for evaluating Japanese VLMs (Vision-Language Models).
 * 📥 26 [bclavie/mmarco-japanese-hard-negatives](https://huggingface.co/datasets/bclavie/mmarco-japanese-hard-negatives) - [Under Construction]This is a repository containing all the queries from the Japanese part of the MMarco dataset, the multilingual version of the MSMarco dataset.
 * 📥 25 [polm-stability/jblimp](https://huggingface.co/datasets/polm-stability/jblimp) - JBLiMPThis is the data from "JBLiMP: Japanese Benchmark of Linguistic Minimal Pairs" (Someya and Oseki, 2023).
 * 📥 23 [llm-book/jawiki-paragraphs](https://huggingface.co/datasets/llm-book/jawiki-paragraphs) - GitHub リポジトリ singletongue/wikipedia-utils で公開されているデータセットを利用しています。
 * 📥 23 [Verah/tatoeba_dedupe_en-jp_2024-March-01](https://huggingface.co/datasets/Verah/tatoeba_dedupe_en-jp_2024-March-01) - English - Japanese pairs taken from https://tatoeba.org/en/downloads and then deduplicated.
 * 📥 21 [aixsatoshi/Swallow-MX-chatbot-DPO](https://huggingface.co/datasets/aixsatoshi/Swallow-MX-chatbot-DPO) - Chatbot Arena Conversationsの質問文から、aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2を使用して応答文を作成しました質問文は、以下のモデルのPrompt部分を使用しましたChatbot Arena Conversations JA (calm2)以下引用です。
 * 📥 20 [oshizo/japanese-wikipedia-paragraphs](https://huggingface.co/datasets/oshizo/japanese-wikipedia-paragraphs) - A slightly modified version of the parsing and chunking method for singletongue/wikipedia-utils.
 * 📥 20 [yulanfmy/databricks-qa-ja](https://huggingface.co/datasets/yulanfmy/databricks-qa-ja) - データセット概要手動で作成したDatabricksに関する質問と回答ペアの日本語データセットです。
 * 📥 19 [saldra/sakura_japanese_dataset](https://huggingface.co/datasets/saldra/sakura_japanese_dataset) - Sakura_dataset商用利用可能な超小規模高品質日本語データセット。categoryは以下commonsense_qa: 常識問題Calc-ape210k: 数学問題japanese-commonsense-openqa: 日本の常識問題(自作)下記データセットを使用しています。commonsense_qaMU-NLPC/Calc-ape210kLICENSEThis dataset is licensed under Database Contents License (DbCL)
 * 📥 17 [hpprc/mqa-ja](https://huggingface.co/datasets/hpprc/mqa-ja) - mqaデータセットのquery--passageのペアについて重複を削除したデータセットです。
 * 📥 17 [RyokoAI/Syosetu711K](https://huggingface.co/datasets/RyokoAI/Syosetu711K) - Not all information here may be accurate or accessible.
 * 📥 15 [sakusakumura/databricks-dolly-15k-ja-scored](https://huggingface.co/datasets/sakusakumura/databricks-dolly-15k-ja-scored) - For the English version, please click here.
 * 📥 14 [baobab-trees/wikipedia-human-retrieval-ja](https://huggingface.co/datasets/baobab-trees/wikipedia-human-retrieval-ja) - Japanese Wikipedia Human Retrieval datasetThis is a Japanese question answereing dataset with retrieval on Wikipedia articlesby trained human workers.
 * 📥 14 [CausalLM/GPT-4-Self-Instruct-Japanese](https://huggingface.co/datasets/CausalLM/GPT-4-Self-Instruct-Japanese) - Sorry, it's no longer available on Hugging Face.
 * 📥 14 [kunishou/ApolloCorpus-ja](https://huggingface.co/datasets/kunishou/ApolloCorpus-ja) - ApolloCorpus-ja概要多言語医療データセットの ApolloCorpus を日本語に自動翻訳した 525k の指示チューニングデータセットになります。
 * 📥 14 [toshi456/llava-bench-in-the-wild-ja](https://huggingface.co/datasets/toshi456/llava-bench-in-the-wild-ja) - This dataset is the data that corrected the translation errors and untranslated data of the Japanese data in MBZUAI/multilingual-llava-bench-in-the-wild.
 * 📥 14 [aixsatoshi/cosmopedia-japanese-100k](https://huggingface.co/datasets/aixsatoshi/cosmopedia-japanese-100k) - cosmopedia-japanese-20kのデータに、kunishou様から20k-100kをご提供いただけることになり100kまで拡大しました。
