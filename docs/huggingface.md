# awesome-japanese-nlp-resources

This page lists the models and datasets registered with [Haggingface](https://huggingface.co) that are specific to Japanese NLP. At present, 459 models and 101 datasets are listed.

# Contents

 * [Models](#models)
 * [Datasets](#datasets)

## Models

This list is sorted by downloads as of May 13, 2024.

 * 📥 26775140 [tohoku-nlp/bert-base-japanese](https://huggingface.co/tohoku-nlp/bert-base-japanese) - BERT base Japanese (IPA dictionary)This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the IPA dictionary, followed by the WordPiece subword tokenization.
 * 📥 1213634 [sonoisa/sentence-bert-base-ja-mean-tokens-v2](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens-v2) - This is a Japanese sentence-BERT model. 日本語用Sentence-BERTモデル（バージョン2）です。バージョン1よりも良いロス関数であるMultipleNegativesRankingLossを用いて学習した改良版です。手元の非公開データセットでは、バージョン1よりも1.5〜2ポイントほど精度が高い結果が得られました。事前学習済みモデルとしてcl-tohoku/bert-base-japanese-whole-word-maskingを利用しました。従って、推論の実行にはfugashiとipadicが必要です（pip install fugashi ipadic）。旧バージョンの解説https://qiita.com/sonoisa/items/1df94d0a98cd4f209051モデル名を"sonoisa/sentence-bert-base-ja-mean-tokens-v2"に書き換えれば、本モデルを利用した挙動になります。使い方from transformers import BertJapaneseTokenizer, BertModelimport to
 * 📥 1046750 [tsmatz/xlm-roberta-ner-japanese](https://huggingface.co/tsmatz/xlm-roberta-ner-japanese) - xlm-roberta-ner-japanese(Japanese caption : 日本語の固有表現抽出のモデル)This model is a fine-tuned version of xlm-roberta-base (pre-trained cross-lingual RobertaModel) trained for named entity recognition (NER) token classification. The model is fine-tuned on NER dataset provided by Stockmark Inc, in which data is collected from Japanese Wikipedia articles.
 * 📥 516521 [jonatasgrosman/wav2vec2-large-xlsr-53-japanese](https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-japanese) - Fine-tuned XLSR-53 large model for speech recognition in JapaneseFine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the train and validation splits of Common Voice 6.1, CSS10 and JSUT.When using this model, make sure that your speech input is sampled at 16kHz. This model has been fine-tuned thanks to the GPU credits generously given by the OVHcloud :)
 * 📥 509124 [augmxnt/shisa-gamma-7b-v1](https://huggingface.co/augmxnt/shisa-gamma-7b-v1) - shisa-gamma-7b-v1For more information see our main Shisa 7B modelWe applied a version of our fine-tune data set onto Japanese Stable LM Base Gamma 7B and it performed pretty well, just sharing since it might be of interest. Check out our JA MT-Bench results.
 * 📥 420870 [sonoisa/sentence-luke-japanese-base-lite](https://huggingface.co/sonoisa/sentence-luke-japanese-base-lite) - This is a Japanese sentence-LUKE model. 日本語用Sentence-LUKEモデルです。日本語Sentence-BERTモデルと同一のデータセットと設定で学習しました。手元の非公開データセットでは、日本語Sentence-BERTモデルと比べて定量的な精度が同等〜0.5pt程度高く、定性的な精度は本モデルの方が高い結果でした。事前学習済みモデルとしてstudio-ousia/luke-japanese-base-liteを利用させていただきました。推論の実行にはSentencePieceが必要です（pip install sentencepiece）。使い方from transformers import MLukeTokenizer, LukeModelimport torchclass SentenceLukeJapanese:def __init__(self, model_name_or_path, device=None):self.tokenizer = MLukeTokenizer.from_pretrained(model_name_or_path)self
 * 📥 185829 [tohoku-nlp/bert-base-japanese-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-whole-word-masking) - BERT base Japanese (IPA dictionary, whole word masking enabled)This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the IPA dictionary, followed by the WordPiece subword tokenization.
 * 📥 125132 [kha-white/manga-ocr-base](https://huggingface.co/kha-white/manga-ocr-base) - Manga OCROptical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework.
 * 📥 123052 [hotchpotch/japanese-reranker-cross-encoder-xsmall-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-xsmall-v1) - hotchpotch/japanese-reranker-cross-encoder-xsmall-v1日本語で学習させた Reranker (CrossEncoder) シリーズです。モデル名layershidden_sizehotchpotch/japanese-reranker-cross-encoder-xsmall-v16384hotchpotch/japanese-reranker-cross-encoder-small-v112384hotchpotch/japanese-reranker-cross-encoder-base-v112768hotchpotch/japanese-reranker-cross-encoder-large-v1241024hotchpotch/japanese-bge-reranker-v2-m3-v1241024Reranker についてや、技術レポート・評価等は以下を参考ください。日本語最高性能のRerankerをリリース / そもそも Reranker とは?日本語 Reranker 作成のテクニカルレポート使い方SentenceTransformersfr
 * 📥 110673 [tohoku-nlp/bert-base-japanese-char-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v2) - BERT base Japanese (character-level tokenization with whole word masking, jawiki-20200831)This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the Unidic 2.1.2 dictionary (available in unidic-lite package), followed by character-level tokenization.
 * 📥 104772 [tohoku-nlp/bert-base-japanese-char](https://huggingface.co/tohoku-nlp/bert-base-japanese-char) - BERT base Japanese (character tokenization)This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the IPA dictionary, followed by character-level tokenization.
 * 📥 77775 [elyza/ELYZA-japanese-Llama-2-7b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-instruct) - ELYZA-japanese-Llama-2-7bModel DescriptionELYZA-japanese-Llama-2-7b は、 Llama2をベースとして日本語能力を拡張するために追加事前学習を行ったモデルです。詳細は Blog記事 を参照してください。Usageimport torchfrom transformers import AutoModelForCausalLM, AutoTokenizerB_INST, E_INST = "[INST]", "[/INST]"B_SYS, E_SYS = "&lt;&lt;SYS&gt;&gt;\n", "\n&lt;&lt;/SYS&gt;&gt;\n\n"DEFAULT_SYSTEM_PROMPT = "あなたは誠実で優秀な日本人のアシスタントです。"text = "クマが海辺に行ってアザラシと友達になり、最終的には家に帰るというプロットの短編小説を書いてください。"model_name = "elyza/ELYZA-japanese-Llama-2-7b-instruct"tokenizer = AutoTokenizer.from_pre
 * 📥 60368 [sonoisa/sentence-bert-base-ja-mean-tokens](https://huggingface.co/sonoisa/sentence-bert-base-ja-mean-tokens) - This is a Japanese sentence-BERT model. 日本語用Sentence-BERTモデル（バージョン1）です。※: 精度が1.5ポイントほど向上したバージョン2モデルもあります。解説https://qiita.com/sonoisa/items/1df94d0a98cd4f209051使い方from transformers import BertJapaneseTokenizer, BertModelimport torchclass SentenceBertJapanese:def __init__(self, model_name_or_path, device=None):self.tokenizer = BertJapaneseTokenizer.from_pretrained(model_name_or_path)self.model = BertModel.from_pretrained(model_name_or_path)self.model.eval()if device is None:device = "cuda" if torch.cuda.is_ava
 * 📥 59719 [colorfulscoop/sbert-base-ja](https://huggingface.co/colorfulscoop/sbert-base-ja) - Sentence BERT base Japanese modelThis repository contains a Sentence BERT base model for Japanese. Pretrained modelThis model utilizes a Japanese BERT model colorfulscoop/bert-base-ja v1.0 released under Creative Commons Attribution-ShareAlike 3.0 as a pretrained model.
 * 📥 48568 [llm-book/bert-base-japanese-v3-ner-wikipedia-dataset](https://huggingface.co/llm-book/bert-base-japanese-v3-ner-wikipedia-dataset) - llm-book/bert-base-japanese-v3-ner-wikipedia-dataset「大規模言語モデル入門」の第6章で紹介している固有表現認識のモデルです。cl-tohoku/bert-base-japanese-v3をllm-book/ner-wikipedia-datasetでファインチューニングして構築されています。関連リンクGitHubリポジトリColabノートブックデータセット大規模言語モデル入門（Amazon.co.jp）大規模言語モデル入門（gihyo.jp）使い方from transformers import pipelinefrom pprint import pprintner_pipeline = pipeline(model="llm-book/bert-base-japanese-v3-ner-wikipedia-dataset",aggregation_strategy="simple",)text = "大谷翔平は岩手県水沢市出身のプロ野球選手"# text中の固有表現を抽出pprint(ner_pipeline(text)) #
 * 📥 36492 [ku-nlp/deberta-v2-base-japanese](https://huggingface.co/ku-nlp/deberta-v2-base-japanese) - Model Card for Japanese DeBERTa V2 baseModel descriptionThis is a Japanese DeBERTa V2 base model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained('ku-nlp/deberta-v2-base-japanese')model = AutoModelForMaskedLM.from_pretrained('ku-nlp/deberta-v2-base-japanese')sentence = '京都 大学 で 自然 言語
 * 📥 35226 [sazyou-roukaku/BracingEvoMix](https://huggingface.co/sazyou-roukaku/BracingEvoMix) - License:CreativeML Open RAIL-MAdditional Copyright: sazyou_roukaku (TwitterID @sazyou_roukaku) as of May 31, 2023このモデルは『CreativeML Open RAIL-M』でLicenseそのものに変更はありません。 しかし追加著作者として鎖城郎郭の名前が追加されています。
 * 📥 27874 [setu4993/LaBSE](https://huggingface.co/setu4993/LaBSE) - LaBSEModel descriptionLanguage-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling.
 * 📥 25938 [tohoku-nlp/bert-base-japanese-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-v3) - BERT base Japanese (unidic-lite with whole word masking, CC-100 and jawiki-20230102)This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the Unidic 2.1.2 dictionary (available in unidic-lite package), followed by the WordPiece subword tokenization.
 * 📥 25928 [pkshatech/GLuCoSE-base-ja](https://huggingface.co/pkshatech/GLuCoSE-base-ja) - GLuCoSE (General Luke-based Contrastive Sentence Embedding)-base-Japanese日本語のREADME/Japanese READMEGLuCoSE (General LUke-based COntrastive Sentence Embedding, "glucose") is a Japanese text embedding model based on LUKE. In order to create a general-purpose, user-friendly Japanese text embedding model, GLuCoSE has been trained on a mix of web data and various datasets associated with natural language inference and search.
 * 📥 17520 [cyberagent/open-calm-medium](https://huggingface.co/cyberagent/open-calm-medium) - OpenCALM-MediumModel DescriptionOpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by CyberAgent, Inc.
 * 📥 14498 [tokyotech-llm/Swallow-70b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-instruct-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data. The tuned versions use supervised fine-tuning (SFT).Links to other models can be found in the index.
 * 📥 13867 [rinna/japanese-roberta-base](https://huggingface.co/rinna/japanese-roberta-base) - japanese-roberta-baseThis repository provides a base-sized Japanese RoBERTa model. The model was trained using code from Github repository rinnakk/japanese-pretrained-models by rinna Co., Ltd.
 * 📥 13658 [tohoku-nlp/bert-base-japanese-v2](https://huggingface.co/tohoku-nlp/bert-base-japanese-v2) - BERT base Japanese (unidic-lite with whole word masking, jawiki-20200831)This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the Unidic 2.1.2 dictionary (available in unidic-lite package), followed by the WordPiece subword tokenization.
 * 📥 13281 [tokyotech-llm/Swallow-7b-NVE-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-NVE-instruct-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data. The tuned versions use supervised fine-tuning (SFT).Links to other models can be found in the index.
 * 📥 12699 [rinna/japanese-clip-vit-b-16](https://huggingface.co/rinna/japanese-clip-vit-b-16) - rinna/japanese-clip-vit-b-16This is a Japanese CLIP (Contrastive Language-Image Pre-Training) model trained by rinna Co., Ltd..Please see japanese-clip for the other available models. How to use the modelInstall package$ pip install git+https://github.com/rinnakk/japanese-clip.gitRunimport ioimport requestsfrom PIL import Imageimport torchimport japanese_clip as ja_clipdevice = "cuda" if torch.cuda.is_available() else "cpu"model, preprocess = ja_clip.load("rinna/japanese-clip-vit-b-16", cache_dir="/tmp/japan
 * 📥 12559 [sazyou-roukaku/chilled_remix](https://huggingface.co/sazyou-roukaku/chilled_remix) - 【告知】chilled_remix及びreversemixは2023年5月21日にVersion変更を行い、v2へ移行いたしました。 伴いv1は削除致しました。
 * 📥 11143 [reazon-research/reazonspeech-nemo-v2](https://huggingface.co/reazon-research/reazonspeech-nemo-v2) - reazonspeech-nemo-v2reazonspeech-nemo-v2 is an automatic speech recognition model trainedon ReazonSpeech v2.0 corpus. This model supports inference of long-form Japanese audio clips up toseveral hours.
 * 📥 10859 [tokyotech-llm/Swallow-7b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-instruct-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data. The tuned versions use supervised fine-tuning (SFT).Links to other models can be found in the index.
 * 📥 9977 [elyza/ELYZA-japanese-Llama-2-7b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b) - ELYZA-japanese-Llama-2-7bModel DescriptionELYZA-japanese-Llama-2-7b は、 Llama2をベースとして日本語能力を拡張するために追加事前学習を行ったモデルです。詳細は Blog記事 を参照してください。Usageimport torchfrom transformers import AutoModelForCausalLM, AutoTokenizerB_INST, E_INST = "[INST]", "[/INST]"B_SYS, E_SYS = "&lt;&lt;SYS&gt;&gt;\n", "\n&lt;&lt;/SYS&gt;&gt;\n\n"DEFAULT_SYSTEM_PROMPT = "あなたは誠実で優秀な日本人のアシスタントです。"text = "クマが海辺に行ってアザラシと友達になり、最終的には家に帰るというプロットの短編小説を書いてください。"model_name = "elyza/ELYZA-japanese-Llama-2-7b-instruct"tokenizer = AutoTokenizer.from_pre
 * 📥 9885 [Lasorco/lametta](https://huggingface.co/Lasorco/lametta) - このモデルは何？ 個人的な普段遣いのためにマージしたモデルです、癖が強いと思います。
 * 📥 9256 [cyberagent/open-calm-3b](https://huggingface.co/cyberagent/open-calm-3b) - OpenCALM-3BModel DescriptionOpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by CyberAgent, Inc. Usageimport torchfrom transformers import AutoModelForCausalLM, AutoTokenizermodel = AutoModelForCausalLM.from_pretrained("cyberagent/open-calm-3b", device_map="auto", torch_dtype=torch.float16)tokenizer = AutoTokenizer.from_pretrained("cyberagent/open-calm-3b")inputs = tokenizer("AIによって私達の暮らしは、", return_tensors="pt").to(model.device)with torch.no_grad():tokens = mod
 * 📥 9129 [elyza/ELYZA-japanese-Llama-2-13b-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-instruct) - ELYZA-japanese-Llama-2-13bModel DescriptionELYZA-japanese-Llama-2-13b は、 Llama 2をベースとして日本語能力を拡張するために追加事前学習を行ったモデルです。詳細は Blog記事 を参照してください。Usageimport torchfrom transformers import AutoModelForCausalLM, AutoTokenizerB_INST, E_INST = "[INST]", "[/INST]"B_SYS, E_SYS = "&lt;&lt;SYS&gt;&gt;\n", "\n&lt;&lt;/SYS&gt;&gt;\n\n"DEFAULT_SYSTEM_PROMPT = "あなたは誠実で優秀な日本人のアシスタントです。"text = "仕事の熱意を取り戻すためのアイデアを5つ挙げてください。"model_name = "elyza/ELYZA-japanese-Llama-2-13b-instruct"tokenizer = AutoTokenizer.from_pretrained(model_name
 * 📥 9111 [alabnii/jmedroberta-base-sentencepiece-vocab50000](https://huggingface.co/alabnii/jmedroberta-base-sentencepiece-vocab50000) - alabnii/jmedroberta-base-sentencepiece-vocab50000Model descriptionThis is a Japanese RoBERTa base model pre-trained on academic articles in medical sciences collected by Japan Science and Technology Agency (JST).This model is released under the Creative Commons 4.0 International License (CC BY-NC-SA 4.0).ReferenceJa:@InProceedings{sugimoto_nlp2023_jmedroberta,author =    "杉本海人 and 壹岐太一 and 知田悠生 and 金沢輝一 and 相澤彰子",title =     "J{M}ed{R}o{BERT}a: 日本語の医学論文にもとづいた事前学習済み言語モデルの構築と評価",booktitle = "言語処理学会第29回年次大会",y
 * 📥 7983 [cyberagent/open-calm-7b](https://huggingface.co/cyberagent/open-calm-7b) - OpenCALM-7BModel DescriptionOpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by CyberAgent, Inc. Usageimport torchfrom transformers import AutoModelForCausalLM, AutoTokenizermodel = AutoModelForCausalLM.from_pretrained("cyberagent/open-calm-7b", device_map="auto", torch_dtype=torch.float16)tokenizer = AutoTokenizer.from_pretrained("cyberagent/open-calm-7b")inputs = tokenizer("AIによって私達の暮らしは、", return_tensors="pt").to(model.device)with torch.no_grad():tokens = mod
 * 📥 7919 [oshizo/sbert-jsnli-luke-japanese-base-lite](https://huggingface.co/oshizo/sbert-jsnli-luke-japanese-base-lite) - sbert-jsnli-luke-japanese-base-liteThis is a sentence-transformers model: It maps sentences &amp; paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search. The base model is studio-ousia/luke-japanese-base-lite and was trained 1 epoch with shunk031/jsnli.
 * 📥 7884 [elyza/ELYZA-japanese-Llama-2-7b-fast-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast-instruct) - ELYZA-japanese-Llama-2-7bModel DescriptionELYZA-japanese-Llama-2-7b は、 Llama2をベースとして日本語能力を拡張するために追加事前学習を行ったモデルです。 詳細は Blog記事 を参照してください。
 * 📥 7812 [tohoku-nlp/bert-large-japanese](https://huggingface.co/tohoku-nlp/bert-large-japanese) - BERT large Japanese (unidic-lite with whole word masking, jawiki-20200831)This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the Unidic 2.1.2 dictionary (available in unidic-lite package), followed by the WordPiece subword tokenization.
 * 📥 6638 [abeja/gpt-neox-japanese-2.7b](https://huggingface.co/abeja/gpt-neox-japanese-2.7b) - gpt-neox-japanese-2.7bThe open PR is merged on 2022/9/14.You can use this model with v4.23 and higher versions of transformers as follows,pip install transformersThis repository provides a 2.7B-parameter Japanese GPT-NeoX-based model. The model was trained by ABEJA, IncHow to useWhen using pipeline for text generation.from transformers import pipelinegenerator
 * 📥 6491 [stabilityai/japanese-stablelm-base-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-gamma-7b) - Japanese Stable LM Base Gamma 7BModel DescriptionThis is a 7B-parameter decoder-only language model with a focus on maximizing Japanese language modeling performance and Japanese downstream task performance.
 * 📥 6162 [mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf) - ELYZA-japanese-Llama-2-7b-fast-instruct-ggufELYZAさんが公開しているELYZA-japanese-Llama-2-7b-fast-instructのggufフォーマット変換版です。他のモデルはこちら通常版: llama2に日本語のデータセットで学習したモデルmmnga/ELYZA-japanese-Llama-2-7b-ggufmmnga/ELYZA-japanese-Llama-2-7b-instruct-ggufFast版 日本語の語彙を追加してトークンコストを減らし、1.8倍高速化したモデルmmnga/ELYZA-japanese-Llama-2-7b-fast-ggufmmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-ggufCodellama版 GGUFmmnga/ELYZA-japanese-CodeLlama-7b-ggufmmnga/ELYZA-japanese-CodeLlama-7b-instruct-ggufCodellama版 GPTQmmnga/ELYZA-japanese-CodeLlama-
 * 📥 6087 [elyza/ELYZA-japanese-Llama-2-13b-fast-instruct](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-fast-instruct) - ELYZA-japanese-Llama-2-13b-fast-instructModel DescriptionELYZA-japanese-Llama-2-13b は、 Llama 2をベースとして日本語能力を拡張するために追加事前学習を行ったモデルです。詳細は Blog記事 を参照してください。Usageimport torchfrom transformers import AutoModelForCausalLM, AutoTokenizerB_INST, E_INST = "[INST]", "[/INST]"B_SYS, E_SYS = "&lt;&lt;SYS&gt;&gt;\n", "\n&lt;&lt;/SYS&gt;&gt;\n\n"DEFAULT_SYSTEM_PROMPT = "あなたは誠実で優秀な日本人のアシスタントです。"text = "仕事の熱意を取り戻すためのアイデアを5つ挙げてください。"model_name = "elyza/ELYZA-japanese-Llama-2-13b-fast-instruct"tokenizer = AutoTokenizer.from_pr
 * 📥 6051 [mmnga/umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf](https://huggingface.co/mmnga/umiyuki-Japanese-Chat-Umievo-itr001-7b-gguf) - umiyuki-Japanese-Chat-Umievo-itr001-7b-ggufumiyukiさんが公開しているJapanese-Chat-Umievo-itr001-7bのggufフォーマット変換版です。imatrixのデータはTFMC/imatrix-dataset-for-japanese-llmを使用して作成しました。Usagegit clone https://github.com/ggerganov/llama.cpp.gitcd llama.cppmake -j./main -m 'umiyuki-Japanese-Chat-Umievo-itr001-7b-Q4_0.gguf' -p "[INST] 今晩の夕食のレシピを教えて
 * 📥 5823 [cheonboy/sentence_embedding_japanese](https://huggingface.co/cheonboy/sentence_embedding_japanese) - This is a Japanese sentence-LUKE model. 日本語用Sentence-LUKEモデルです。日本語Sentence-BERTモデルと同一のデータセットと設定で学習しました。手元の非公開データセットでは、日本語Sentence-BERTモデルと比べて定量的な精度が同等〜0.5pt程度高く、定性的な精度は本モデルの方が高い結果でした。事前学習済みモデルとしてstudio-ousia/luke-japanese-base-liteを利用させていただきました。推論の実行にはSentencePieceが必要です（pip install sentencepiece）。使い方from transformers import MLukeTokenizer, LukeModelimport torchclass SentenceLukeJapanese:def __init__(self, model_name_or_path, device=None):self.tokenizer = MLukeTokenizer.from_pretrained(model_name_or_path)self
 * 📥 5546 [kotoba-tech/kotoba-whisper-v1.0](https://huggingface.co/kotoba-tech/kotoba-whisper-v1.0) - Kotoba-WhisperKotoba-Whisper is a collection of distilled Whisper models for Japanese ASR, developed through the collaboration bewteenAsahi Ushio and Kotoba Technologies. Following the original work of distil-whisper (Robust Knowledge Distillation via Large-Scale Pseudo Labelling),we employ OpenAI's Whisper large-v3 as the teacher model, and the student model consists the full encoder of theteacher large-v3 model and the decoder with two layers initialized from the first and last layer of the large-v3 model.
 * 📥 5175 [stabilityai/japanese-stablelm-instruct-gamma-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-gamma-7b) - Japanese Stable LM Instruct Gamma 7BModel DescriptionThis is a 7B-parameter decoder-only Japanese language model fine-tuned on instruction-following datasets, built on top of the base model Japanese Stable LM Base Gamma 7B.If you are in search of a smaller model, please check Japanese StableLM-3B-4E1T Instruct.
 * 📥 5171 [megagonlabs/transformers-ud-japanese-electra-base-ginza-510](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-ginza-510) - transformers-ud-japanese-electra-ginza-510 (sudachitra-wordpiece, mC4 Japanese)This is an ELECTRA model pretrained on approximately 200M Japanese sentences extracted from the mC4 and finetuned by spaCy v3 on UD_Japanese_BCCWJ r2.8.The base pretrain model is megagonlabs/transformers-ud-japanese-electra-base-discrimininator. The entire spaCy v3 model is distributed as a python package named ja_ginza_electra from PyPI along with GiNZA v5 which provides some custom pipeline components to recognize the Japanese b
 * 📥 4985 [mmnga/gemma-7b-it-gguf](https://huggingface.co/mmnga/gemma-7b-it-gguf) - gemma-7b-it-ggufgoogleさんが公開しているgemma-7b-itのggufフォーマット変換版です。現在量子化された出力が不安定な問題があるらしくQ8_0を推奨します。gemma : token_embd.weight テンソルに Q8_0 を使用します #5650Licencegemma-terms-of-use 利用規約をご利用前に必ずご確認ください。他のモデルmmnga/codegemma-1.1-7b-it-ggufmmnga/codegemma-1.1-2b-ggufmmnga/gemma-2b-it-ggufmmnga/gemma-7b-it-ggufmmnga/gemma-1.1-7b-it-ggufmmnga/codegemma-7b-it-ggufUsagegit clone https://github.com/ggerganov/llama.cpp.gitcd llama.cppmake -j./main -m 'gemma-7b-it-q4_0.gguf' -p "&lt;start_of_turn&gt;user\n日本の文化を１０個教えて。&lt;end_of_t
 * 📥 4965 [tokyotech-llm/Swallow-13b-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-instruct-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data. The tuned versions use supervised fine-tuning (SFT).Links to other models can be found in the index.
 * 📥 4925 [rinna/youri-7b](https://huggingface.co/rinna/youri-7b) - rinna/youri-7bOverviewWe conduct continual pre-training of llama2-7b on 40B tokens from a mixture of Japanese and English datasets. The continual pre-training significantly improves the model's performance on Japanese tasks.
 * 📥 4801 [elyza/ELYZA-japanese-Llama-2-13b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b) - ELYZA-japanese-Llama-2-13bModel DescriptionELYZA-japanese-Llama-2-13b は、 Llama 2をベースとして日本語能力を拡張するために追加事前学習を行ったモデルです。詳細は Blog記事 を参照してください。Usageimport torchfrom transformers import AutoModelForCausalLM,
 * 📥 4737 [tokyotech-llm/Swallow-70b-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data. The tuned versions use supervised fine-tuning (SFT).Links to other models can be found in the index.
 * 📥 4704 [mmnga/aixsatoshi-Llama-3-8b-Cosmopedia-japanese-gguf](https://huggingface.co/mmnga/aixsatoshi-Llama-3-8b-Cosmopedia-japanese-gguf) - aixsatoshi-Llama-3-8b-Cosmopedia-japanese-ggufaixsatoshiさんが公開しているLlama-3-8b-Cosmopedia-japaneseのggufフォーマット変換版です。imatrixのデータはTFMC/imatrix-dataset-for-japanese-llmを使用して作成しました。他のモデルmmnga/aixsatoshi-Llama-3-8b-Cosmopedia-japanese-ggufmmnga/aixsatoshi-Honyaku-7b-v2-ggufmmnga/aixsatoshi-Honyaku-Multi-Translator-Swallow-ms7b-ggufmmnga/aixsatoshi-Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2-ggufmmnga/aixsatoshi-Mixtral-8x7B-ja-sft-ChatbotArenaJAcalm2-bnb4bitmmnga/aixsatoshi-calm2-7b-chat-7b-moe-ggufUsagegit c
 * 📥 4679 [tokyotech-llm/Swallow-7b-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data. The tuned versions use supervised fine-tuning (SFT).Links to other models can be found in the index.
 * 📥 4609 [mmnga/lightblue-suzume-llama-3-8B-japanese-gguf](https://huggingface.co/mmnga/lightblue-suzume-llama-3-8B-japanese-gguf) - lightblue-suzume-llama-3-8B-japanese-gguflightblueさんが公開しているsuzume-llama-3-8B-japaneseのggufフォーマット変換版です。imatrixのデータはTFMC/imatrix-dataset-for-japanese-llmを使用して作成しました。他のモデルmmnga/lightblue-Karasu-Mixtral-8x22B-v0.1-ggufmmnga/lightblue-suzume-llama-3-8B-multilingual-ggufmmnga/lightblue-suzume-llama-3-8B-japanese-ggufmmnga/lightblue-ao-karasu-72B-ggufmmnga/lightblue-karasu-1.1B-ggufmmnga/lightblue-karasu-7B-chat-plus-unleashed-ggufmmnga/lightblue-qarasu-14B-chat-plus-unleashed-ggufUsagegit clone https://github.com
 * 📥 4589 [mmnga/haqishen-Llama-3-8B-Japanese-Instruct-gguf](https://huggingface.co/mmnga/haqishen-Llama-3-8B-Japanese-Instruct-gguf) - haqishen-Llama-3-8B-Japanese-Instruct-ggufhaqishenさんが公開しているLlama-3-8B-Japanese-Instructのggufフォーマット変換版です。imatrixのデータはTFMC/imatrix-dataset-for-japanese-llmを使用して作成しました。他のモデルmmnga/haqishen-Llama-3-8B-Japanese-Instruct-ggufUsagegit clone https://github.com/ggerganov/llama.cpp.gitcd llama.cppmake -j./main
 * 📥 4582 [ku-nlp/deberta-v2-tiny-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-tiny-japanese-char-wwm) - Model Card for Japanese character-level DeBERTa V2 tinyModel descriptionThis is a Japanese DeBERTa V2 tiny model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.This model is trained with character-level tokenization and whole word masking. How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained('ku-nlp/deberta-v2-tiny-japanese-char-wwm')m
 * 📥 4535 [stabilityai/japanese-stable-clip-vit-l-16](https://huggingface.co/stabilityai/japanese-stable-clip-vit-l-16) - By downloading, using, or distributing any portion or element of this model, you agree to be bound by the agreement described in the LICENSE file.
 * 📥 4488 [elyza/ELYZA-japanese-Llama-2-13b-fast](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-13b-fast) - ELYZA-japanese-Llama-2-13b-fastModel DescriptionELYZA-japanese-Llama-2-13b は、 Llama 2をベースとして日本語能力を拡張するために追加事前学習を行ったモデルです。詳細は Blog記事 を参照してください。Usageimport torchfrom transformers import AutoModelForCausalLM,
 * 📥 4444 [bclavie/JaColBERT](https://huggingface.co/bclavie/JaColBERT) - このドキュメントの日本語版はまだ作成中です。申し訳ありません。IntroDetailed report in the arXiv ReportIf you just want to check out how to use the model, please check out the Usage section below!Welcome to JaColBERT version 1, the initial release of JaColBERT, a Japanese-only document retrieval model based on ColBERT.It outperforms previous common Japanese models used for document retrieval, and gets close to the performance of multilingual models, despite the evaluation datasets being out-of-domain for our models but in-domain for multi
 * 📥 4399 [tohoku-nlp/bert-base-japanese-char-whole-word-masking](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-whole-word-masking) - BERT base Japanese (character tokenization, whole word masking enabled)This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the IPA dictionary, followed by character-level tokenization.
 * 📥 4316 [elyza/ELYZA-japanese-Llama-2-7b-fast](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast) - ELYZA-japanese-Llama-2-7bModel DescriptionELYZA-japanese-Llama-2-7b は、 Llama2をベースとして日本語能力を拡張するために追加事前学習を行ったモデルです。詳細は Blog記事 を参照してください。Usageimport torchfrom transformers import AutoModelForCausalLM, AutoTokenizerB_INST, E_INST = "[INST]", "[/INST]"B_SYS, E_SYS = "&lt;&lt;SYS&gt;&gt;\n", "\n&lt;&lt;/SYS&gt;&gt;\n\n"DEFAULT_SYSTEM_PROMPT = "あなたは誠実で優秀な日本人のアシスタントです。"text = "クマが海辺に行ってアザラシと友達になり、最終的には家に帰るというプロットの短編小説を書いてください。"model_name = "elyza/ELYZA-japanese-Llama-2-7b-instruct"tokenizer = AutoTokenizer.from_pre
 * 📥 4296 [rinna/japanese-gpt2-small](https://huggingface.co/rinna/japanese-gpt2-small) - japanese-gpt2-smallThis repository provides a small-sized Japanese GPT-2 model. The model was trained using code from Github repository rinnakk/japanese-pretrained-models by rinna Co., Ltd.
 * 📥 4115 [KoichiYasuoka/bert-base-japanese-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-upos) - bert-base-japanese-uposModel DescriptionThis is a BERT model pre-trained on Japanese Wikipedia texts for POS-tagging and dependency-parsing, derived from bert-base-japanese-char-extended. Every short-unit-word is tagged by UPOS (Universal Part-Of-Speech).How to Useimport torchfrom transformers import AutoTokenizer,AutoModelForTokenClassificationtokenizer=AutoTokenizer.from_pretrained("KoichiYasuoka/bert-base-japanese-upos")model=AutoModelForTokenClassification.from_pretrained("KoichiYasuoka/bert-base-japane
 * 📥 4090 [sonoisa/t5-base-japanese](https://huggingface.co/sonoisa/t5-base-japanese) - 日本語T5事前学習済みモデルThis is a T5 (Text-to-Text Transfer Transformer) model pretrained on Japanese corpus. 次の日本語コーパス（約100GB）を用いて事前学習を行ったT5 (Text-to-Text Transfer Transformer) モデルです。
 * 📥 3879 [cyberagent/open-calm-large](https://huggingface.co/cyberagent/open-calm-large) - OpenCALM-LargeModel DescriptionOpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by CyberAgent, Inc.
 * 📥 3762 [Helsinki-NLP/opus-tatoeba-en-ja](https://huggingface.co/Helsinki-NLP/opus-tatoeba-en-ja) - en-jasource group: Englishtarget group: JapaneseOPUS readme: eng-jpnmodel: transformer-alignsource language(s): engtarget language(s): jpnmodel: transformer-alignpre-processing: normalization + SentencePiece (spm32k,spm32k)download original weights: opus+bt-2021-04-10.ziptest set translations: opus+bt-2021-04-10.test.txttest set scores: opus+bt-2021-04-10.eval.txtBenchmarkstestsetBLEUchr-F#sent#wordsBPTatoeba-test.eng-jpn15.20.25810000992061.000System Info:hf_name: en-jasource_languages: engtarget_languages
 * 📥 3759 [FINGU-AI/FinguAI-Chat-v1](https://huggingface.co/FINGU-AI/FinguAI-Chat-v1) - FINGU-AI/FinguAI-Chat-v1OverviewThe FINGU-AI/FinguAI-Chat-v1 model offers a specialized curriculum tailored to English, Korean, and Japanese speakers interested in finance, investment, and legal frameworks. It aims to enhance language proficiency while providing insights into global finance markets and regulatory landscapes.
 * 📥 3720 [den2nova/FlexDreamHK](https://huggingface.co/den2nova/FlexDreamHK) - 🎈 FlexDreamHKFlexDreamHKはリークされたNovelAIモデルの入っていない、あるいはそのリスクを可能な限り低くしたモデルを目指して作成しました。 モデル名はマージに使用したモデルたちに敬意を表し、主要なモデル名を組み合わせて命名しています。
 * 📥 3582 [rinna/bilingual-gpt-neox-4b](https://huggingface.co/rinna/bilingual-gpt-neox-4b) - bilingual-gpt-neox-4bOverviewThis repository provides an English-Japanese bilingual GPT-NeoX model of 3.8 billion parameters. LibraryThe model was trained using code based on EleutherAI/gpt-neox.
 * 📥 3507 [rinna/japanese-hubert-base](https://huggingface.co/rinna/japanese-hubert-base) - rinna/japanese-hubert-baseOverviewThis is a Japanese HuBERT Base model trained by rinna Co., Ltd.Model summaryThe model architecture is the same as the original HuBERT Base model, which contains 12 transformer layers with 12 attention heads. The model was trained using code from the official repository, and the detailed training configuration can be found in the same repository and the original paper.
 * 📥 3472 [pkshatech/simcse-ja-bert-base-clcmlp](https://huggingface.co/pkshatech/simcse-ja-bert-base-clcmlp) - Japanese SimCSE (BERT-base)日本語のREADME/Japanese READMEsummarymodel name: pkshatech/simcse-ja-bert-base-clcmlpThis is a Japanese SimCSE model. You can easily extract sentence embedding representations from Japanese sentences.
 * 📥 3388 [augmxnt/shisa-base-7b-v1](https://huggingface.co/augmxnt/shisa-base-7b-v1) - shisa-base-7b-v1shisa-base-7b-v1 takes Mistral 7B and adds an additional 8B tokens of primarily Japanese pre-training. Japanese tokens were sourced from MADLAD-400, using DSIR, along with 10% English tokens sampled from a mix of MADLAD-400 EN and various open datasources added in to prevent catastrophic forgetting.
 * 📥 3350 [line-corporation/line-distilbert-base-japanese](https://huggingface.co/line-corporation/line-distilbert-base-japanese) - LINE DistilBERT JapaneseThis is a DistilBERT model pre-trained on 131 GB of Japanese web text.
 * 📥 3220 [augmxnt/shisa-7b-v1](https://huggingface.co/augmxnt/shisa-7b-v1) - Shisa 7BShisa 7B (shisa-7b-v1) is a bilingual Japanese and English (JA/EN) general-purpose chat model that aims to achieve strong Japanese language performance while retaining robust English capabilities, using a synthetic-data driven approach. This model is based on Mistral 7B with a custom JA-optimized extended tokenizer that is &gt;2X more efficient in Japanese than Mistral's original tokenizer.
 * 📥 3117 [cyberagent/calm2-7b](https://huggingface.co/cyberagent/calm2-7b) - CyberAgentLM2-7B (CALM2-7B)Model DescriptionCyberAgentLM2 is a decoder-only language model pre-trained on the 1.3T tokens of publicly available Japanese and English datasets. Variant: CyberAgentLM2-ChatRequirementstransformers &gt;= 4.34.1accelerateUsageimport transformersfrom transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamerassert transformers.__version__ &gt;= "4.34.1"model = AutoModelForCausalLM.from_pretrained("cyberagent/calm2-7b", device_map="auto", torch_dtype="auto")tokenizer = Au
 * 📥 3094 [OrionStarAI/Orion-14B-Chat](https://huggingface.co/OrionStarAI/Orion-14B-Chat) - Orion-14B🌐English | 🇨🇳中文 | 🇯🇵日本語 | 🇰🇷한국어🤗 HuggingFace Mainpage | 🤖 ModelScope Mainpage🎬 HuggingFace Demo | 🎫 ModelScope Demo😺 GitHub📖 Tech ReportTable of Contents📖 Model Introduction🔗 Model Download🔖 Model Benchmark📊 Model Inference📜 Declarations &amp; License🥇 Company Introduction1. Model IntroductionOrion-14B series models are open-source multilingual large language models trained from scratch by OrionStarAI.  
 * 📥 3081 [ku-nlp/deberta-v2-tiny-japanese](https://huggingface.co/ku-nlp/deberta-v2-tiny-japanese) - Model Card for Japanese DeBERTa V2 tinyModel descriptionThis is a Japanese DeBERTa V2 tiny model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained('ku-nlp/deberta-v2-tiny-japanese')model = AutoModelForMaskedLM.from_pretrained('ku-nlp/deberta-v2-tiny-japanese')sentence = '京都 大学 で 自然 言語
 * 📥 3058 [ku-nlp/deberta-v2-large-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-large-japanese-char-wwm) - Model Card for Japanese character-level DeBERTa V2 largeModel descriptionThis is a Japanese DeBERTa V2 large model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.This model is trained with character-level tokenization and whole word masking. How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained('ku-nlp/deberta-v2-large-japanese-char-wwm
 * 📥 3028 [rinna/bilingual-gpt-neox-4b-8k](https://huggingface.co/rinna/bilingual-gpt-neox-4b-8k) - bilingual-gpt-neox-4b-8kOverviewNotice: This model requires transformers&gt;=4.31.0 to work properly. This repository provides an English-Japanese bilingual GPT-NeoX model of 3.8 billion parameters.
 * 📥 2867 [rinna/japanese-gpt2-xsmall](https://huggingface.co/rinna/japanese-gpt2-xsmall) - japanese-gpt2-xsmallThis repository provides an extra-small-sized Japanese GPT-2 model. The model was trained using code from Github repository rinnakk/japanese-pretrained-models by rinna Co., Ltd.
 * 📥 2783 [rinna/japanese-gpt-neox-3.6b](https://huggingface.co/rinna/japanese-gpt-neox-3.6b) - japanese-gpt-neox-3.6bOverviewThis repository provides a Japanese GPT-NeoX model of 3.6 billion parameters. LibraryThe model was trained using code based on EleutherAI/gpt-neox.
 * 📥 2674 [Aratako/c4ai-command-r-v01-japanese-instruct](https://huggingface.co/Aratako/c4ai-command-r-v01-japanese-instruct) - c4ai-command-r-v01-japanese-instructGGUF版はこちら/Click here for the GGUF version概要CohereForAI/c4ai-command-r-v01を、ichikara-instructionを使って追加で日本語インストラクションチューニングを施したモデルです。学習の設定RunpodでGPUサーバを借り、A6000x4で学習を行いました。主な学習パラメータは以下の通りです。lora_r: 64lisa_alpha: 128lora_dropout: 0.05lora_target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]learning_rate: 2e-5num_train_epochs: 10epochsbatch_size: 50max_seq_length: 2048評価jsquad(jsquad-1.1-0.3, 2-shots)、jcommonsenseqa(jcommonsenseqa-1.1-0
 * 📥 2665 [tohoku-nlp/bert-large-japanese-v2](https://huggingface.co/tohoku-nlp/bert-large-japanese-v2) - BERT large Japanese (unidic-lite with whole word masking, CC-100 and jawiki-20230102)This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the Unidic 2.1.2 dictionary (available in unidic-lite package), followed by the WordPiece subword tokenization.
 * 📥 2661 [rinna/japanese-gpt2-medium](https://huggingface.co/rinna/japanese-gpt2-medium) - japanese-gpt2-mediumThis repository provides a medium-sized Japanese GPT-2 model. The model was trained using code from Github repository rinnakk/japanese-pretrained-models by rinna Co., Ltd.
 * 📥 2658 [megagonlabs/transformers-ud-japanese-electra-base-discriminator](https://huggingface.co/megagonlabs/transformers-ud-japanese-electra-base-discriminator) - transformers-ud-japanese-electra-ginza (sudachitra-wordpiece, mC4 Japanese) - MIYAGINOThis is an ELECTRA model pretrained on approximately 200M Japanese sentences.
 * 📥 2599 [karakuri-ai/karakuri-lm-70b-chat-v0.1](https://huggingface.co/karakuri-ai/karakuri-lm-70b-chat-v0.1) - KARAKURI LMKARAKURI LM is a pretrained language model that builds upon Llama 2.Our model enhances Llama 2's capabilities by incorporating additional Japanese vocabulary and further pretraining on a mixture of Japanese and multilingual corpora. KARAKURI LM Chat is a fine-tuned version of KARAKURI LM, which was trained on a mixture of publicly available and closed datasets using the SteerLM technique.
 * 📥 2525 [mmnga/ELYZA-japanese-Llama-2-7b-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-instruct-gguf) - ELYZA-japanese-Llama-2-7b-instruct-ggufELYZAさんが公開しているELYZA-japanese-Llama-2-7b-instructのggufフォーマット変換版です。他のモデルはこちら通常版: llama2に日本語のデータセットで学習したモデルmmnga/ELYZA-japanese-Llama-2-7b-ggufmmnga/ELYZA-japanese-Llama-2-7b-instruct-ggufFast版 日本語の語彙を追加してトークンコストを減らし、1.8倍高速化したモデルmmnga/ELYZA-japanese-Llama-2-7b-fast-ggufmmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-ggufCodellama版 GGUFmmnga/ELYZA-japanese-CodeLlama-7b-ggufmmnga/ELYZA-japanese-CodeLlama-7b-instruct-ggufCodellama版 GPTQmmnga/ELYZA-japanese-CodeLlama-7b-instruc
 * 📥 2510 [rinna/bilingual-gpt-neox-4b-instruction-ppo](https://huggingface.co/rinna/bilingual-gpt-neox-4b-instruction-ppo) - bilingual-gpt-neox-4b-instruction-ppoOverviewThis repository provides an English-Japanese bilingual GPT-NeoX model of 3.8 billion parameters. The model is based on rinna/bilingual-gpt-neox-4b-instruction-sft and has been aligned to serve as an instruction-following conversational agent.
 * 📥 2394 [stabilityai/japanese-stablelm-instruct-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-beta-70b) - Japanese-StableLM-Instruct-Beta-70BA cute robot wearing a kimono writes calligraphy with one single brush — Stable Diffusion XLModel Descriptionjapanese-stablelm-instruct-beta-70b is a 70B-parameter decoder-only language model based on japanese-stablelm-base-beta-70b and further fine tuned on Databricks Dolly-15k, Anthropic HH, and other public data. This model is also available in a smaller 7b version, or a smaller and faster version with a specialized tokenizer.
 * 📥 2312 [hotchpotch/japanese-reranker-cross-encoder-large-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-large-v1) - hotchpotch/japanese-reranker-cross-encoder-large-v1日本語で学習させた Reranker (CrossEncoder) シリーズです。モデル名layershidden_sizehotchpotch/japanese-reranker-cross-encoder-xsmall-v16384hotchpotch/japanese-reranker-cross-encoder-small-v112384hotchpotch/japanese-reranker-cross-encoder-base-v112768hotchpotch/japanese-reranker-cross-encoder-large-v1241024hotchpotch/japanese-bge-reranker-v2-m3-v1241024Reranker についてや、技術レポート・評価等は以下を参考ください。日本語最高性能のRerankerをリリース / そもそも Reranker とは?日本語 Reranker 作成のテクニカルレポート使い方SentenceTransformersfro
 * 📥 2283 [mmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf) - ELYZA-japanese-Llama-2-13b-fast-instruct-ggufELYZAさんが公開しているELYZA-japanese-Llama-2-13b-fast-instructのggufフォーマット変換版です。他のモデルはこちら通常版: llama2に日本語のデータセットで学習したモデルmmnga/ELYZA-japanese-Llama-2-7b-ggufmmnga/ELYZA-japanese-Llama-2-7b-instruct-ggufFast版 日本語の語彙を追加してトークンコストを減らし、1.8倍高速化したモデルmmnga/ELYZA-japanese-Llama-2-7b-fast-ggufmmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-ggufmmnga/ELYZA-japanese-Llama-2-13b-fast-ggufmmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-ggufCodellama版 GGUFmmnga/ELYZA-japanese-CodeLlama-7b-gg
 * 📥 2238 [SakanaAI/EvoLLM-JP-v1-7B](https://huggingface.co/SakanaAI/EvoLLM-JP-v1-7B) - 🐟 EvoLLM-JP-v1-7B🤗 Models | 📚 Paper | 📝 Blog | 🐦 TwitterEvoLLM-JP-v1-7B is an experimental general-purpose Japanese LLM.
 * 📥 2160 [rinna/japanese-gpt-neox-3.6b-instruction-ppo](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-ppo) - japanese-gpt-neox-3.6b-instruction-ppoOverviewThis repository provides a Japanese GPT-NeoX model of 3.6 billion parameters. The model is based on rinna/japanese-gpt-neox-3.6b-instruction-sft-v2 and has been aligned to serve as an instruction-following conversational agent.
 * 📥 2154 [TKU410410103/hubert-base-japanese-asr](https://huggingface.co/TKU410410103/hubert-base-japanese-asr) - hubert-base-asrThis model is a fine-tuned version of rinna/japanese-hubert-base on the common_voice_11_0 dataset for ASR tasks. This model can only predict Hiragana.
 * 📥 2153 [MCZK/Assistance-7B-GGUF](https://huggingface.co/MCZK/Assistance-7B-GGUF) - Local-Novel-LLM-project様の Assistance をGGUF形式に変換したものです。 K量子化モデルについてもiMatrix適用してあります。
 * 📥 2104 [SakanaAI/EvoVLM-JP-v1-7B](https://huggingface.co/SakanaAI/EvoVLM-JP-v1-7B) - 🐟 EvoVLM-JP-v1-7B🤗 Models | 📚 Paper | 📝 Blog | 🐦 TwitterEvoVLM-JP-v1-7B is an experimental general-purpose Japanese VLM.This model was created using the Evolutionary Model Merge method.
 * 📥 2070 [mmnga/aibuncho-japanese-novel-gpt-j-6b-gguf](https://huggingface.co/mmnga/aibuncho-japanese-novel-gpt-j-6b-gguf) - AIBunCho/japanese-novel-gpt-j-6bAI BunChoさんが公開しているjapanese-novel-gpt-j-6bのgguf変換版です。注意:こちらはブランチで試用になります。llama.cpp本家にgptneox, gpt2が実装された時に、このggufファイルが使用できない可能性があります。GitHubリポジトリの readme はこちらUsage (試用)git clone --branch mmnga-dev https://github.com/mmnga/llama.cpp.gitcd llama.cppmake
 * 📥 2031 [TFMC/Japanese-Starling-ChatV-7B-GGUF](https://huggingface.co/TFMC/Japanese-Starling-ChatV-7B-GGUF) - Japanese-Starling-ChatV-7B-GGUFGGUF conversion of "Japanese-Starling-ChatV-7B""Japanese-Starling-ChatV-7B" is a Japanese chat model built on top of "chatntq-ja-7b-v1.0", originally based on Mistral-7B-v0.1.I applied the chat vector acquired by subtracting the weights of Mistral-7B-v0.1 from the weights of "Starling-LM-7B-beta" to this model. このモデルはchatntq-ja-7b-v1.0をベースにした7Bパラメータの日本語チャットモデルです。高性能の英語モデルであるStarling-LM-7B-betaの重みからMistral-7B-v0.1の重みを差し引くことで得たchat vectorを適用しています（ブログ記事）。PerformanceModel(Q8_0 quan
 * 📥 2019 [TKU410410103/wav2vec2-base-japanese-asr](https://huggingface.co/TKU410410103/wav2vec2-base-japanese-asr) - wav2vec2-base-asrThis model is a fine-tuned version of rinna/japanese-wav2vec2-base on the common_voice_11_0 dataset for ASR tasks. This model can only predict Hiragana.
 * 📥 2007 [TKU410410103/hubert-large-japanese-asr](https://huggingface.co/TKU410410103/hubert-large-japanese-asr) - hubert-large-asrThis model is a fine-tuned version of rinna/japanese-hubert-large ASR. Initially fine-tuned on the reazonspeech(small) dataset, it was subsequently further fine-tuned on the common_voice_11_0 dataset for ASR tasks.
 * 📥 1905 [stabilityai/japanese-stablelm-base-beta-70b](https://huggingface.co/stabilityai/japanese-stablelm-base-beta-70b) - Japanese-StableLM-Base-Beta-70BA cute robot wearing a kimono writes calligraphy with one single brush — Stable Diffusion XLModel Descriptionjapanese-stablelm-base-beta-70b is a 70B-parameter decoder-only language model based on Llama-2-70b that has been fine-tuned on a diverse collection of Japanese data, with the intent of maximizing downstream performance on Japanese language tasks. For an instruction-following model, check Japanese-StableLM-Instruct-Beta-70B.
 * 📥 1813 [cyberagent/open-calm-1b](https://huggingface.co/cyberagent/open-calm-1b) - OpenCALM-1BModel DescriptionOpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by CyberAgent, Inc. Usageimport torchfrom transformers import AutoModelForCausalLM, AutoTokenizermodel = AutoModelForCausalLM.from_pretrained("cyberagent/open-calm-1b", device_map="auto", torch_dtype=torch.float16)tokenizer = AutoTokenizer.from_pretrained("cyberagent/open-calm-1b")inputs = tokenizer("AIによって私達の暮らしは、", return_tensors="pt").to(model.device)with torch.no_grad():tokens = mod
 * 📥 1780 [rinna/japanese-cloob-vit-b-16](https://huggingface.co/rinna/japanese-cloob-vit-b-16) - rinna/japanese-cloob-vit-b-16This is a Japanese CLOOB (Contrastive Leave One Out Boost) model trained by rinna Co., Ltd..Please see japanese-clip for the other available models. How to use the modelInstall package$ pip install git+https://github.com/rinnakk/japanese-clip.gitRunimport ioimport requestsfrom PIL import Imageimport torchimport japanese_clip as ja_clipdevice = "cuda" if torch.cuda.is_available() else "cpu"model, preprocess = ja_clip.load("rinna/japanese-cloob-vit-b-16", device=device)tokenizer =
 * 📥 1753 [stockmark/stockmark-13b](https://huggingface.co/stockmark/stockmark-13b) - stockmark/stockmark-13bStockmark-13b is a 13 billion parameter LLM pretrained from scratch based on Japanese corpus of about 220B tokens. This model is developed by Stockmark Inc.
 * 📥 1749 [jarvisx17/japanese-sentiment-analysis](https://huggingface.co/jarvisx17/japanese-sentiment-analysis) - japanese-sentiment-analysisThis model was trained from scratch on the chABSA dataset. It achieves the following results on the evaluation set:Loss: 0.0001Accuracy: 1.0F1: 1.0Model descriptionModel Train for Japanese sentence sentiments.
 * 📥 1696 [Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime](https://huggingface.co/Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime) - このモデルはLuke-japanese-large-liteをファインチューニングしたものです。このモデルは８つの感情（喜び、悲しみ、期待、驚き、怒り、恐れ、嫌悪、信頼）の内、どの感情が文章に含まれているのか分析することができます。このモデルはwrimeデータセット（https://huggingface.co/datasets/shunk031/wrime）を用いて学習を行いました。This model is based on Luke-japanese-large-liteThis model is fine-tuned model which besed on studio-ousia/Luke-japanese-large-lite. This could be able to analyze which emotions (joy or sadness or anticipation or surprise or anger or fear or disdust or trust ) are included.
 * 📥 1689 [OrionStarAI/Orion-14B-Base](https://huggingface.co/OrionStarAI/Orion-14B-Base) - Orion-14B🌐English | 🇨🇳中文 | 🇯🇵日本語 |🇰🇷한국어🤗 HuggingFace Mainpage | 🤖 ModelScope Mainpage🎬 HuggingFace Demo | 🎫 ModelScope Demo😺 GitHub📖 Tech ReportTable of Contents📖 Model Introduction🔗 Model Download🔖 Model Benchmark📊 Model Inference📜 Declarations &amp; License🥇 Company Introduction1. Model IntroductionOrion-14B series models are open-source multilingual large language models trained from scratch by OrionStarAI.  
 * 📥 1591 [mmnga/stockmark-gpt-neox-japanese-1.4b-gguf](https://huggingface.co/mmnga/stockmark-gpt-neox-japanese-1.4b-gguf) - stockmark-gpt-neox-japanese-1.4b-ggufstockmarkさんが公開しているgpt-neox-japanese-1.4bのggufフォーマット変換版です。注意:こちらはブランチで試用になります。llama.cpp本家にgptneoxが実装された時に、このggufファイルが使用できない可能性があります。GitHubリポジトリの readme はこちらUsage (試用)git clone --branch mmnga-dev https://github.com/mmnga/llama.cpp.gitcd llama.cppmake
 * 📥 1573 [stabilityai/japanese-stablelm-instruct-alpha-7b-v2](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b-v2) - Japanese-StableLM-Instruct-Alpha-7B-v2"A parrot able to speak Japanese, ukiyoe, edo period" — Stable Diffusion XLModel Descriptionjapanese-stablelm-instruct-alpha-7b-v2 is a 7B parameter decoder-only language models pre-trained built on top of the Japanese-StableLM-Base-Alpha-7B model and further fine-tuned on various instruction-following datasets. UsageFirst install additional dependencies in requirements.txt:pip install sentencepiece einopsThen start generating text with japanese-stablelm-instruct-alpha-7
 * 📥 1551 [stabilityai/japanese-stablelm-3b-4e1t-instruct](https://huggingface.co/stabilityai/japanese-stablelm-3b-4e1t-instruct) - Japanese StableLM-3B-4E1T InstructModel DescriptionThis is a 3B-parameter decoder-only Japanese language model fine-tuned on instruction-following datasets, built on top of the base model Japanese StableLM-3B-4E1T Base. If you are in search of a larger model, please check Japanese Stable LM Instruct Gamma 7B.Usageimport torchfrom transformers import AutoTokenizer, AutoModelForCausalLMtokenizer = AutoTokenizer.from_pretrained("stabilityai/japanese-stablelm-3b-4e1t-instruct")model = AutoModelForCausalLM.from_p
 * 📥 1549 [Lasorco/lametta_old](https://huggingface.co/Lasorco/lametta_old) - old？ 一度上げたモデルは何らかの問題がない限りいつでも誰でも使えるべきと思うのでここはそんな保管庫です旧モデル、没モデル、メイン外のお遊び雑マージモデルが雑においてあります。
 * 📥 1542 [cyberagent/open-calm-small](https://huggingface.co/cyberagent/open-calm-small) - OpenCALM-SmallModel DescriptionOpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by CyberAgent, Inc.
 * 📥 1527 [rinna/japanese-gpt-1b](https://huggingface.co/rinna/japanese-gpt-1b) - japanese-gpt-1bThis repository provides a 1.3B-parameter Japanese GPT model. The model was trained by rinna Co., Ltd.How to use the modelimport torchfrom transformers import AutoTokenizer, AutoModelForCausalLMtokenizer = AutoTokenizer.from_pretrained("rinna/japanese-gpt-1b", use_fast=False)model = AutoModelForCausalLM.from_pretrained("rinna/japanese-gpt-1b")if torch.cuda.is_available():model = model.to("cuda")text = "西田幾多郎は、"token_ids = tokenizer.encode(text, add_special_tokens=False, return_tensors="pt")wi
 * 📥 1453 [jurabi/bert-ner-japanese](https://huggingface.co/jurabi/bert-ner-japanese) - BERTによる日本語固有表現抽出のモデルBertForTokenClassificationを用いて、日本語の文から固有表現を抽出します。抽出される固有表現のタイプは、以下の8種類です。人名法人名（法人または法人に類する組織）政治的組織名（政治的組織名、政党名、政府組織名、行政組織名、軍隊名、国際組織名）その他の組織名	（競技組織名、公演組織名、その他）地名施設名製品名（商品名、番組名、映画名、書籍名、歌名、ブランド名等）イベント名使用方法必要なライブラリ（transformers、unidic_lite、fugashi）をpipなどでインストールして、下記のコードを実行するだけです。from transformers import BertJapaneseTokenizer, BertForTokenClassificationfrom transformers import pipelinemodel = BertForTokenClassification.from_pretrained("jurabi/bert-ner-japanese")tokenizer = BertJapaneseTokeniz
 * 📥 1427 [tohoku-nlp/bert-base-japanese-char-v3](https://huggingface.co/tohoku-nlp/bert-base-japanese-char-v3) - BERT base Japanese (character-level tokenization with whole word masking, CC-100 and jawiki-20230102)This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the Unidic 2.1.2 dictionary (available in unidic-lite package), followed by character-level tokenization.
 * 📥 1400 [dahara1/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ](https://huggingface.co/dahara1/ELYZA-japanese-Llama-2-7b-fast-instruct-GPTQ) - Model Card for Model IDOriginal model elyza/ELYZA-japanese-Llama-2-7b-fast-instruct which is based on Meta's "Llama 2" and has undergone additional pre-training in Japanese, and thier original post-training and speed up tuning. This model is a quantized(miniaturized to 4.11GB) version of the original model(13.69GB).Model DetailsQuantization reduces the amount of memory required and improves execution speed, but unfortunately performance deteriorates.
 * 📥 1332 [christian-phu/bert-finetuned-japanese-sentiment](https://huggingface.co/christian-phu/bert-finetuned-japanese-sentiment) - bert-finetuned-japanese-sentimentThis model is a fine-tuned version of cl-tohoku/bert-base-japanese-v2 on product amazon reviews japanese dataset. Model descriptionModel Train for amazon reviews Japanese sentence sentiments.
 * 📥 1332 [line-corporation/japanese-large-lm-3.6b-instruction-sft](https://huggingface.co/line-corporation/japanese-large-lm-3.6b-instruction-sft) - japanese-large-lm-3.6b-instruction-sftThis repository provides a 3.6B parameters Japanese language model, fine-tuned and trained by LINE Corporation. For Japanese詳細な説明や実験に関しては「Instruction Tuningにより対話性能を向上させた3.6B日本語言語モデルを公開します」をご覧ください。How to useimport torchfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipelinetokenizer = AutoTokenizer.from_pretrained("line-corporation/japanese-large-lm-3.6b-instruction-sft", use_fast=False)model = AutoModelForCausalLM.from_pretrained("line-corporation/japanese
 * 📥 1331 [karakuri-ai/karakuri-lm-70b-v0.1](https://huggingface.co/karakuri-ai/karakuri-lm-70b-v0.1) - KARAKURI LMKARAKURI LM is a pretrained language model that builds upon Llama 2.Our model enhances Llama 2's capabilities by incorporating additional Japanese vocabulary and further pretraining on a mixture of Japanese and multilingual corpora. KARAKURI LM Chat is a fine-tuned version of KARAKURI LM, which was trained on a mixture of publicly available and closed datasets using the SteerLM technique.
 * 📥 1259 [KBlueLeaf/guanaco-7b-leh-v2](https://huggingface.co/KBlueLeaf/guanaco-7b-leh-v2) - Guanaco-leh-V2: A Multilingual Instruction-Following Language Model Based on LLaMA 7BThis model is trained with guanaco-lora with lora + embed_tokens
 * 📥 1214 [retrieva-jp/t5-small-short](https://huggingface.co/retrieva-jp/t5-small-short) - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus. Model detailsT5 is a Transformer-based Encoder-Decoder model, now in v1.1, with the following improvements over the original T5.GEGLU activation in feed-forward hidden layer, rather than ReLU - see
 * 📥 1175 [nlp-waseda/roberta-large-japanese-seq512](https://huggingface.co/nlp-waseda/roberta-large-japanese-seq512) - nlp-waseda/roberta-large-japanese-seq512Model descriptionThis is a Japanese RoBERTa large model pretrained on Japanese Wikipedia and the Japanese portion of CC-100 with the maximum sequence length of 512.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained("nlp-waseda/roberta-large-japanese-seq512")model = AutoModelForMaskedLM.from_pretrained("nlp-waseda/roberta-large-japanese-seq512")se
 * 📥 1165 [llm-book/bert-base-japanese-v3-marc_ja](https://huggingface.co/llm-book/bert-base-japanese-v3-marc_ja) - bert-base-japanese-v3-marc_ja「大規模言語モデル入門」の第5章で紹介している(感情分析)のモデルです。cl-tohoku/bert-base-japanese-v3をJGLUEのMARC-jaデータセットでファインチューニングして構築されています。関連リンクGitHubリポジトリColabノートブック（訓練）Colabノートブック（推論）データセット大規模言語モデル入門（Amazon.co.jp）大規模言語モデル入門（gihyo.jp）使い方from transformers import pipelinetext_classification_pipeline = pipeline(model="llm-book/bert-base-japanese-v3-marc_ja")print(text_classification_pipeline("世界には言葉がわからなくても感動する音楽がある。")[0])# {'label': 'positive', 'score': 0.9993619322776794}ライセンスApache License 2.0
 * 📥 1142 [stabilityai/japanese-stablelm-instruct-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-beta-7b) - Japanese-StableLM-Instruct-Beta-7BA cute robot wearing a kimono writes calligraphy with one single brush — Stable Diffusion XLModel Descriptionjapanese-stablelm-instruct-beta-7b is a 7B-parameter decoder-only language model based on japanese-stablelm-base-beta-7b and further fine tuned on Databricks Dolly-15k, Anthropic HH, and other public data.
 * 📥 1142 [OrionStarAI/Orion-14B-Chat-RAG](https://huggingface.co/OrionStarAI/Orion-14B-Chat-RAG) - Orion-14B🌐English | 🇨🇳中文 | 🇯🇵日本語 | 🇰🇷한국어🤗 HuggingFace Mainpage | 🤖 ModelScope Mainpage🎬 HuggingFace Demo | 🎫 ModelScope Demo😺 GitHub📖 Tech ReportTable of Contents📖 Model Introduction🔗 Model Download🔖 Model Benchmark📊 Model Inference📜 Declarations &amp; License🥇 Company Introduction1. Model IntroductionOrion-14B series models are open-source multilingual large language models trained from scratch by OrionStarAI.  
 * 📥 1127 [tokyotech-llm/Swallow-13b-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data. The tuned versions use supervised fine-tuning (SFT).Links to other models can be found in the index.
 * 📥 1115 [umiyuki/Japanese-WizardLM2-ChatV-7B-GGUF](https://huggingface.co/umiyuki/Japanese-WizardLM2-ChatV-7B-GGUF) - Japanese-WizardLM2-ChatV-7B-GGUFGGUF conversion of "Japanese-WizardLM2-ChatV-7B"This model, Japanese-WizardLM2-ChatV-7B, is based on "chatntq-ja-7b-v1.0 ", and was created by subtracting "Mistral-7B-v0.1" from "WizardLM-2-7b" ChatVector was added by a factor of 1.0.We aimed to add the high performance of WizardLM-2 to the Japanese language capability of ChatNTQ.このモデル、Japanese-WizardLM2-ChatV-7Bは、”chatntq-ja-7b-v1.0”をベースに、"WizardLM-2-7b"から"Mistral-7B-v0.1"を差し引いて作ったChatVectorを1.0倍で足しました。ChatNTQの日本語能力にWizardLM
 * 📥 1095 [tokyotech-llm/Swallow-MX-8x7b-NVE-v0.1](https://huggingface.co/tokyotech-llm/Swallow-MX-8x7b-NVE-v0.1) - Swallow-MX-8x7b-NVE-v0.1Our Swallow-MX-8x7b-NVE-v0.1 model has undergone continuous pre-training from the Mixtral-8x7B-Instruct-v0.1, primarily with the addition of Japanese language data. Model DetailsModel type: Please refer to Mixtral technical report for details on the model architecture.
 * 📥 1049 [rinna/japanese-gpt-neox-3.6b-instruction-sft](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft) - japanese-gpt-neox-3.6b-instruction-sftOverviewThis repository provides a Japanese GPT-NeoX model of 3.6 billion parameters. The model is based on rinna/japanese-gpt-neox-3.6b and has been finetuned to serve as an instruction-following conversational agent.
 * 📥 1039 [stabilityai/japanese-stablelm-base-alpha-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-alpha-7b) - Japanese-StableLM-Base-Alpha-7B"A parrot able to speak Japanese, ukiyoe, edo period" — Stable Diffusion XLModel Descriptionjapanese-stablelm-base-alpha-7b is a 7B-parameter decoder-only language model pre-trained on a diverse collection of Japanese and English datasets which focus on maximizing Japanese language modeling performance and Japanese downstream task performance. For an instruction-following model, check Japanese-StableLM-Instruct-Alpha-7B and get access by accepting the terms and conditions.
 * 📥 1037 [rinna/japanese-gpt-neox-small](https://huggingface.co/rinna/japanese-gpt-neox-small) - japanese-gpt-neox-smallThis repository provides a small-sized Japanese GPT-NeoX model. The model was trained using code based on EleutherAI/gpt-neox.
 * 📥 1029 [haqishen/Llama-3-8B-Japanese-Instruct](https://huggingface.co/haqishen/Llama-3-8B-Japanese-Instruct) - IntroductionWho am I: Qishen Ha [Kaggle] [X] [LinkedIn]This is a meta-llama/Meta-Llama-3-8B-Instruct model that finetuned on Japanese conversation dataset.
 * 📥 1021 [izumi-lab/bert-base-japanese-fin-additional](https://huggingface.co/izumi-lab/bert-base-japanese-fin-additional) - Additional pretrained BERT base Japanese financeThis is a BERT model pretrained on texts in the Japanese language. The codes for the pretraining are available at retarfi/language-pretraining.
 * 📥 1020 [abeja/gpt2-large-japanese](https://huggingface.co/abeja/gpt2-large-japanese) - gpt2-large-japaneseThis repository provides a large sized Japanese GPT-2 model. The model was trained by ABEJA, IncHow to useFirst, install sentencepiece.
 * 📥 1012 [ken11/albert-base-japanese-v1](https://huggingface.co/ken11/albert-base-japanese-v1) - albert-base-japanese-v1日本語事前学習済みALBERTモデルですHow to useファインチューニングこのモデルはPreTrainedモデルです基本的には各種タスク用にファインチューニングして使用されることを想定していますFill-MaskこのモデルではTokenizerにSentencepieceを利用していますそのままでは[MASK]トークンのあとに余計なトークンが混入する問題があるので、利用する際には以下のようにする必要がありますfor PyTorchfrom transformers import (AlbertForMaskedLM, AlbertTokenizerFast)import torchtokenizer = AlbertTokenizerFast.from_pretrained("ken11/albert-base-japanese-v1")model = AlbertForMaskedLM.from_pretrained("ken11/albert-base-japanese-v1")text = "大学で[MASK]の研究をしています"tokenized_t
 * 📥 1006 [mmnga/ELYZA-japanese-Llama-2-7b-fast-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-gguf) - ELYZA-japanese-Llama-2-7b-fast-ggufELYZAさんが公開しているELYZA-japanese-Llama-2-7b-fastのggufフォーマット変換版です。他のモデルはこちら通常版: llama2に日本語のデータセットで学習したモデルmmnga/ELYZA-japanese-Llama-2-7b-ggufmmnga/ELYZA-japanese-Llama-2-7b-instruct-ggufFast版 日本語の語彙を追加してトークンコストを減らし、1.8倍高速化したモデルmmnga/ELYZA-japanese-Llama-2-7b-fast-ggufmmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-ggufCodellama版 GGUFmmnga/ELYZA-japanese-CodeLlama-7b-ggufmmnga/ELYZA-japanese-CodeLlama-7b-instruct-ggufCodellama版 GPTQmmnga/ELYZA-japanese-CodeLlama-7b-instruct-GPTQ-c
 * 📥 1004 [pfnet/plamo-13b-instruct](https://huggingface.co/pfnet/plamo-13b-instruct) - PLaMo-13B-InstructModel DescriptionPLaMo-13B-Instruct is an instruct fine-tuned model built upon the 8192 context length version of PLaMo-13B text generation model. PLaMo-13B-Instruct is fine-tuned using multiple publicly available Japanese datasets.
 * 📥 1003 [nlp-waseda/bigbird-base-japanese](https://huggingface.co/nlp-waseda/bigbird-base-japanese) - nlp-waseda/bigbird-base-japaneseModel descriptionThis is a Japanese BigBird base model pretrained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained("nlp-waseda/bigbird-base-japanese")model = AutoModelForMaskedLM.from_pretrained("nlp-waseda/bigbird-base-japanese")sentence = '[MASK] 大学 で 自然 言語 処理 を
 * 📥 992 [ThePioneer/CoolerWaifuDiffusion](https://huggingface.co/ThePioneer/CoolerWaifuDiffusion) - モデル説明 (model explanation)CoolJapanDiffusion 2.1.1とWaifuDiffusion 1.4 anime epoch2のマージ。 比率はckptファイル名の記載の通り。
 * 📥 941 [stockmark/gpt-neox-japanese-1.4b](https://huggingface.co/stockmark/gpt-neox-japanese-1.4b) - stockmark/gpt-neox-japanese-1.4bThis repository provides a GPT-NeoX based model with 1.4B parameters pre-trained on Japanese corpus of about 20B tokens. This model is developed by Stockmark Inc.
 * 📥 938 [TFMC/Japanese-Starling-ChatV-7B](https://huggingface.co/TFMC/Japanese-Starling-ChatV-7B) - Japanese-Starling-ChatV-7Bこのモデルは"chatntq-ja-7b-v1.0"をベースにした7Bパラメータの日本語チャットモデルです。"Japanese-Starling-ChatV-7B" is a Japanese chat model built on top of "chatntq-ja-7b-v1.0", originally based on Mistral-7B-v0.1.詳細とGGUF版はこちら。Details and GGUFs are here.
 * 📥 934 [elyza/ELYZA-japanese-CodeLlama-7b-instruct](https://huggingface.co/elyza/ELYZA-japanese-CodeLlama-7b-instruct) - ELYZA-japanese-CodeLlama-7bModel DescriptionELYZA-japanese-CodeLlama-7b は、 Code Llamaをベースとして日本語能力を拡張するために追加事前学習を行ったモデルです。詳細は Blog記事 を参照してください。Usageimport torchfrom transformers import AutoModelForCausalLM, AutoTokenizerB_INST, E_INST = "[INST]", "[/INST]"B_SYS, E_SYS = "&lt;&lt;SYS&gt;&gt;\n", "\n&lt;&lt;/SYS&gt;&gt;\n\n"DEFAULT_SYSTEM_PROMPT = "あなたは誠実で優秀な日本人のアシスタントです。"text = "エラトステネスの篩についてサンプルコードを示し、解説してください。"model_name = "elyza/ELYZA-japanese-CodeLlama-7b-instruct"tokenizer = AutoTokenizer.from_pretrained
 * 📥 924 [line-corporation/japanese-large-lm-3.6b](https://huggingface.co/line-corporation/japanese-large-lm-3.6b) - japanese-large-lm-3.6bThis repository provides a 3.6B parameters Japanese language model, trained by LINE Corporation. Tech Blog explains details.
 * 📥 922 [sazyou-roukaku/LittleStepMix](https://huggingface.co/sazyou-roukaku/LittleStepMix) - License:CreativeML Open RAIL-MAdditional Copyright: sazyou_roukaku (TwitterID @sazyou_roukaku) as of June 25, 2023このモデルは『CreativeML Open RAIL-M』でLicenseそのものに変更はありません。 しかし追加著作者として佐城郎画の名前が追加されています。
 * 📥 921 [NTQAI/chatntq-ja-7b-v1.0](https://huggingface.co/NTQAI/chatntq-ja-7b-v1.0) - ChatNTQ JA 7B V1.0Model DescriptionThis is a 7B-parameter decoder-only Japanese language model fine-tuned on our instruction-following datasets, built on top of the base model Japanese Stable LM Base Gamma 7B.PerformanceFor our final model, we've used Stability AI Japan's Japanese MT-Bench as a more representative test of our model's capabilities.
 * 📥 908 [tokyotech-llm/Swallow-7b-plus-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-plus-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data. The tuned versions use supervised fine-tuning (SFT).Links to other models can be found in the index.
 * 📥 904 [TKU410410103/uniTKU-hubert-japanese-asr](https://huggingface.co/TKU410410103/uniTKU-hubert-japanese-asr) - uniTKU-hubert-japanese-asrThis model was fine-tuned on a dataset provided by uniTKU, and it has maintained the original performance metrics on the common_voice_11_0 dataset. This model can only predict Hiragana.
 * 📥 899 [studio-ousia/luke-japanese-large](https://huggingface.co/studio-ousia/luke-japanese-large) - luke-japanese-largeluke-japanese is the Japanese version of LUKE (LanguageUnderstanding with Knowledge-based Embeddings), a pre-trainedknowledge-enhanced contextualized representation of words and entities. LUKEtreats words and entities in a given text as independent tokens, and outputscontextualized representations of them.
 * 📥 878 [TheBloke/japanese-stablelm-instruct-gamma-7B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-gamma-7B-GGUF) - Chat &amp; support: TheBloke's Discord serverWant to contribute? TheBloke's Patreon pageTheBloke's LLM work is generously supported by a grant from andreessen horowitz (a16z)Japanese StableLM
 * 📥 856 [rinna/nekomata-14b](https://huggingface.co/rinna/nekomata-14b) - rinna/nekomata-14bOverviewWe conduct continual pre-training of qwen-14b on 66B tokens from a mixture of Japanese and English datasets. The continual pre-training significantly improves the model's performance on Japanese tasks.
 * 📥 851 [rinna/llama-3-youko-8b](https://huggingface.co/rinna/llama-3-youko-8b) - Llama 3 Youko 8B (rinna/llama-3-youko-8b)OverviewWe conduct continual pre-training of meta-llama/Meta-Llama-3-8B on 22B tokens from a mixture of Japanese and English datasets. The continual pre-training significantly improves the model's performance on Japanese tasks.
 * 📥 835 [rinna/nekomata-7b](https://huggingface.co/rinna/nekomata-7b) - rinna/nekomata-7bOverviewWe conduct continual pre-training of qwen-7b on 30B tokens from a mixture of Japanese and English datasets. The continual pre-training significantly improves the model's performance on Japanese tasks.
 * 📥 822 [mmnga/ELYZA-japanese-CodeLlama-7b-instruct-gguf](https://huggingface.co/mmnga/ELYZA-japanese-CodeLlama-7b-instruct-gguf) - ELYZA-japanese-CodeLlama-7b-instruct-ggufELYZAさんが公開しているELYZA-japanese-CodeLlama-7b-instructのggufフォーマット変換版です。他のモデルはこちら通常版: llama2に日本語のデータセットで学習したモデルmmnga/ELYZA-japanese-Llama-2-7b-ggufmmnga/ELYZA-japanese-Llama-2-7b-instruct-ggufFast版 日本語の語彙を追加してトークンコストを減らし、1.8倍高速化したモデルmmnga/ELYZA-japanese-Llama-2-7b-fast-ggufmmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-ggufCodellama版 GGUFmmnga/ELYZA-japanese-CodeLlama-7b-ggufmmnga/ELYZA-japanese-CodeLlama-7b-instruct-ggufCodellama版 GPTQmmnga/ELYZA-japanese-CodeLlama-7b-ins
 * 📥 811 [KoichiYasuoka/roberta-small-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/roberta-small-japanese-luw-upos) - roberta-small-japanese-luw-uposModel DescriptionThis is a RoBERTa model pre-trained on 青空文庫 texts for POS-tagging and dependency-parsing, derived from roberta-small-japanese-aozora.
 * 📥 805 [Aratako/c4ai-command-r-v01-japanese-instruct-GGUF](https://huggingface.co/Aratako/c4ai-command-r-v01-japanese-instruct-GGUF) - c4ai-command-r-v01-japanese-instruct-GGUF概要Aratako/c4ai-command-r-v01-japanese-instructの量子化済みGGUF版です。ライセンス等詳細は元モデルをご確認ください。
 * 📥 795 [pfnet/plamo-13b-instruct-nc](https://huggingface.co/pfnet/plamo-13b-instruct-nc) - PLaMo-13B-Instruct-NCModel DescriptionPLaMo-13B-Instruct-NC is a noncommercial instruct fine-tuned model built upon the 8192 context length version of PLaMo-13B text generation model. PLaMo-13B-Instruct-NC is fine-tuned using multiple publicly available Japanese datasets.
 * 📥 783 [stabilityai/japanese-stablelm-3b-4e1t-base](https://huggingface.co/stabilityai/japanese-stablelm-3b-4e1t-base) - Japanese StableLM-3B-4E1T BaseModel DescriptionThis is a 3B-parameter decoder-only language model with a focus on maximizing Japanese language modeling performance and Japanese downstream task performance. We conducted continued pretraining using Japanese data on the English language model, StableLM-3B-4E1T, to transfer the model's knowledge and capabilities to Japanese.
 * 📥 767 [stabilityai/japanese-stablelm-base-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-beta-7b) - Japanese-StableLM-Base-Beta-7BA cute robot wearing a kimono writes calligraphy with one single brush — Stable Diffusion XLModel Descriptionjapanese-stablelm-base-beta-7b is a 7B-parameter decoder-only language model based on Llama-2-7b that has been fine-tuned on a diverse collection of Japanese data, with the intent of maximizing downstream performance on Japanese language tasks. For an instruction-following model, check Japanese-StableLM-Instruct-Beta-7B. The base and instruct models are also available in
 * 📥 764 [tokyotech-llm/Swallow-7b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-7b-instruct-v0.1) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data. The tuned versions use supervised fine-tuning (SFT).Links to other models can be found in the index.
 * 📥 751 [llm-book/bert-base-japanese-v3-jnli](https://huggingface.co/llm-book/bert-base-japanese-v3-jnli) - bert-base-japanese-v3-jnli「大規模言語モデル入門」の第5章で紹介している(自然言語推論)のモデルです。cl-tohoku/bert-base-japanese-v3をJGLUEのMARC-jaデータセットでファインチューニングして構築されています。関連リンクGitHubリポジトリColabノートブック（訓練）Colabノートブック（推論）データセット大規模言語モデル入門（Amazon.co.jp）大規模言語モデル入門（gihyo.jp）使い方from transformers import pipelinenli_pipeline = pipeline(model="llm-book/bert-base-japanese-v3-jnli")text = "二人の男性がジェット機を見ています"entailment_text = "ジェット機を見ている人が二人います"# textとentailment_textの論理関係を予測print(nli_pipeline({"text": text, "text_pair": entailment_text}))# {'label': 'enta
 * 📥 721 [mmnga/c4ai-command-r-plus-gguf](https://huggingface.co/mmnga/c4ai-command-r-plus-gguf) - c4ai-command-r-plus-ggufCohereForAIさんが公開しているc4ai-command-r-plusのggufフォーマット変換版です。imatrixのデータはTFMC/imatrix-dataset-for-japanese-llmを使用して作成しました。分割されたファイルについてq6_kやq8_0のファイルはサイズが大きく分割されているので結合する必要があります。cat c4ai-command-r-plus-Q5_K_M.gguf.* &gt; c4ai-command-r-plus-Q5_K_M.ggufUsagegit clone https://github.com/ggerganov/llama.cpp.gitcd llama.cppmake -j./main -m 'c4ai-command-r-plus-Q4_0.gguf' -p "&lt;|START_OF_TURN_TOKEN|&gt;&lt;|SYSTEM_TOKEN|&gt;あなたは日本語を話すCommand-Rです&lt;|END_OF_TURN_TOKEN|&gt;&lt;|START_OF_TURN_T
 * 📥 703 [studio-ousia/luke-japanese-base-lite](https://huggingface.co/studio-ousia/luke-japanese-base-lite) - luke-japaneseluke-japanese is the Japanese version of LUKE (LanguageUnderstanding with Knowledge-based Embeddings), a pre-trainedknowledge-enhanced contextualized representation of words and entities. LUKEtreats words and entities in a given text as independent tokens, and outputscontextualized representations of them.
 * 📥 687 [line-corporation/japanese-large-lm-1.7b-instruction-sft](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft) - japanese-large-lm-1.7b-instruction-sftThis repository provides a 1.7B parameters Japanese language model, fine-tuned and trained by LINE Corporation. For Japanese詳細な説明や実験に関しては「Instruction Tuningにより対話性能を向上させた3.6B日本語言語モデルを公開します」をご覧ください。How to useimport torchfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipelinemodel = AutoModelForCausalLM.from_pretrained("line-corporation/japanese-large-lm-1.7b-instruction-sft")tokenizer = AutoTokenizer.from_pretrained("line-corporation/japanese-large-lm-1.7b-i
 * 📥 659 [llm-book/bert-base-japanese-v3-jsts](https://huggingface.co/llm-book/bert-base-japanese-v3-jsts) - bert-base-japanese-v3-jsts「大規模言語モデル入門」の第5章で紹介している(意味類似度計算)のモデルです。 cl-tohoku/bert-base-japanese-v3をJGLUEのJSTSデータセットでファインチューニングして構築されています。
 * 📥 658 [retrieva-jp/t5-base-long](https://huggingface.co/retrieva-jp/t5-base-long) - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus. Model detailsT5 is a Transformer-based Encoder-Decoder model, now in v1.1, with the following improvements over the original T5.GEGLU activation in feed-forward hidden layer, rather than ReLU - see
 * 📥 654 [Tanrei/GPTSAN-japanese](https://huggingface.co/Tanrei/GPTSAN-japanese) - Model Card for Tanrei/GPTSAN-japaneseGeneral-purpose Swich transformer based Japanese language modelGPTSAN has some unique features. It has a model structure of Prefix-LM.
 * 📥 652 [nlp-waseda/roberta-base-japanese](https://huggingface.co/nlp-waseda/roberta-base-japanese) - nlp-waseda/roberta-base-japaneseModel descriptionThis is a Japanese RoBERTa base model pretrained on Japanese Wikipedia and the Japanese portion of CC-100.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained("nlp-waseda/roberta-base-japanese")model = AutoModelForMaskedLM.from_pretrained("nlp-waseda/roberta-base-japanese")sentence = '早稲田 大学 で 自然 言語 処理 を [MASK] する 。' # input should be segm
 * 📥 643 [TheBloke/japanese-stablelm-instruct-beta-70B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-70B-GGUF) - Chat &amp; support: TheBloke's Discord serverWant to contribute? TheBloke's Patreon pageTheBloke's LLM work is generously supported by a grant from andreessen horowitz (a16z)Japanese StableLM
 * 📥 637 [line-corporation/japanese-large-lm-1.7b](https://huggingface.co/line-corporation/japanese-large-lm-1.7b) - japanese-large-lm-1.7bThis repository provides a 1.7B parameters Japanese language model, trained by LINE Corporation. Tech Blog explains details.
 * 📥 611 [aken12/splade-japanese-v3](https://huggingface.co/aken12/splade-japanese-v3) - Evaluation on MIRACL japaneseThese models don't train on the MIRACL training data. ModelnDCG@10Recall@1000Recall@5Recall@30BM250.3690.931--splade-japanese0.4050.9310.4060.663splade-japanese-efficient0.4080.9540.4190.718splade-japanese-v20.5800.9670.6290.844splade-japanese-v2-doc0.4780.9300.5140.759splade-japanese-v30.6040.9790.6470.877*'splade-japanese-v2-doc' model does not require query encoder during inference.
 * 📥 605 [mmnga/ELYZA-japanese-Llama-2-13b-fast-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-13b-fast-gguf) - ELYZA-japanese-Llama-2-13b-fast-ggufELYZAさんが公開しているELYZA-japanese-Llama-2-13b-fastのggufフォーマット変換版です。他のモデルはこちら通常版: llama2に日本語のデータセットで学習したモデルmmnga/ELYZA-japanese-Llama-2-7b-ggufmmnga/ELYZA-japanese-Llama-2-7b-instruct-ggufFast版 日本語の語彙を追加してトークンコストを減らし、1.8倍高速化したモデルmmnga/ELYZA-japanese-Llama-2-7b-fast-ggufmmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-ggufmmnga/ELYZA-japanese-Llama-2-13b-fast-ggufmmnga/ELYZA-japanese-Llama-2-13b-fast-instruct-ggufCodellama版 GGUFmmnga/ELYZA-japanese-CodeLlama-7b-ggufmmnga/ELYZA-japa
 * 📥 588 [rinna/nekomata-14b-instruction-gguf](https://huggingface.co/rinna/nekomata-14b-instruction-gguf) - rinna/nekomata-14b-instruction-ggufOverviewThe model is the GGUF version of rinna/nekomata-14b-instruction. It can be used with llama.cpp for lightweight inference.
 * 📥 585 [mmnga/ELYZA-japanese-Llama-2-7b-gguf](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-gguf) - ELYZA-japanese-Llama-2-7b-ggufELYZAさんが公開しているELYZA-japanese-Llama-2-7bのggufフォーマット変換版です。他のモデルはこちら通常版: llama2に日本語のデータセットで学習したモデルmmnga/ELYZA-japanese-Llama-2-7b-ggufmmnga/ELYZA-japanese-Llama-2-7b-instruct-ggufFast版 日本語の語彙を追加してトークンコストを減らし、1.8倍高速化したモデルmmnga/ELYZA-japanese-Llama-2-7b-fast-ggufmmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-ggufCodellama版 GGUFmmnga/ELYZA-japanese-CodeLlama-7b-ggufmmnga/ELYZA-japanese-CodeLlama-7b-instruct-ggufCodellama版 GPTQmmnga/ELYZA-japanese-CodeLlama-7b-instruct-GPTQ-calib-ja-1k
 * 📥 582 [ku-nlp/deberta-v2-base-japanese-char-wwm](https://huggingface.co/ku-nlp/deberta-v2-base-japanese-char-wwm) - Model Card for Japanese character-level DeBERTa V2 baseModel descriptionThis is a Japanese DeBERTa V2 base model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.This model is trained with character-level tokenization and whole word masking.
 * 📥 579 [TareHimself/manga-ocr-base](https://huggingface.co/TareHimself/manga-ocr-base) - Original ModelOptical character recognition for Japanese text, with the main focus being Japanese manga.
 * 📥 543 [mmnga/rinna-japanese-gpt-neox-3.6b-instruction-ppo-gguf](https://huggingface.co/mmnga/rinna-japanese-gpt-neox-3.6b-instruction-ppo-gguf) - rinna/japanese-gpt-neox-3.6b-instruction-pporinnaさんが公開しているjapanese-gpt-neox-3.6b-instruction-ppoのgguf変換版です。他モデルはこちらmmnga/rinna-bilingual-gpt-neox-4b-ggufmmnga/rinna-bilingual-gpt-neox-4b-8k-ggufmmnga/rinna-bilingual-gpt-neox-4b-instruction-ppo-ggufmmnga/rinna-japanese-gpt-neox-3.6b-ggufmmnga/rinna-japanese-gpt-neox-3.6b-instruction-ppo-gguf注意:こちらはブランチで試用になります。llama.cpp本家にgptneoxが実装された時に、このggufファイルが使用できない可能性があります。GitHubリポジトリの readme はこちらUsage (試用)git clone --branch mmnga-dev https://github.com/mmnga/llama.cp
 * 📥 542 [mmnga/rinna-japanese-gpt-neox-3.6b-gguf](https://huggingface.co/mmnga/rinna-japanese-gpt-neox-3.6b-gguf) - rinna/japanese-gpt-neox-3.6brinnaさんが公開しているjapanese-gpt-neox-3.6bのgguf変換版です。他モデルはこちらmmnga/rinna-bilingual-gpt-neox-4b-ggufmmnga/rinna-bilingual-gpt-neox-4b-8k-ggufmmnga/rinna-bilingual-gpt-neox-4b-instruction-ppo-ggufmmnga/rinna-japanese-gpt-neox-3.6b-ggufmmnga/rinna-japanese-gpt-neox-3.6b-instruction-ppo-gguf注意:こちらはブランチで試用になります。llama.cpp本家にgptneoxが実装された時に、このggufファイルが使用できない可能性があります。GitHubリポジトリの readme はこちらUsage (試用)git clone --branch mmnga-dev https://github.com/mmnga/llama.cpp.gitcd llama.cppmake -j./main -
 * 📥 536 [nlp-waseda/comet-t5-base-japanese](https://huggingface.co/nlp-waseda/comet-t5-base-japanese) - COMET-T5 jaFinetuned T5 on ATOMIC ja using a text-to-text language modeling objective. It was introduced in this paper.
 * 📥 536 [mmnga/gemma-1.1-7b-it-gguf](https://huggingface.co/mmnga/gemma-1.1-7b-it-gguf) - gemma-1.1-7b-it-ggufgoogleさんが公開しているgemma-1.1-7b-itのggufフォーマット変換版です。Licencegemma-terms-of-use 利用規約をご利用前に必ずご確認ください。他のモデルmmnga/codegemma-1.1-7b-it-ggufmmnga/codegemma-1.1-2b-ggufmmnga/gemma-2b-it-ggufmmnga/gemma-7b-it-ggufmmnga/gemma-1.1-7b-it-ggufmmnga/codegemma-7b-it-ggufUsagegit clone https://github.com/ggerganov/llama.cpp.gitcd llama.cppmake -j./main -m 'gemma-1.1-7b-it-q4_0.gguf' -p "&lt;start_of_turn&gt;user\n日本の文化を１０個教えて。&lt;end_of_turn&gt;\n&lt;start_of_turn&gt;model\n" -n 128
 * 📥 525 [hotchpotch/japanese-reranker-cross-encoder-small-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-small-v1) - hotchpotch/japanese-reranker-cross-encoder-small-v1日本語で学習させた Reranker (CrossEncoder) シリーズです。モデル名layershidden_sizehotchpotch/japanese-reranker-cross-encoder-xsmall-v16384hotchpotch/japanese-reranker-cross-encoder-small-v112384hotchpotch/japanese-reranker-cross-encoder-base-v112768hotchpotch/japanese-reranker-cross-encoder-large-v1241024hotchpotch/japanese-bge-reranker-v2-m3-v1241024Reranker についてや、技術レポート・評価等は以下を参考ください。日本語最高性能のRerankerをリリース / そもそも Reranker とは?日本語 Reranker 作成のテクニカルレポート使い方SentenceTransformersfro
 * 📥 505 [tohoku-nlp/bert-large-japanese-char-v2](https://huggingface.co/tohoku-nlp/bert-large-japanese-char-v2) - BERT large Japanese (character-level tokenization with whole word masking, CC-100 and jawiki-20230102)This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the Unidic 2.1.2 dictionary (available in unidic-lite package), followed by character-level tokenization.
 * 📥 481 [tokyotech-llm/Swallow-7b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-7b-NVE-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data. The tuned versions use supervised fine-tuning (SFT).Links to other models can be found in the index.
 * 📥 479 [rinna/japanese-gpt-neox-3.6b-instruction-sft-v2](https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-sft-v2) - japanese-gpt-neox-3.6b-instruction-sft-v2OverviewThis repository provides a Japanese GPT-NeoX model of 3.6 billion parameters. The model is based on rinna/japanese-gpt-neox-3.6b and has been finetuned to serve as an instruction-following conversational agent.
 * 📥 475 [rinna/japanese-wav2vec2-base](https://huggingface.co/rinna/japanese-wav2vec2-base) - rinna/japanese-wav2vec2-baseOverviewThis is a Japanese wav2vec 2.0 Base model trained by rinna Co., Ltd.Model summaryThe model architecture is the same as the original wav2vec 2.0 Base model, which contains 12 transformer layers with 12 attention heads. The model was trained using code from the official repository, and the detailed training configuration can be found in the same repository and the original paper.
 * 📥 457 [second-state/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF](https://huggingface.co/second-state/ELYZA-japanese-Llama-2-13b-fast-instruct-GGUF) - ELYZA-japanese-Llama-2-13b-fast-instruct-GGUFOriginal Modelelyza/ELYZA-japanese-Llama-2-13b-fast-instructRun with LlamaEdgeLlamaEdge version: v0.2.8 and abovePrompt templatePrompt type: llama-2-chatPrompt string&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;{{ system_prompt }}&lt;&lt;/SYS&gt;&gt;{{ user_msg_1 }} [/INST] {{ model_answer_1 }} &lt;/s&gt;&lt;s&gt;[INST] {{ user_msg_2 }}Context size: 5120Run as LlamaEdge servicewasmedge --dir .:.
 * 📥 456 [retrieva-jp/t5-large-long](https://huggingface.co/retrieva-jp/t5-large-long) - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus. Model detailsT5 is a Transformer-based Encoder-Decoder model, now in v1.1, with the following improvements over the original T5.GEGLU activation in feed-forward hidden layer, rather than ReLU - see
 * 📥 450 [kotoba-tech/kotoba-whisper-v1.1](https://huggingface.co/kotoba-tech/kotoba-whisper-v1.1) - Kotoba-Whisper-v1.1Kotoba-Whisper-v1.1 is a Japanese ASR model based on kotoba-tech/kotoba-whisper-v1.0, withadditional postprocessing stacks integrated as pipeline. The new features includes(i) improved timestamp achieved by stable-ts and (ii) adding punctuation with punctuators.
 * 📥 438 [tsmatz/mt5_summarize_japanese](https://huggingface.co/tsmatz/mt5_summarize_japanese) - mt5_summarize_japanese(Japanese caption : 日本語の要約のモデル)This model is a fine-tuned version of google/mt5-small trained for Japanese summarization. This model is fine-tuned on BBC news articles (XL-Sum Japanese dataset), in which the first sentence (headline sentence) is used for summary and others are used for article.
 * 📥 431 [retrieva-jp/t5-small-long](https://huggingface.co/retrieva-jp/t5-small-long) - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus. Model detailsT5 is a Transformer-based Encoder-Decoder model, now in v1.1, with the following improvements over the original T5.GEGLU activation in feed-forward hidden layer, rather than ReLU - see
 * 📥 431 [Local-Novel-LLM-project/Vecteus-v1](https://huggingface.co/Local-Novel-LLM-project/Vecteus-v1) - Our ModelsVecteusNinja-v1Ninja-v1-NSFWNinja-v1-128kNinja-v1-NSFW-128kModel Card for VecTeus-v1.0The Mistral-7B--based Large Language Model (LLM) is an noveldataset fine-tuned version of the Mistral-7B-v0.1VecTeus has the following changes compared to Mistral-7B-v0.1.128k context window (8k context in v0.1)Achieving both high quality Japanese and English generationCan be generated NSFWMemory ability that does not forget even after long-context generationThis model was created with the help of GPUs from the f
 * 📥 426 [llm-book/bert-base-japanese-v3-unsup-simcse-jawiki](https://huggingface.co/llm-book/bert-base-japanese-v3-unsup-simcse-jawiki) - bert-base-japanese-v3-unsup-simcse-jawiki「大規模言語モデル入門」の第8章で紹介している教師なしSimCSEのモデルです。cl-tohoku/bert-base-japanese-v3 を llm-book/jawiki-sentences でファインチューニングして構築されています。関連リンクGitHubリポジトリColabノートブック（訓練）Colabノートブック（推論）データセット大規模言語モデル入門（Amazon.co.jp）大規模言語モデル入門（gihyo.jp）使い方from torch.nn.functional import cosine_similarityfrom transformers import pipelinesim_enc_pipeline = pipeline(model="llm-book/bert-base-japanese-v3-unsup-simcse-jawiki", task="feature-extraction")text = "川べりでサーフボードを持った人たちがいます"sim_text = "サーファーたちが川べりに
 * 📥 424 [megagonlabs/t5-base-japanese-web](https://huggingface.co/megagonlabs/t5-base-japanese-web) - t5-base-japanese-web (with Byte-fallback, 32K)Descriptionmegagonlabs/t5-base-japanese-web is a T5 (Text-to-Text Transfer Transformer) model pre-trained on Japanese web texts. Training codes are available on GitHub.
 * 📥 415 [ku-nlp/deberta-v3-base-japanese](https://huggingface.co/ku-nlp/deberta-v3-base-japanese) - Model Card for Japanese DeBERTa V3 baseModel descriptionThis is a Japanese DeBERTa V3 base model pre-trained on LLM-jp corpus v1.0.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained('ku-nlp/deberta-v3-base-japanese')model = AutoModelForMaskedLM.from_pretrained('ku-nlp/deberta-v3-base-japanese')sentences =
 * 📥 414 [maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-instruct-gguf](https://huggingface.co/maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-instruct-gguf) - I'm constantly enhancing these model descriptions to provide you with the most relevant and comprehensive informationjapanese-stablelm-3b-4e1t-instruct - GGUFModel creator: stabilityaiOriginal model: japanese-stablelm-3b-4e1t-instructStableLMThis is a Model based on StableLM.Stablelm is a familiy of Language Models by Stability AI.Note:Current (as of 2023-11-15) implementations of Llama.cpp only support GPU offloading up to 34 Layers with these StableLM Models. The model will crash immediately if -ngl is lar
 * 📥 414 [TheBloke/japanese-stablelm-base-beta-70B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-base-beta-70B-GGUF) - Chat &amp; support: TheBloke's Discord serverWant to contribute? TheBloke's Patreon pageTheBloke's LLM work is generously supported by a grant from andreessen horowitz (a16z)Japanese StableLM
 * 📥 413 [alfredplpl/Llama-3-8B-Instruct-Ja](https://huggingface.co/alfredplpl/Llama-3-8B-Instruct-Ja) - 日本語向け Llama 3 8BはじめにこのリポジトリはLlama 3を日本語化しようとしたモデルのリポジトリです。 4/23に更新したため、新しくダウンロードすることをオススメします。
 * 📥 408 [stabilityai/japanese-stable-diffusion-xl](https://huggingface.co/stabilityai/japanese-stable-diffusion-xl) - By downloading, using, or distributing any portion or element of this model, you agree to be bound by the agreement described in the LICENSE file.
 * 📥 403 [mmnga/ELYZA-japanese-CodeLlama-7b-gguf](https://huggingface.co/mmnga/ELYZA-japanese-CodeLlama-7b-gguf) - ELYZA-japanese-CodeLlama-7b-ggufELYZAさんが公開しているELYZA-japanese-CodeLlama-7b-instructのggufフォーマット変換版です。他のモデルはこちら通常版: llama2に日本語のデータセットで学習したモデルmmnga/ELYZA-japanese-Llama-2-7b-ggufmmnga/ELYZA-japanese-Llama-2-7b-instruct-ggufFast版 日本語の語彙を追加してトークンコストを減らし、1.8倍高速化したモデルmmnga/ELYZA-japanese-Llama-2-7b-fast-ggufmmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-ggufCodellama版 GGUFmmnga/ELYZA-japanese-CodeLlama-7b-ggufmmnga/ELYZA-japanese-CodeLlama-7b-instruct-ggufCodellama版 GPTQmmnga/ELYZA-japanese-CodeLlama-7b-instruct-GPT
 * 📥 402 [sarulab-speech/hubert-base-jtube](https://huggingface.co/sarulab-speech/hubert-base-jtube) - hubert-base-jtubeThis repo provides model weights for the hubert-base model trained on the JTubeSpeech corpus.Scroll down for the model usageFAQQ.  何をするモデル？
 * 📥 401 [MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF) - MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUFModel creator: MaziyarPanahiOriginal model: MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1DescriptionMaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF contains GGUF format model files for MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1.How to useThanks to TheBloke for preparing an amazing README on how to use GGUF models:About GGUFGGUF is a new format introduced
 * 📥 397 [ku-nlp/gpt2-small-japanese-char](https://huggingface.co/ku-nlp/gpt2-small-japanese-char) - Model Card for Japanese character-level GPT-2 SmallModel descriptionThis is a Japanese character-level GPT-2 Small (90M parameters) language model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.How to useYou can use this model directly with a pipeline for text generation.&gt;&gt;&gt; from transformers import pipeline, set_seed&gt;&gt;&gt; generator = pipeline('text-generation', model='ku-nlp/gpt2-small-japanese-char')&gt;&gt;&gt; set_seed(5)&gt;&gt;&gt;
 * 📥 390 [maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-base-gguf](https://huggingface.co/maddes8cht/stabilityai-japanese-stablelm-3b-4e1t-base-gguf) - I'm constantly enhancing these model descriptions to provide you with the most relevant and comprehensive informationjapanese-stablelm-3b-4e1t-base - GGUFModel creator: stabilityaiOriginal model: japanese-stablelm-3b-4e1t-baseStableLMThis is a Model based on StableLM.Stablelm is a familiy of Language Models by Stability AI.Note:Current (as of 2023-11-15) implementations of Llama.cpp only support GPU offloading up to 34 Layers with these StableLM Models. The model will crash immediately if -ngl is larger than
 * 📥 375 [vumichien/wav2vec2-large-xlsr-japanese-hiragana](https://huggingface.co/vumichien/wav2vec2-large-xlsr-japanese-hiragana) - Wav2Vec2-Large-XLSR-53-JapaneseFine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the Common Voice and Japanese speech corpus of Saruwatari-lab, University of Tokyo JSUT.When using this model, make sure that your speech input is sampled at 16kHz. UsageThe model can be used directly (without a language model) as follows:!pip install mecab-python3!pip install unidic-lite!pip install pykakasi!python
 * 📥 367 [hotchpotch/japanese-bge-reranker-v2-m3-v1](https://huggingface.co/hotchpotch/japanese-bge-reranker-v2-m3-v1) - hotchpotch/japanese-bge-reranker-v2-m3-v1日本語で学習させた Reranker (CrossEncoder) シリーズです。モデル名layershidden_sizehotchpotch/japanese-reranker-cross-encoder-xsmall-v16384hotchpotch/japanese-reranker-cross-encoder-small-v112384hotchpotch/japanese-reranker-cross-encoder-base-v112768hotchpotch/japanese-reranker-cross-encoder-large-v1241024hotchpotch/japanese-bge-reranker-v2-m3-v1241024Reranker についてや、技術レポート・評価等は以下を参考ください。日本語最高性能のRerankerをリリース / そもそも Reranker とは?日本語 Reranker 作成のテクニカルレポート使い方SentenceTransformersfrom sentence
 * 📥 363 [bclavie/fio-base-japanese-v0.1](https://huggingface.co/bclavie/fio-base-japanese-v0.1) - fio-base-japanese-v0.1日本語版は近日公開予定です（日本語を勉強中なので、間違いはご容赦ください！）fio-base-japanese-v0.1 is a proof of concept, and the first release of the Fio family of Japanese embeddings. It is based on cl-tohoku/bert-base-japanese-v3 and trained on limited volumes of data on a single GPU.For more information, please refer to my notes on Fio.
 * 📥 359 [mmnga/SakanaAI-EvoLLM-JP-A-v1-7B-gguf](https://huggingface.co/mmnga/SakanaAI-EvoLLM-JP-A-v1-7B-gguf) - SakanaAI-EvoLLM-JP-A-v1-7B-ggufSakanaAIさんが公開しているEvoLLM-JP-A-v1-7Bのggufフォーマット変換版です。 こちらはベースモデルになります。
 * 📥 354 [tsmatz/roberta_qa_japanese](https://huggingface.co/tsmatz/roberta_qa_japanese) - roberta_qa_japanese(Japanese caption : 日本語の (抽出型) 質問応答のモデル)This model is a fine-tuned version of rinna/japanese-roberta-base (pre-trained RoBERTa model provided by rinna Co., Ltd.) trained for extractive question answering. The model is fine-tuned on JaQuAD dataset provided by Skelter Labs, in which data is collected from Japanese Wikipedia articles and annotated by a human.
 * 📥 348 [cyberagent/xlm-roberta-large-jnli-jsick](https://huggingface.co/cyberagent/xlm-roberta-large-jnli-jsick) - Japanese Natural Language Inference ModelThis model was trained using SentenceTransformers Cross-Encoder class, gradient accumulation PR, and the code from CyberAgentAILab/japanese-nli-model. Training DataThe model was trained on the JGLUE-JNLI and JSICK datasets.
 * 📥 344 [Mizuiro-sakura/luke-japanese-base-finetuned-QA](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-QA) - このモデルはluke-japanese-base-liteをファインチューニングして、Question-Answeringに用いれるようにしたものです。このモデルはluke-japanese-base-liteを運転ドメインQAデータセット（DDQA）（　https://nlp.ist.i.kyoto-u.ac.jp/index.php?Driving%20domain%20QA%20datasets　）を用いてファインチューニングしたものです。Question-Answeringタスク（SQuAD）に用いることができます。This model is fine-tuned model for Question-Answering which is based on luke-japanese-base-liteThis model is fine-tuned by using DDQA dataset. You could use this model for Question-Answering tasks.モデルの精度 accuracy of model'em(厳密一致)': 0.8459330143540
 * 📥 343 [retrieva-jp/t5-xl](https://huggingface.co/retrieva-jp/t5-xl) - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus. Model detailsT5 is a Transformer-based Encoder-Decoder model, now in v1.1, with the following improvements over the original T5.GEGLU activation in feed-forward hidden layer, rather than ReLU - see
 * 📥 341 [izumi-lab/bert-small-japanese](https://huggingface.co/izumi-lab/bert-small-japanese) - BERT small Japanese financeThis is a BERT model pretrained on texts in the Japanese language. The codes for the pretraining are available at retarfi/language-pretraining.
 * 📥 333 [OrionStarAI/Orion-14B-LongChat](https://huggingface.co/OrionStarAI/Orion-14B-LongChat) - Orion-14B🌐English | 🇨🇳中文 | 🇯🇵日本語 | 🇰🇷한국어🤗 HuggingFace Mainpage | 🤖 ModelScope Mainpage🎬 HuggingFace Demo | 🎫 ModelScope Demo😺 GitHub📖 Tech ReportTable of Contents📖 Model Introduction🔗 Model Download🔖 Model Benchmark📊 Model Inference📜 Declarations &amp; License🥇 Company Introduction1. Model IntroductionOrion-14B series models are open-source multilingual large language models trained from scratch by OrionStarAI.  
 * 📥 331 [KoichiYasuoka/bert-base-japanese-wikipedia-ud-head](https://huggingface.co/KoichiYasuoka/bert-base-japanese-wikipedia-ud-head) - bert-base-japanese-wikipedia-ud-headModel DescriptionThis is a BERT model pretrained on Japanese Wikipedia texts for dependency-parsing (head-detection on long-unit-words) as question-answering, derived from bert-base-japanese-char-extended and UD_Japanese-GSDLUW.
 * 📥 329 [turing-motors/heron-chat-git-ja-stablelm-base-7b-v1](https://huggingface.co/turing-motors/heron-chat-git-ja-stablelm-base-7b-v1) - Heron GIT Japanese StableLM Base 7BModel DetailsHeron GIT Japanese StableLM
 * 📥 329 [rinna/nekomata-14b-gguf](https://huggingface.co/rinna/nekomata-14b-gguf) - rinna/nekomata-14b-ggufOverviewThe model is the GGUF version of rinna/nekomata-14b. It can be used with llama.cpp for lightweight inference.
 * 📥 328 [ku-nlp/deberta-v2-large-japanese](https://huggingface.co/ku-nlp/deberta-v2-large-japanese) - Model Card for Japanese DeBERTa V2 largeModel descriptionThis is a Japanese DeBERTa V2 large model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and theJapanese portion of OSCAR.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained('ku-nlp/deberta-v2-large-japanese')model = AutoModelForMaskedLM.from_pretrained('ku-nlp/deberta-v2-large-japanese')sentence = '京都 大学 で 自然
 * 📥 327 [MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF) - MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUFModel creator: MaziyarPanahiOriginal model: MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1DescriptionMaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1-GGUF contains GGUF format model files for MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1.How to useThanks to TheBloke for preparing an amazing README on how to use GGUF models:About GGUFGGUF is a new f
 * 📥 317 [mmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-gguf) - line-corporation/japanese-large-lm-3.6b-instruction-sftline-corporationさんが公開しているjapanese-large-lm-3.6b-instruction-sftのgguf変換版です。他モデルはこちらGPT-NEOXmmnga/line-corp-japanese-large-lm-3.6b-ggufmmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-ggufGPT-2mmnga/line-corp-japanese-large-lm-1.7b-ggufmmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-gguf注意:こちらはブランチで試用になります。llama.cpp本家にgptneox, gpt2が実装された時に、このggufファイルが使用できない可能性があります。GitHubリポジトリの readme はこちらUsage (試用)git clone --branch mmnga-dev https://github.
 * 📥 315 [hajime9652/xlnet-japanese](https://huggingface.co/hajime9652/xlnet-japanese) - XLNet-japaneseModel descriptionThis model require Mecab and senetencepiece with XLNetTokenizer. See details https://qiita.com/mkt3/items/4d0ae36f3f212aee8002This model uses NFKD as the normalization method for character encoding.
 * 📥 314 [TheBloke/japanese-stablelm-instruct-beta-7B-GGUF](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-7B-GGUF) - Chat &amp; support: TheBloke's Discord serverWant to contribute? TheBloke's Patreon pageTheBloke's LLM work is generously supported by a grant from andreessen horowitz (a16z)Japanese StableLM
 * 📥 310 [webbigdata/C3TR-Adapter](https://huggingface.co/webbigdata/C3TR-Adapter) - モデルカード(Model Card for Model ID)C3TR-AdapterはGoogleが発表したLLMであるgemma-7bの日英・英日翻訳性能を向上させるQLoRA Adapterです。C3TR-Adapter is a QLoRA Adapter that improves the Japanese-English and English-Japanese translation performance of gemma-7b released by Google. モデル詳細(Model Details)C3TR-Adapterは翻訳ベンチマークで多言語翻訳モデルであるGoogleのMadlad400やmetaのSeamless m4t v2 large、ALMA-Ja-V2 (私達の以前のllama 2ベースのモデル)よりも大幅に優れた日英・日英翻訳性能を持っています。Benchmarks show significantly better English-Japanese and Japanese-English translation performance than Google's
 * 📥 306 [llm-book/t5-base-long-livedoor-news-corpus](https://huggingface.co/llm-book/t5-base-long-livedoor-news-corpus) - llm-book/t5-base-long-livedoor-news-corpus「大規模言語モデル入門」の第7章で紹介している要約生成のモデルです。 retrieva-jp/t5-base-longをllm-book/livedoor-news-corpusでファインチューニングして構築されています。
 * 📥 303 [eepj/wstcg-mt-ja-en](https://huggingface.co/eepj/wstcg-mt-ja-en) - WS TCG Card Text TranslatorA Japanese-English machine translation model specifically trained for translating card text from the Weiss Schwarz (WS) Trading Card Game, fine-tuned on Helsinki-NLP/opus-mt-ja-en. Hugging Face Space DemoCheck out the demo at https://huggingface.co/spaces/eepj/wstcg-mt.DatasetOfficial WS Card ListJapanese-English parallel card text comprising 6000+ card text retrieved from the offical card list.
 * 📥 297 [nlp-waseda/roberta-large-japanese](https://huggingface.co/nlp-waseda/roberta-large-japanese) - nlp-waseda/roberta-large-japaneseModel descriptionThis is a Japanese RoBERTa large model pretrained on Japanese Wikipedia and the Japanese portion of CC-100.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained("nlp-waseda/roberta-large-japanese")model = AutoModelForMaskedLM.from_pretrained("nlp-waseda/roberta-large-japanese")sentence = '早稲田 大学 で 自然 言語 処理 を [MASK] する 。' # input should be
 * 📥 293 [stanfordnlp/stanza-ja](https://huggingface.co/stanfordnlp/stanza-ja) - Stanza model for Japanese (ja)Stanza is a collection of accurate and efficient tools for the linguistic analysis of many human languages. Starting from raw text to syntactic analysis and entity recognition, Stanza brings state-of-the-art NLP models to languages of your choosing.
 * 📥 288 [SakanaAI/EvoLLM-JP-v1-10B](https://huggingface.co/SakanaAI/EvoLLM-JP-v1-10B) - 🐟 EvoLLM-JP-v1-10B🤗 Models | 📚 Paper | 📝 Blog | 🐦 TwitterEvoLLM-JP-v1-10B is an experimental general-purpose Japanese LLM.This model was created using the Evolutionary Model Merge method.
 * 📥 286 [SakanaAI/EvoLLM-JP-A-v1-7B](https://huggingface.co/SakanaAI/EvoLLM-JP-A-v1-7B) - 🐟 EvoLLM-JP-A-v1-7B🤗 Models | 📚 Paper | 📝 Blog | 🐦 TwitterEvoLLM-JP-A-v1-7B is an experimental general-purpose Japanese LLM.This model was created using the Evolutionary Model Merge method.
 * 📥 283 [clu-ling/whisper-large-v2-japanese-5k-steps](https://huggingface.co/clu-ling/whisper-large-v2-japanese-5k-steps) - whisper-large-v2-japanese-5k-stepsThis model is a fine-tuned version of openai/whisper-large-v2 on the Japanese CommonVoice dataset (v11).. It achieves the following results on the evaluation set:Loss: 0.4200Wer: 0.7449Model descriptionThis model is finetuned for 5000 steps for research purposes which means that the transcriptions might not be that satisfactory for users.
 * 📥 280 [mmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-gguf) - line-corporation/japanese-large-lm-1.7b-instruction-sftline-corporationさんが公開しているjapanese-large-lm-1.7b-instruction-sftのgguf変換版です。他モデルはこちらGPT-NEOXmmnga/line-corp-japanese-large-lm-3.6b-ggufmmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-ggufGPT-2mmnga/line-corp-japanese-large-lm-1.7b-ggufmmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-gguf変換スクリプトline-gpt2_convert-hf-to-gguf.pyUsage (試用)git clone https://github.com/ggerganov/llama.cpp.gitcd llama.cppmake -j./main -m
 * 📥 276 [mmnga/line-corp-japanese-large-lm-1.7b-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-1.7b-gguf) - line-corporation/japanese-large-lm-1.7bline-corporationさんが公開しているjapanese-large-lm-1.7bのgguf変換版です。他モデルはこちらGPT-NEOXmmnga/line-corp-japanese-large-lm-3.6b-ggufmmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-ggufGPT-2mmnga/line-corp-japanese-large-lm-1.7b-ggufmmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-gguf変換スクリプトline-gpt2_convert-hf-to-gguf.pyUsagegit clone --branch mmnga-dev-merge https://github.com/mmnga/llama.cpp.gitcd llama.cppmake -j./main -m 'line-corp-japanese-large-lm-1.7b-q4_0.gguf'
 * 📥 270 [KoichiYasuoka/deberta-base-japanese-aozora-ud-head](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-aozora-ud-head) - deberta-base-japanese-aozora-ud-headModel DescriptionThis is a DeBERTa(V2) model pretrained on 青空文庫 for dependency-parsing (head-detection on long-unit-words) as question-answering, derived from deberta-base-japanese-aozora and UD_Japanese-GSDLUW.
 * 📥 266 [ku-nlp/gpt2-large-japanese-char](https://huggingface.co/ku-nlp/gpt2-large-japanese-char) - Model Card for Japanese character-level GPT-2 LargeModel descriptionThis is a Japanese character-level GPT-2 Large (717M parameters) language model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.How to useYou can use this model directly with a pipeline for text generation.&gt;&gt;&gt; from transformers import pipeline, set_seed&gt;&gt;&gt; generator = pipeline('text-generation', model='ku-nlp/gpt2-large-japanese-char')&gt;&gt;&gt; set_seed(5)&gt;&gt;&gt;
 * 📥 260 [stabilityai/japanese-stablelm-base-ja_vocab-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-base-ja_vocab-beta-7b) - Japanese-StableLM-Base-JAVocab-Beta-7BA cute robot wearing a kimono writes calligraphy with one single brush — Stable Diffusion XLModel Descriptionjapanese-stablelm-base-ja_vocab-beta-7b is a 7B-parameter decoder-only language model based on Llama-2-7b that has been fine-tuned on a diverse collection of Japanese data, with the intent of maximizing downstream performance on Japanese language tasks. Compared to the standard base model, this model uses a tokenizer with an expanded vocabulary derived from Japane
 * 📥 257 [mmnga/shisa-7b-v1-gguf](https://huggingface.co/mmnga/shisa-7b-v1-gguf) - shisa-7b-v1-ggufaugmxntさんが公開しているshisa-7b-v1のggufフォーマット変換版です。 Usagegit clone https://github.com/ggerganov/llama.cpp.gitcd llama.cppmake -j.
 * 📥 245 [stabilityai/japanese-stablelm-instruct-ja_vocab-beta-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-ja_vocab-beta-7b) - Japanese-StableLM-Instruct-JAVocab-Beta-7BA cute robot wearing a kimono writes calligraphy with one single brush — Stable Diffusion XLModel Descriptionjapanese-stablelm-instruct-ja_vocab-beta-7b is a 7B-parameter decoder-only language model based on japanese-stablelm-ja_vocab-beta-7b and further fine tuned on Databricks Dolly-15k, Anthropic HH, and other public data. Compared to the standard base model, this model uses a tokenizer with an expanded vocabulary derived from Japanese data.
 * 📥 235 [OrionStarAI/Orion-14B-Chat-Int4](https://huggingface.co/OrionStarAI/Orion-14B-Chat-Int4) - Orion-14B🌐English | 🇨🇳中文 | 🇯🇵日本語 | 🇰🇷한국어🤗 HuggingFace Mainpage | 🤖 ModelScope Mainpage🎬 HuggingFace Demo | 🎫 ModelScope Demo😺 GitHub📖 Tech ReportTable of Contents📖 Model Introduction🔗 Model Download🔖 Model Benchmark📊 Model Inference📜 Declarations &amp; License🥇 Company Introduction1. Model IntroductionOrion-14B series models are open-source multilingual large language models trained from scratch by OrionStarAI.  
 * 📥 229 [TFMC/ChatNTQ-JA-7b-v1.0-GGUF](https://huggingface.co/TFMC/ChatNTQ-JA-7b-v1.0-GGUF) - GGUF conversion of NTQAI/chatntq-ja-7b-v1.0ChatNTQ-JA-7b-v1.0 is a Japanese chat fine-tuned model built on top of the stabilityai/japanese-stablelm-base-gamma-7b, which is originally based on Mistral 7B v0.1.
 * 📥 228 [cameltech/japanese-gpt-1b-PII-masking](https://huggingface.co/cameltech/japanese-gpt-1b-PII-masking) - japanese-gpt-1b-PII-maskingModel Descriptionjapanese-gpt-1b-PII-masking は、 日本語事前学習済み1B GPTモデルをベースとして、日本語の文章から個人情報をマスキングするように学習したモデルです。 個人情報は以下の対応関係でマスキングされます。
 * 📥 227 [Mizuiro-sakura/luke-japanese-base-finetuned-ner](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-ner) - このモデルはluke-japanese-baseをファインチューニングして、固有表現抽出（NER）に用いれるようにしたものです。このモデルはluke-japanese-baseをWikipediaを用いた日本語の固有表現抽出データセット(ストックマーク社、https://github.com/stockmarkteam/ner-wikipedia-dataset )を用いてファインチューニングしたものです。固有表現抽出（NER）タスクに用いることができます。This model is fine-tuned model for Named-Entity-Recognition(NER) which is based on luke-japanese-baseThis model is fine-tuned by using Wikipedia dataset. You could use this model for NER tasks.モデルの精度 accuracy of modelprecisionrecallf1-scoresupportその他の組織名0.760.770.77238イベント名0.830.90
 * 📥 223 [tokyotech-llm/Swallow-13b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-13b-NVE-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data. The tuned versions use supervised fine-tuning (SFT).Links to other models can be found in the index.
 * 📥 215 [mmnga/SakanaAI-EvoLLM-JP-v1-7B-gguf](https://huggingface.co/mmnga/SakanaAI-EvoLLM-JP-v1-7B-gguf) - SakanaAI-EvoLLM-JP-v1-7B-ggufSakanaAIさんが公開しているEvoLLM-JP-v1-7Bのggufフォーマット変換版です。 こちらはベースモデルになります。
 * 📥 214 [sonoisa/t5-base-japanese-v1.1](https://huggingface.co/sonoisa/t5-base-japanese-v1.1) - 日本語T5事前学習済みモデルThis is a T5 (Text-to-Text Transfer Transformer) model pretrained on Japanese corpus. 次の日本語コーパス（約100GB）を用いて事前学習を行ったT5 (Text-to-Text Transfer Transformer) v1.1アーキテクチャのモデルです。
 * 📥 211 [NTQAI/wav2vec2-large-japanese](https://huggingface.co/NTQAI/wav2vec2-large-japanese) - Wav2Vec2-Large-JapaneseFine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the Common Voice, JSUT, TEDxJP and some other data. This model is a model trained on public data.
 * 📥 206 [minutillamolinara/bert-japanese_finetuned-sentiment-analysis](https://huggingface.co/minutillamolinara/bert-japanese_finetuned-sentiment-analysis) - bert-japanese_finetuned-sentiment-analysisThis model was trained from scratch on the Japanese Sentiment Polarity Dictionary dataset. Pre-trained modeljarvisx17/japanese-sentiment-analysisLink : https://huggingface.co/jarvisx17/japanese-sentiment-analysisTraining DataThe model was trained on Japanese Sentiment Polarity Dictionary dataset.link :
 * 📥 203 [sonoisa/clip-vit-b-32-japanese-v1](https://huggingface.co/sonoisa/clip-vit-b-32-japanese-v1) - 日本語版CLIPモデルThis is a CLIP text/image encoder model for Japanese. 英語版CLIPモデルのテキストエンコーダーを一種の蒸留を用いて日本語化したモデルです。
 * 📥 203 [mmnga/Deepreneur-blue-lizard-gguf](https://huggingface.co/mmnga/Deepreneur-blue-lizard-gguf) - Deepreneur-blue-lizard-ggufDeepreneurさんが公開しているblue-lizardのggufフォーマット変換版です。 モデルサイズは7Bになります。
 * 📥 202 [studio-ousia/luke-japanese-large-lite](https://huggingface.co/studio-ousia/luke-japanese-large-lite) - luke-japanese-large-liteluke-japanese is the Japanese version of LUKE (LanguageUnderstanding with Knowledge-based Embeddings), a pre-trainedknowledge-enhanced contextualized representation of words and entities. LUKEtreats words and entities in a given text as independent tokens, and outputscontextualized representations of them.
 * 📥 202 [tokyotech-llm/Swallow-70b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-70b-instruct-v0.1) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data. The tuned versions use supervised fine-tuning (SFT).Links to other models can be found in the index.
 * 📥 201 [tokyotech-llm/Swallow-13b-instruct-v0.1](https://huggingface.co/tokyotech-llm/Swallow-13b-instruct-v0.1) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data. The tuned versions use supervised fine-tuning (SFT).Links to other models can be found in the index.
 * 📥 200 [stabilityai/japanese-stable-vlm](https://huggingface.co/stabilityai/japanese-stable-vlm) - By downloading, using, or distributing any portion or element of this model, you agree to be bound by the agreement described in the LICENSE file.
 * 📥 199 [rinna/japanese-stable-diffusion](https://huggingface.co/rinna/japanese-stable-diffusion) - One more step before getting this model. This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.
 * 📥 190 [reazon-research/reazonspeech-espnet-next](https://huggingface.co/reazon-research/reazonspeech-espnet-next) - reazonspeech-espnet-nextReazonSpeech is a project to maintain freely-available Japanese audiodatasets and ML models.reazonspeech-espnet-next is a "bleeding-edge" repository that containslatest ASR models trained by ReazonSpeech team. We maintain this repository because we want to make our latest researchresults readily available, and try to incorporate feedback from communityas quickly as possible.
 * 📥 186 [tokyotech-llm/Swallow-70b-NVE-instruct-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-NVE-instruct-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data. The tuned versions use supervised fine-tuning (SFT).Links to other models can be found in the index.
 * 📥 185 [hotchpotch/japanese-reranker-cross-encoder-base-v1](https://huggingface.co/hotchpotch/japanese-reranker-cross-encoder-base-v1) - hotchpotch/japanese-reranker-cross-encoder-base-v1日本語で学習させた Reranker (CrossEncoder) シリーズです。モデル名layershidden_sizehotchpotch/japanese-reranker-cross-encoder-xsmall-v16384hotchpotch/japanese-reranker-cross-encoder-small-v112384hotchpotch/japanese-reranker-cross-encoder-base-v112768hotchpotch/japanese-reranker-cross-encoder-large-v1241024hotchpotch/japanese-bge-reranker-v2-m3-v1241024Reranker についてや、技術レポート・評価等は以下を参考ください。日本語最高性能のRerankerをリリース / そもそも Reranker とは?日本語 Reranker 作成のテクニカルレポート使い方SentenceTransformersfrom
 * 📥 183 [Mizuiro-sakura/deberta-v2-base-japanese-finetuned-QAe](https://huggingface.co/Mizuiro-sakura/deberta-v2-base-japanese-finetuned-QAe) - このモデルはdeberta-v2-base-japaneseをファインチューニングしてQAタスクに用いれるようにしたものです。このモデルはdeberta-v2-base-japaneseを運転ドメインQAデータセット（DDQA）（　https://nlp.ist.i.kyoto-u.ac.jp/index.php?Driving%20domain%20QA%20datasets　）を用いてファインチューニングしたものです。Question-Answeringタスク（SQuAD）に用いることができます。This model is fine-tuned model for Question-Answering which is based on deberta-v2-base-japaneseThis model is fine-tuned by using DDQA dataset. You could use this model for Question-Answering tasks.
 * 📥 173 [mmnga/line-corp-japanese-large-lm-3.6b-gguf](https://huggingface.co/mmnga/line-corp-japanese-large-lm-3.6b-gguf) - line-corporation/japanese-large-lm-3.6bline-corporationさんが公開しているjapanese-large-lm-3.6bのgguf変換版です。他モデルはこちらGPT-NEOXmmnga/line-corp-japanese-large-lm-3.6b-ggufmmnga/line-corp-japanese-large-lm-3.6b-instruction-sft-ggufGPT-2mmnga/line-corp-japanese-large-lm-1.7b-ggufmmnga/line-corp-japanese-large-lm-1.7b-instruction-sft-gguf注意:こちらはブランチで試用になります。llama.cpp本家にgptneox, gpt2が実装された時に、このggufファイルが使用できない可能性があります。GitHubリポジトリの readme はこちらUsage (試用)git clone --branch mmnga-dev https://github.com/mmnga/llama.cpp.gitcd llama.
 * 📥 171 [retrieva-jp/t5-large-short](https://huggingface.co/retrieva-jp/t5-large-short) - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus. Model detailsT5 is a Transformer-based Encoder-Decoder model, now in v1.1, with the following improvements over the original T5.GEGLU activation in feed-forward hidden layer, rather than ReLU - see
 * 📥 166 [studio-ousia/luke-japanese-base](https://huggingface.co/studio-ousia/luke-japanese-base) - luke-japaneseluke-japanese is the Japanese version of LUKE (Language Understanding with Knowledge-based Embeddings), a pre-trained knowledge-enhanced contextualized representation of words and entities. LUKE treats words and entities in a given text as independent tokens, and outputs contextualized representations of them.
 * 📥 165 [Mizuiro-sakura/luke-japanese-base-finetuned-jsts](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-finetuned-jsts) - このモデルはluke-japanese-baseをファインチューニングして、JSTS(文章の類似度計算)に用いれるようにしたものです。 このモデルはluke-japanese-baseをyahoo japan/JGLUEのJSTS( https://github.com/yahoojapan/JGLUE )を用いてファインチューニングしたものです。
 * 📥 161 [Local-Novel-LLM-project/Ninja-v1](https://huggingface.co/Local-Novel-LLM-project/Ninja-v1) - Our ModelsVecteusNinja-v1Ninja-v1-NSFWNinja-v1-128kNinja-v1-NSFW-128kModel Card for Ninja-v1.0The Mistral-7B--based Large Language Model (LLM) is an noveldataset fine-tuned version of the Mistral-7B-v0.1Ninja has the following changes compared to Mistral-7B-v0.1.Achieving both high quality Japanese and English generationMemory ability that does not forget even after long-context generationThis model was created with the help of GPUs from the first LocalAI hackathon. We would like to take this opportunity to
 * 📥 161 [nlp-waseda/roberta-large-japanese-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-large-japanese-with-auto-jumanpp) - nlp-waseda/roberta-large-japanese-with-auto-jumanppModel descriptionThis is a Japanese RoBERTa large model pretrained on Japanese Wikipedia and the Japanese portion of CC-100.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained("nlp-waseda/roberta-large-japanese-with-auto-jumanpp")model = AutoModelForMaskedLM.from_pretrained("nlp-waseda/roberta-large-japanese-with-auto-jumanpp")sentence
 * 📥 161 [ThePioneer/MoeDiffusionPlusPlus](https://huggingface.co/ThePioneer/MoeDiffusionPlusPlus) - モデル説明 (model explanation)V1 = MoeDiffusion 1.0 + (HassanBlend 1.5 - VMix03) * 0.2V2 = MoeDiffusion 0.6 : HassanBlend 1.5 0.2 : VMix03 : 0.2マージ元のルーツにNAIリークやInsta系モデルが含まれるという噂があるので、NAIリークアンチ・Insta系モデルアンチには非推奨理想の黒髪ポニテ顔が出せるYaguruMagikuを、ある程度顔が近くて制御しやすいAbyssOrangeMix2と混ぜてみた。 よってマージ者の目的である黒髪ポニテ以外の動作は興味ないし、知らない。
 * 📥 160 [rinna/nue-asr](https://huggingface.co/rinna/nue-asr) - rinna/nue-asrOverview[Paper][GitHub]We propose a novel end-to-end speech recognition model, Nue ASR, which integrates pre-trained speech and language models. The name Nue comes from the Japanese word (鵺/ぬえ/Nue), one of the Japanese legendary creatures (妖怪/ようかい/Yōkai).This model provides end-to-end Japanese speech recognition with recognition accuracy comparable to the recent ASR models.
 * 📥 155 [owner203/japanese-llama-2-13b-gguf](https://huggingface.co/owner203/japanese-llama-2-13b-gguf) - Japanese-LLaMA-2-13B-GGUFJapanese-LLaMA-2-13B-GGUFはJapanese-LLaMA-2-13BのGGUF形式です。モデルURL：https://huggingface.co/owner203/japanese-llama-2-13b
 * 📥 155 [DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUF](https://huggingface.co/DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUF) - DavidAU/alpaca-guanaco-japanese-gpt-1b-Q8_0-GGUFThis model was converted to GGUF format from inu-ai/alpaca-guanaco-japanese-gpt-1b using llama.cpp via the ggml.ai's GGUF-my-repo space. Refer to the original model card for more details on the model.
 * 📥 153 [sappho192/jesc-ja-en-translator](https://huggingface.co/sappho192/jesc-ja-en-translator) - Japanese to English translatorJapanese to English translator model based on EncoderDecoderModel(bert-japanese+GPT2)UsageDemoPlease visit https://huggingface.co/spaces/sappho192/jesc-ja-en-translator-demoDependencies (PyPI)torchtransformersfugashiunidic-liteInferenceimport transformersimport torchencoder_model_name = "cl-tohoku/bert-base-japanese-v2"decoder_model_name = "openai-community/gpt2"src_tokenizer = transformers. BertJapaneseTokenizer.from_pretrained(encoder_model_name)trg_tokenizer
 * 📥 153 [Vsukiyaki/Yaki-Dofu-Mix](https://huggingface.co/Vsukiyaki/Yaki-Dofu-Mix) - Yaki-Dofu-Mix概要 / OverviewYaki-Dofu-Mixは、アニメ風の画風に特化したマージモデルです。 / Yaki-Dofu-Mix is a merge model that specializes in an anime-like painting style. VAEなしでも鮮やかな色合いで出力されます。
 * 📥 145 [Deepreneur/blue-lizard](https://huggingface.co/Deepreneur/blue-lizard) - Deepreneur-blue-lizardModel DescriptionDeepreneur-blue-lizardは、MetaのLlama-2-7bに対して、Wikipediaや書籍等の日本語の学習データを用いて追加事前学習と独自データによるファインチューニングを実施したモデルです。 70億パラメータと非常に軽量なモデルであるにも関わらず、JGLUE（日本語タスクにおける評価ベンチマーク）を用いた評価では、ChatGPT-3.5を超えるスコアが算出されており、公開されている日本語モデルの中では最高性能になります。
 * 📥 138 [rinna/nekomata-7b-instruction-gguf](https://huggingface.co/rinna/nekomata-7b-instruction-gguf) - rinna/nekomata-7b-instruction-ggufOverviewThe model is the GGUF version of rinna/nekomata-7b-instruction. It can be used with llama.cpp for lightweight inference.
 * 📥 138 [tokyotech-llm/Swallow-70b-NVE-hf](https://huggingface.co/tokyotech-llm/Swallow-70b-NVE-hf) - SwallowOur Swallow model has undergone continual pre-training from the Llama 2 family, primarily with the addition of Japanese language data. The tuned versions use supervised fine-tuning (SFT).Links to other models can be found in the index.
 * 📥 137 [sonoisa/t5-base-japanese-title-generation](https://huggingface.co/sonoisa/t5-base-japanese-title-generation) - 記事本文からタイトルを生成するモデルSEE: https://qiita.com/sonoisa/items/a9af64ff641f0bbfed44
 * 📥 137 [izumi-lab/deberta-v2-base-japanese](https://huggingface.co/izumi-lab/deberta-v2-base-japanese) - DeBERTa V2 base JapaneseThis is a DeBERTaV2 model pretrained on Japanese texts. The codes for the pretraining are available at retarfi/language-pretraining.
 * 📥 136 [nold/Orion-14B-Base-GGUF](https://huggingface.co/nold/Orion-14B-Base-GGUF) - Orion-14B🌐English | 🇨🇳中文 | 🇯🇵日本語 |🇰🇷한국어🤗 HuggingFace Mainpage | 🤖 ModelScope Mainpage🎬 HuggingFace Demo | 🎫 ModelScope Demo😺 GitHub📖 Tech ReportTable of Contents📖 Model Introduction🔗 Model Download🔖 Model Benchmark📊 Model Inference📜 Declarations &amp; License🥇 Company Introduction1. Model IntroductionOrion-14B series models are open-source multilingual large language models trained from scratch by OrionStarAI.  
 * 📥 134 [stabilityai/japanese-stablelm-instruct-alpha-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b) - This repository is publicly accessible, but you have to accept the conditions to access its files and content.
 * 📥 130 [toshi456/llava-jp-1.3b-v1.0](https://huggingface.co/toshi456/llava-jp-1.3b-v1.0) - LLaVA-JP Model CardModel detailModel type:LLaVA-JP is a vision-language model that can converse about input images. This model was trained by fine-tuning  llm-jp/llm-jp-1.3b-v1.0 using LLaVA method.
 * 📥 128 [retrieva-jp/t5-small-medium](https://huggingface.co/retrieva-jp/t5-small-medium) - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus. Model detailsT5 is a Transformer-based Encoder-Decoder model, now in v1.1, with the following improvements over the original T5.GEGLU activation in feed-forward hidden layer, rather than ReLU - see
 * 📥 127 [izumi-lab/deberta-v2-small-japanese](https://huggingface.co/izumi-lab/deberta-v2-small-japanese) - DeBERTa V2 small JapaneseThis is a DeBERTaV2 model pretrained on Japanese texts. The codes for the pretraining are available at retarfi/language-pretraining.
 * 📥 121 [skytnt/gpt2-japanese-lyric-small](https://huggingface.co/skytnt/gpt2-japanese-lyric-small) - Japanese GPT2 Lyric ModelModel descriptionThe model is used to generate Japanese lyrics. You can try it on my website https://lyric.fab.moe/How to useimport torchfrom transformers import T5Tokenizer, GPT2LMHeadModeltokenizer = T5Tokenizer.from_pretrained("skytnt/gpt2-japanese-lyric-small")model = GPT2LMHeadModel.from_pretrained("skytnt/gpt2-japanese-lyric-small")def gen_lyric(prompt_text: str):prompt_text = "&lt;s&gt;" + prompt_text.replace("\n", "\\n ")prompt_tokens = tokenizer.tokenize(prompt_text)prompt_t
 * 📥 120 [nu-dialogue/sfc2022-stable-diffusion](https://huggingface.co/nu-dialogue/sfc2022-stable-diffusion) - SFCOCO Stable Diffusion Model CardSFCOCO Stable Diffusion is a Japanese-specific latent text-to-image diffusion model capable of generating photo-realistic images given any text input. This model was fine-tuned by using a powerful Japanese-specific latent text-to-image diffusion model, Japanese Stable Diffusion.
 * 📥 120 [Local-Novel-LLM-project/Ninja-v1-128k](https://huggingface.co/Local-Novel-LLM-project/Ninja-v1-128k) - Our ModelsVecteusNinja-v1Ninja-v1-NSFWNinja-v1-128kNinja-v1-NSFW-128kModel Card for Ninja-v1-128kThe Mistral-7B--based Large Language Model (LLM) is an noveldataset fine-tuned version of the Mistral-7B-v0.1Ninja-128k has the following changes compared to Mistral-7B-v0.1.128k context window (8k context in v0.1)Achieving both high quality Japanese and English generationMemory ability that does not forget even after long-context generationThis model was created with the help of GPUs from the first LocalAI hack
 * 📥 119 [ku-nlp/bart-base-japanese](https://huggingface.co/ku-nlp/bart-base-japanese) - Model Card for Japanese BART baseModel descriptionThis is a Japanese BART base model pre-trained on Japanese Wikipedia.
 * 📥 116 [Local-Novel-LLM-project/Ninja-v1-GGUF](https://huggingface.co/Local-Novel-LLM-project/Ninja-v1-GGUF) - Ninja-v1 のGGUF版Our Models for GGUFVecteus-GGUFNinja-v1-GGUFNinja-v1-NSFW-GGUFNinja-v1-128k-GGUFNinja-v1-NSFW-128k-GGUF
 * 📥 116 [aerner/lm-v2](https://huggingface.co/aerner/lm-v2) - Aerner LM-v2事前学習から全部日本語で学習させたモデルのバージョン2です。 LLaMAベースで、24GBのVRAMで事前学習できる規模に小さなモデルです。
 * 📥 115 [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1) - Heron BLIP Japanese StableLM Base 7B v1Model DetailsHeron BLIP Japanese StableLM
 * 📥 115 [Spiral-AI/Spiral-RetNet-3b-base](https://huggingface.co/Spiral-AI/Spiral-RetNet-3b-base) - SpiralAI Spiral-RetNet-3b-baseWe have conducted pre-training from scratch on the RetNet (https://arxiv.org/abs/2307.08621) architecture model 3b using a mixed dataset of Japanese and English. This model is released primarily for the basic research of "retention mechanism".
 * 📥 114 [alfredplpl/suzume-poc](https://huggingface.co/alfredplpl/suzume-poc) - はじめにGoogleのGemma-2Bを日本語で使えるように継続事前学習を施した、商用利用可能なベースモデルです。小型なのでスマホや家電などに向いています。ただし、Instruction tuningが困難な可能性があります。Colabで試すmmngaさんが作った軽量版をColabで試すUsagefrom transformers import AutoTokenizer, AutoModelForCausalLMtokenizer = AutoTokenizer.from_pretrained("alfredplpl/suzume-poc")model = AutoModelForCausalLM.from_pretrained("alfredplpl/suzume-poc")input_text = """人工知能とは"""input_ids = tokenizer(input_text, return_tensors="pt")outputs = model.generate(**input_ids,max_new_tokens=64)print(tokenizer.decode(outputs[0])
 * 📥 112 [kit-nlp/bert-base-japanese-sentiment-cyberbullying](https://huggingface.co/kit-nlp/bert-base-japanese-sentiment-cyberbullying) - electra-base-cyberbullyingThis is a BERT Base model for the Japanese language finetuned for automatic cyberbullying detection. The model was based on daigo's BERT Base for Japanese sentiment analysis, and later finetuned on a balanced dataset created by unifying two datasets, namely "Harmful BBS Japanese comments dataset" and "Twitter Japanese cyberbullying dataset".
 * 📥 108 [stockmark/stockmark-13b-instruct](https://huggingface.co/stockmark/stockmark-13b-instruct) - Stockmark-13b-instructStockmark-13b-instruct is an instruction-tuned version of Stockmark-13b, a 13 billion parameter Japanese LLM. This model is developed by Stockmark Inc.
 * 📥 107 [inu-ai/dolly-japanese-gpt-1b](https://huggingface.co/inu-ai/dolly-japanese-gpt-1b) - 更新履歴2023年5月7日「oasst1-89k-ja」データセットを追加して対話システムに対応しました。 1024トークンまで会話履歴を保存できます。
 * 📥 106 [mmnga/Llama-3-70B-japanese-suzume-vector-v0.1](https://huggingface.co/mmnga/Llama-3-70B-japanese-suzume-vector-v0.1) - Model Card for Model ID実験モデルです / This is an experimental model.lightblue/suzume-llama-3-8B-japaneseと、meta-llama/Meta-Llama-3-8B-Instructの差分をchat-vectorアプローチで抽出し、meta-llama/Meta-Llama-3-70B-Instructに適用しました結果差分が小さいのかあまり変化がありませんでした今後は倍率など付与してみようと思います.手順/procedurechat_vector.ipynbjameta-llama/Meta-Llama-3-8B-Instructとlightblue/suzume-llama-3-8B-japaneseの差分を作成shapeが異なるので、差分をmeta-llama/Meta-Llama-3-70B-Instruct用にアップサンプリング前から 8-layer、最後から8-layerはそのまま適用中間layerを引き延ばして適用enCreate the difference between meta-llama/Meta
 * 📥 100 [Lasorco/spekulatius](https://huggingface.co/Lasorco/spekulatius) - spekulatiusマージしているとたまに出てくる「目的の意図とは違うのだけどなんだか消すにはもったいないモデル」をおすそ分けするシリーズです。 公開する以上最低限調整して出そうとは思いますがあまり期待はしないでください。
 * 📥 100 [recruit-jp/japanese-clip-vit-b-32-roberta-base](https://huggingface.co/recruit-jp/japanese-clip-vit-b-32-roberta-base) - recruit-jp/japanese-clip-vit-b-32-roberta-baseOverviewDeveloped by: Recruit Co., Ltd.Model type: Contrastive Language-Image Pretrained ModelLanguage(s): JapaneseLICENSE: CC-BY-4.0More details are described in our tech blog post.日本語CLIP学習済みモデルとその評価用データセットの公開Model DetailsThis model is a Japanese CLIP.
 * 📥 100 [natsusakiyomi/Riga_Collection](https://huggingface.co/natsusakiyomi/Riga_Collection) - Riga_collectionとは？ Riga_collectionは様々なモデルをマージしたモデルですHimawariMixと同じで背景や細部の表現が強いモデルですイメージ的にはHimawariMixをりがさんのアイディアを元にチューニングしたようなモデルです！
 * 📥 99 [Lasorco/Kokuwa](https://huggingface.co/Lasorco/Kokuwa) - Kokuwalamettaの改良でマージさせるモデル探しをしていたらKiwiMixという面白そうなモデルを見つけました。 LoRAでリリースされていたものがモデル化したようですが、それによってライセンスが明確になったので早速使わせていただくことにしたのがこれです。
 * 📥 97 [ku-nlp/gpt2-medium-japanese-char](https://huggingface.co/ku-nlp/gpt2-medium-japanese-char) - Model Card for Japanese character-level GPT-2 MediumModel descriptionThis is a Japanese character-level GPT-2 Medium (310M parameters) language model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.How to useYou can use this model directly with a pipeline for text generation.&gt;&gt;&gt; from transformers import pipeline, set_seed&gt;&gt;&gt; generator = pipeline('text-generation', model='ku-nlp/gpt2-medium-japanese-char')&gt;&gt;&gt; set_seed(5)&gt;&gt;&
 * 📥 97 [oshizo/japanese-sexual-moderation-v2](https://huggingface.co/oshizo/japanese-sexual-moderation-v2) - japanese-sexual-moderation-v2は、studio-ousia/luke-japanese-large-liteをファインチューニングしたモデルです。短文が性的かどうかをスコアリングします。regressionで学習しており、出力するスコアはおおむね0-1の範囲を取りますが負の値や1を超える値が出る場合があります。長い文章は学習しておらず、入力は改行単位で分割することを想定しています。0.0-0.2: 全く性的ではない0.2-0.4: ほとんど性的な内容を含まない0.4-0.6: 性的な内容を含む可能性がある0.6-0.8: 性的な内容を含んでいる0.8-1.0: 非常に性的な内容であるUsageimport torchfrom transformers import AutoModelForSequenceClassification, AutoTokenizermodel_id = "oshizo/japanese-sexual-moderation-v2"tokenizer = AutoTokenizer.from_pretrained(model_id)model = Auto
 * 📥 96 [sazyou-roukaku/AfterRealXL](https://huggingface.co/sazyou-roukaku/AfterRealXL) - こちらでアップロードできないので、civitaiにて先に公開しています。 AfterRealXL_beta2(https://civitai.com/models/150212/afterrealxl)
 * 📥 94 [ysakuramoto/mobilebert-ja](https://huggingface.co/ysakuramoto/mobilebert-ja) - MobileBERT 日本語事前学習済みモデル爆誕！！ AI関係の仕事をしている櫻本です。
 * 📥 93 [retrieva-jp/t5-base-medium](https://huggingface.co/retrieva-jp/t5-base-medium) - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus. Model detailsT5 is a Transformer-based Encoder-Decoder model, now in v1.1, with the following improvements over the original T5.GEGLU activation in feed-forward hidden layer, rather than ReLU - see
 * 📥 92 [aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2](https://huggingface.co/aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2) - Swallow-MX-8x7b-NVE-v0.1に対し、Mixtral-8x7B-Instruct-v0.1とMixtral-8x7B-v0.1の差分をマージしたモデルです。Swallow-MX-8x7b-NVE-v0.1 + 0.8*(Mixtral-8x7B-Instruct-v0.1 - Mixtral-8x7B-v0.1)aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct のアウトプットの語彙がおかしい場合に使用してください。日本語がより自然になりますcontext size 32k token使用可能な日本語対応ローカル用LLMとしては2024年3月時点では最高クラスの性能です
 * 📥 88 [esnya/japanese_speecht5_tts](https://huggingface.co/esnya/japanese_speecht5_tts) - SpeechT5 (TTS task) for JapaneseSpeechT5 model fine-tuned for Japanese speech synthesis (text-to-speech) on JVS.This model utilizes the JVS dataset which encompasses 100 speakers. From this dataset, speaker embeddings were crafted, segregating them based on male and female voice types, and producing a unique speaker embedding vector.
 * 📥 84 [kotoba-tech/kotoba-speech-v0.1](https://huggingface.co/kotoba-tech/kotoba-speech-v0.1) - Kotoba-Speech-v0.1Kotoba-Speech v0.1 is a 1.2B Transformer-based speech generative model. It supports the following properties:Fluent text-to-speech generation in JapaneseOne-shot voice cloning through speech promptUsagePlesae check out our HF Spaces demo.
 * 📥 84 [ThePioneer/MoeSharpV1](https://huggingface.co/ThePioneer/MoeSharpV1) - モデル説明 (model explanation)MoeDiffusionPlusPlus 0.7 : DreamShaper 3.3 (full) 0.3。 WaifuDiffusionのvaeと、StableDiffusionのvaeをbake inして、MoeDiffusion系の発色の弱さを改善。
 * 📥 83 [ThePioneer/MoeDiffusion](https://huggingface.co/ThePioneer/MoeDiffusion) - モデル説明 (model explanation)YaguruMagiku 0.6 : AbyssOrangeMix2_sfw 0.4マージ元のルーツにNAIリークが含まれるという噂があるので、NAIリークアンチには非推奨理想の黒髪ポニテ顔が出せるYaguruMagikuを、ある程度顔が近くて制御しやすいAbyssOrangeMix2と混ぜてみた。 よってマージ者の目的である黒髪ポニテ以外の動作は興味ないし、知らない。
 * 📥 83 [watashiha/watashiha-gpt-6b](https://huggingface.co/watashiha/watashiha-gpt-6b) - モデル概要AWSのtrn1インスタンスを用いて開発した大喜利言語モデルです。事前学習後に大喜利データでFine-tuningしています。Architecture: GPT2Vocab size: 44880Model size: 6B paramsLicense: Apache License 2.0Library: aws-neuron-reference-for-megatron-lm学習データ以下のコーパスを使用して、事前学習を行いました。その際のトークン数は477億トークンでした。C4の日本語データCC-100の日本語データOSCARの日本語データWikipediaの日本語ダンプデータ自社データFine-tuningは、693万件の大喜利データを用いて行いました。使用方法import torchfrom transformers import AutoModelForCausalLM, AutoTokenizermodel_name = "watashiha/watashiha-gpt-6b"tokenizer = AutoTokenizer.from_pretrained(model_name, u
 * 📥 81 [oshizo/donut-base-japanese-visual-novel](https://huggingface.co/oshizo/donut-base-japanese-visual-novel) - Donut (base-sized model, fine-tuned on visual novel like synthetic dataset )ビジュアルノベル風画像の合成データセットでnaver-clova-ix/donut-baseを訓練したモデルです。 使い方サンプルノートブックsample_predictions_colab.ipynbを参照してください。
 * 📥 80 [sappho192/aihub-ja-ko-translator](https://huggingface.co/sappho192/aihub-ja-ko-translator) - Japanese to Korean translatorJapanese to Korean translator model based on EncoderDecoderModel(bert-japanese+kogpt2)UsageDemoPlease visit https://huggingface.co/spaces/sappho192/aihub-ja-ko-translator-demoDependencies (PyPI)torchtransformersfugashiunidic-liteInferencefrom transformers import(EncoderDecoderModel,PreTrainedTokenizerFast,BertJapaneseTokenizer,)import torchencoder_model_name = "cl-tohoku/bert-base-japanese-v2"decoder_model_name = "skt/kogpt2-base-v2"src_tokenizer = BertJapaneseTokenizer.from_pre
 * 📥 79 [stabilityai/japanese-instructblip-alpha](https://huggingface.co/stabilityai/japanese-instructblip-alpha) - Japanese InstructBLIP AlphaModel DetailsJapanese InstructBLIP Alpha is a vision-language instruction-following model that enables to generate Japanese descriptions for input images and optionally input texts such as questions. UsageFirst install additional dependencies in requirements.txt:pip install sentencepiece einopsimport torchfrom transformers import LlamaTokenizer, AutoModelForVision2Seq, BlipImageProcessorfrom PIL import Imageimport requests# helper function to format input promptsdef build_prompt(pr
 * 📥 79 [ttop324/wav2vec2-live-japanese](https://huggingface.co/ttop324/wav2vec2-live-japanese) - wav2vec2-live-japanesehttps://github.com/ttop32/wav2vec2-live-japanese-translatorFine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese hiragana using thecommon_voiceJSUTCSS10TEDxJP-10KJVSJSSSInference#usageimport torchimport torchaudiofrom datasets import load_datasetfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Processormodel = Wav2Vec2ForCTC.from_pretrained("ttop324/wav2vec2-live-japanese")processor = Wav2Vec2Processor.from_pretrained("ttop324/wav2vec2-live-japanese")test_dataset = load_dataset("commo
 * 📥 79 [spow12/Visual-novel-transcriptor](https://huggingface.co/spow12/Visual-novel-transcriptor) - Model Card for Model IDFine tunned ASR model from distil-whisper/distil-large-v2.This model aimed to transcribe japanese audio especially visual novel. Model DetailsModel DescriptionThis is the model card of a 🤗 transformers model that has been pushed on the Hub.
 * 📥 78 [ushikado/yuyuyui-chatbot](https://huggingface.co/ushikado/yuyuyui-chatbot) - yuyuyui-chatbotThis model is based on rinna/japanese-gpt2-medium and finetuned on Yuyuyui scenario corpus. UsageThe model takes a sequence of utterances (context) to generate a subsequent utterance (response).
 * 📥 77 [abeja/Mixtral-8x7B-Instruct-v0.1-japanese](https://huggingface.co/abeja/Mixtral-8x7B-Instruct-v0.1-japanese) - Mixtral-8x7B-Instruct-v0.1-japaneseMixtral-8x7B-Instruct-v0.1-japaneseはMixtral-8x7B-Instruct-v0.1をベースに日本語の語彙拡張継続事前学習を実施したモデルです。詳細はABEJAのテックブログを参照してください。学習を実施したMetagton-LMのレポジトリはこちらです。使い方import torchfrom transformers import AutoModelForCausalLM, AutoTokenizermodel_name = "abeja/Mixtral-8x7B-Instruct-v0.1-japanese"tokenizer = AutoTokenizer.from_pretrained(model_name)model = AutoModelForCausalLM.from_pretrained(model_name,torch_dtype=torch.float16,use_cache=True,device_map="auto",)model.eval()input_text = """#
 * 📥 73 [owner203/japanese-alpaca-2-13b-gguf](https://huggingface.co/owner203/japanese-alpaca-2-13b-gguf) - Japanese-Alpaca-2-13B-GGUFJapanese-Alpaca-2-13B-GGUFはJapanese-Alpaca-2-13BのGGUF形式です。モデルURL：https://huggingface.co/owner203/japanese-alpaca-2-13b
 * 📥 73 [alabnii/jmedroberta-base-manbyo-wordpiece](https://huggingface.co/alabnii/jmedroberta-base-manbyo-wordpiece) - alabnii/jmedroberta-base-manbyo-wordpieceModel descriptionThis is a Japanese RoBERTa base model pre-trained on academic articles in medical sciences collected by Japan Science and Technology Agency (JST).This model is released under the Creative Commons 4.0 International License (CC BY-NC-SA 4.0).ReferenceJa:@InProceedings{sugimoto_nlp2023_jmedroberta,author =    "杉本海人 and 壹岐太一 and 知田悠生 and 金沢輝一 and 相澤彰子",title =     "J{M}ed{R}o{BERT}a: 日本語の医学論文にもとづいた事前学習済み言語モデルの構築と評価",booktitle = "言語処理学会第29回年次大会",year =   
 * 📥 72 [sambanovasystems/SambaLingo-Japanese-Chat](https://huggingface.co/sambanovasystems/SambaLingo-Japanese-Chat) - SambaLingo-Japanese-ChatSambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English. It is trained using direct preference optimization on top the base model SambaLingo-Japanese-Base.
 * 📥 72 [alfredplpl/gemma-2b-it-ja-poc-3](https://huggingface.co/alfredplpl/gemma-2b-it-ja-poc-3) - はじめになんか日本語が話せる商用利用可能なAIです。小型なのでスマホや家電などに向いています。Usagefrom transformers import AutoTokenizer, AutoModelForCausalLMimport torchfrom peft import PeftModel# トークナイザーとモデルの準備tokenizer = AutoTokenizer.from_pretrained("alfredplpl/ja-aozora-wikipedia-gemmba-2b")model = AutoModelForCausalLM.from_pretrained("alfredplpl/ja-aozora-wikipedia-gemmba-2b")model = PeftModel.from_pretrained(model = model, model_id = "alfredplpl/gemma-2b-it-ja-poc-3")# プロンプトの準備prompt="""あなたは親切なアシスタントです。英語は喋らず、日本語だけ喋ってください。&lt;start_of_turn&gt;us
 * 📥 70 [kit-nlp/bert-base-japanese-sentiment-irony](https://huggingface.co/kit-nlp/bert-base-japanese-sentiment-irony) - BERT Base Japanese for IronyThis is a BERT Base model for sentiment analysis in Japanese additionally finetuned for automatic irony detection. The model was based on bert-base-japanese-sentiment, and later finetuned on a dataset containing ironic and sarcastic tweets.
 * 📥 69 [ken11/albert-base-japanese-v1-with-japanese-tokenizer](https://huggingface.co/ken11/albert-base-japanese-v1-with-japanese-tokenizer) - albert-base-japanese-v1-with-japanese日本語事前学習済みALBERTモデルですこのモデルではTokenizerにBertJapaneseTokenizerクラスを利用していますalbert-base-japanese-v1よりトークナイズ処理が楽になっていますHow to useファインチューニングこのモデルはPreTrainedモデルです基本的には各種タスク用にファインチューニングして使用されることを想定していますFill-Maskfor PyTorchfrom transformers import (AutoModelForMaskedLM, AutoTokenizer)tokenizer = AutoTokenizer.from_pretrained("ken11/albert-base-japanese-v1-with-japanese-tokenizer")model = AutoModelForMaskedLM.from_pretrained("ken11/albert-base-japanese-v1-with-japanese-tokenizer")tex
 * 📥 67 [sonoisa/sentence-bert-base-ja-en-mean-tokens](https://huggingface.co/sonoisa/sentence-bert-base-ja-en-mean-tokens) - This is a Japanese+English sentence-BERT model. 日本語+英語用Sentence-BERTモデルです。日本語のみバージョンと比べて、手元の非公開データセットでは日本語の精度が0.8pt低く、英語STSbenchmarkでは精度が8.3pt高い（Cosine-Similarity Spearmanが79.11%）結果が得られました。事前学習済みモデルとしてcl-tohoku/bert-base-japanese-whole-word-maskingを利用しました。推論の実行にはfugashiとipadicが必要です（pip install fugashi ipadic）。日本語のみバージョンの解説https://qiita.com/sonoisa/items/1df94d0a98cd4f209051モデル名を"sonoisa/sentence-bert-base-ja-en-mean-tokens"に書き換えれば、本モデルを利用した挙動になります。使い方from transformers import BertJapaneseTokenizer, BertModeli
 * 📥 65 [rinna/nekomata-7b-gguf](https://huggingface.co/rinna/nekomata-7b-gguf) - rinna/nekomata-7b-ggufOverviewThe model is the GGUF version of rinna/nekomata-7b. It can be used with llama.cpp for lightweight inference.
 * 📥 65 [nlp-waseda/gpt2-small-japanese](https://huggingface.co/nlp-waseda/gpt2-small-japanese) - nlp-waseda/gpt2-small-japaneseThis model is Japanese GPT-2 pretrained on Japanese Wikipedia and CC-100.Intended uses &amp; limitationsYou can use the raw model for text generation or fine-tune it to a downstream task. Note that the texts should be segmented into words using Juman++ in advance.
 * 📥 64 [AIBunCho/japanese-novel-gpt-j-6b](https://huggingface.co/AIBunCho/japanese-novel-gpt-j-6b) - AIBunCho/japanese-novel-gpt-j-6bAI BunChoで利用しているモデルです。2021年に作った小説用言語モデルです。Model DetailsGPT-J-6BをTPUで2週間日本語tokenizerを用いて日本語データで事前学習し、その後2週間小説データで転移学習したものです。UsesGoogle colabのT4 High-RAMで動作確認しています。pip install transformers sentencepiece acceleratefrom transformers import GPTJForCausalLM, AlbertTokenizerimport torchtokenizer = AlbertTokenizer.from_pretrained('AIBunCho/japanese-novel-gpt-j-6b', keep_accents=True, remove_space=False)model = GPTJForCausalLM.from_pretrained("AIBunCho/japanese-novel-gpt-j-6b", torch_
 * 📥 62 [offtoung/tsukuyomi-chan-calm2-7b](https://huggingface.co/offtoung/tsukuyomi-chan-calm2-7b) - つくよみちゃんデータセットを用いて calm-2-7b-chat をファインチューニングしたモデルです。 下記に記載のライセンスの範囲内でご自由に利用いただけます。
 * 📥 61 [sonoisa/sentence-t5-base-ja-mean-tokens](https://huggingface.co/sonoisa/sentence-t5-base-ja-mean-tokens) - This is a Japanese sentence-T5 model. 日本語用Sentence-T5モデルです。事前学習済みモデルとしてsonoisa/t5-base-japaneseを利用しました。推論の実行にはsentencepieceが必要です（pip install sentencepiece）。手元の非公開データセットでは、精度はsonoisa/sentence-bert-base-ja-mean-tokensと同程度です。使い方from transformers import T5Tokenizer, T5Modelimport torchclass SentenceT5:def __init__(self, model_name_or_path, device=None):self.tokenizer = T5Tokenizer.from_pretrained(model_name_or_path, is_fast=False)self.model = T5Model.from_pretrained(model_name_or_path).encoderself.model.eval()if
 * 📥 61 [sociocom/MedNERN-CR-JA](https://huggingface.co/sociocom/MedNERN-CR-JA) - This is a model for named entity recognition of Japanese medical documents. IntroductionThis repository contains the base model and a support predict script for using the model and providing a XML tagged text output.
 * 📥 60 [Helsinki-NLP/opus-mt-ja-it](https://huggingface.co/Helsinki-NLP/opus-mt-ja-it) - jpn-itasource group: Japanesetarget group: ItalianOPUS readme: jpn-itamodel: transformer-alignsource language(s): jpn jpn_Hani jpn_Hira jpn_Kana jpn_Latn jpn_Yiiitarget language(s): itamodel: transformer-alignpre-processing: normalization + SentencePiece (spm32k,spm32k)download original weights: opus-2020-06-17.ziptest set translations: opus-2020-06-17.test.txttest set scores: opus-2020-06-17.eval.txtBenchmarkstestsetBLEUchr-FTatoeba-test.jpn.ita22.80.460System Info:hf_name: jpn-itasource_languages: jpntarg
 * 📥 59 [mr4/bert-base-jp-sentiment-analysis](https://huggingface.co/mr4/bert-base-jp-sentiment-analysis) - Sentiment Analysis in Japanese - Phân tích cảm xúc trong tiếng NhậtBert phân tích cảm xúcModel descriptionMô hình có tác dụng xác định cảm xúc của đoạn văn. Sử dụng nhãn: "positive", "negative"Ví dụ:今日はいい天気ですねnegative: 6.001393558108248e-05positive: 0.999940037727356今日の食べ物はとてもつまらないnegative: 0.9999252557754517positive: 7.470489799743518e-05Base modelMô hình được đạo tạo dựa trên cơ sở của model Base JapaneseTraining dataMô hình được đào tạo dựa trên dữ liệu được thu thập bởi TAKAHIRO KUBO (https://www.kaggle.
 * 📥 57 [wolf4032/bert-japanese-token-classification-search-local-cuisine](https://huggingface.co/wolf4032/bert-japanese-token-classification-search-local-cuisine) - Model Card for Model ID料理を検索するための質問文から、検索検索用キーワードである固有表現を抽出しますModel DetailsModel Description例えば、「東京の肉料理で、春に食べられる、鶏肉を使った料理を教えてください」という文章を入力すると、「東京　→　都道府県/地方(AREA)」　「肉料理　→　種類(TYPE)」　「春　→　季節(SZN)」　「鶏肉　→　食材(INGR)」のように、固有表現を抽出します抽出対象は、AREA、TYPE、SZN、INGRの４つですLanguage(s) (NLP): 日本語License: mitFinetuned from model: tohoku-nlp/bert-base-japanese-v2Model SourcesRepository: wolf4032/nlp-token-classificationデータセット、言語モデル、アプリの作成に使ったコードが掲載されていますPaper: [More Information Needed]Demo: wolf4032/japanese-token-classification-s
 * 📥 56 [MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1](https://huggingface.co/MaziyarPanahi/japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1) - japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1japanese-stablelm-base-gamma-7b-Mistral-7B-Instruct-v0.1 is a merge of the following models:mistralai/Mistral-7B-Instruct-v0.1stabilityai/japanese-stablelm-base-gamma-7b🧩 Configurationslices:- sources:- model: mistralai/Mistral-7B-Instruct-v0.1layer_range:
 * 📥 56 [MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1](https://huggingface.co/MaziyarPanahi/japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1) - japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1japanese-stablelm-instruct-gamma-7b-Mistral-7B-Instruct-v0.1 is a merge of the following models:mistralai/Mistral-7B-Instruct-v0.1stabilityai/japanese-stablelm-instruct-gamma-7b🧩 Configurationslices:- sources:- model: mistralai/Mistral-7B-Instruct-v0.1layer_range:
 * 📥 56 [LoneStriker/SambaLingo-Japanese-Chat-GGUF](https://huggingface.co/LoneStriker/SambaLingo-Japanese-Chat-GGUF) - SambaLingo-Japanese-ChatSambaLingo-Japanese-Chat is a human aligned chat model trained in Japanese and English. It is trained using direct preference optimization on top the base model SambaLingo-Japanese-Base.
 * 📥 55 [patrickramos/bert-base-japanese-v2-wrime-fine-tune](https://huggingface.co/patrickramos/bert-base-japanese-v2-wrime-fine-tune) - WRIME-fine-tuned BERT base JapaneseThis model is a Japanese BERTBASE fine-tuned on the WRIME dataset. It was trained as part of the paper "Emotion Analysis of Writers and Readers of Japanese Tweets on Vaccinations".
 * 📥 55 [SkelterLabsInc/bert-base-japanese-jaquad](https://huggingface.co/SkelterLabsInc/bert-base-japanese-jaquad) - BERT base Japanese - JaQuADDescriptionA Japanese Question Answering model fine-tuned on JaQuAD.Please refer BERT base Japanese for details about the pre-training model. The codes for the fine-tuning are available at SkelterLabsInc/JaQuADEvaluation resultsOn the development set.{"f1": 77.35, "exact_match": 61.01}On the test set.{"f1": 78.92, "exact_match": 63.38}Usagefrom transformers import AutoModelForQuestionAnswering, AutoTokenizerquestion = 'アレクサンダー・グラハム・ベルは、どこで生まれたの?'context = 'アレクサンダー・グラハム・ベルは、スコットランド生
 * 📥 52 [ryota39/Phi-3-mini-4k-instruct-dpo](https://huggingface.co/ryota39/Phi-3-mini-4k-instruct-dpo) - モデルベースモデル：microsoft/Phi-3-mini-4k-instruct学習データセット：llm-jp/hh-rlhf-12k-ja学習方式：フルパラメータチューニングサンプルimport torchfrom transformers import AutoTokenizer, AutoModelForCausalLMtokenizer = AutoTokenizer.from_pretrained("ryota39/Phi-3-mini-4k-instruct-dpo",trust_remote_code=True,)model = AutoModelForCausalLM.from_pretrained("ryota39/Phi-3-mini-4k-instruct-dpo",device_map="auto",torch_dtype='auto',trust_remote_code=True,)text = "&lt;|user|&gt;\n与えられた質問に対して英語で思考し、日本語で答えてください。東京の観光地を教えてください。\n&lt;|end|&gt;\n&lt;|assistant
 * 📥 52 [megagonlabs/roberta-long-japanese](https://huggingface.co/megagonlabs/roberta-long-japanese) - roberta-long-japanese (jumanpp + sentencepiece, mC4 Japanese)This is the longer input version of RoBERTa Japanese model pretrained on approximately 200 M Japanese sentences.max_position_embeddings has been increased to 1282, allowing it to handle much longer inputs than the basic RoBERTa model.
 * 📥 52 [sonoisa/t5-base-english-japanese](https://huggingface.co/sonoisa/t5-base-english-japanese) - 英語+日本語T5事前学習済みモデルThis is a T5 (Text-to-Text Transfer Transformer) model pretrained on English and Japanese balanced corpus. 次の日本語コーパス（約500GB）を用いて事前学習を行ったT5 (Text-to-Text Transfer Transformer) モデルです。
 * 📥 52 [TeamFnord/manga-ocr](https://huggingface.co/TeamFnord/manga-ocr) - Manga OCROptical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework.
 * 📥 51 [colorfulscoop/gpt2-small-ja](https://huggingface.co/colorfulscoop/gpt2-small-ja) - GPT-2 small Japanese modelThis repository contains a GPT2-small model trained on Japanese Wikipedia dataset. Training dataJapanese Wikipedia dataset as of Aug20, 2021 released under Creative Commons Attribution-ShareAlike 3.0 is used for both tokenizer and GPT-2 model.
 * 📥 50 [Ivydata/whisper-base-japanese](https://huggingface.co/Ivydata/whisper-base-japanese) - Fine-tuned Japanese Whisper model for speech recognition using whisper-baseFine-tuned openai/whisper-base on Japanese using Common Voice, JVS and JSUT.When using this model, make sure that your speech input is sampled at 16kHz. UsageThe model can be used directly as follows.from transformers import WhisperForConditionalGeneration, WhisperProcessorfrom datasets import load_datasetimport librosaimport torchLANG_ID
 * 📥 49 [alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000](https://huggingface.co/alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000) - alabnii/jmedroberta-base-manbyo-wordpiece-vocab50000Model descriptionThis is a Japanese RoBERTa base model pre-trained on academic articles in medical sciences collected by Japan Science and Technology Agency (JST).This model is released under the Creative Commons 4.0 International License (CC BY-NC-SA 4.0).ReferenceJa:@InProceedings{sugimoto_nlp2023_jmedroberta,author =    "杉本海人 and 壹岐太一 and 知田悠生 and 金沢輝一 and 相澤彰子",title =     "J{M}ed{R}o{BERT}a: 日本語の医学論文にもとづいた事前学習済み言語モデルの構築と評価",booktitle = "言語処理学会第29回年次大会
 * 📥 45 [KoichiYasuoka/bert-base-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/bert-base-japanese-unidic-luw-upos) - bert-base-japanese-unidic-luw-uposModel DescriptionThis is a BERT model pre-trained on Japanese Wikipedia texts for POS-tagging and dependency-parsing, derived from bert-base-japanese-v2.
 * 📥 44 [nlp-waseda/comet-gpt2-small-japanese](https://huggingface.co/nlp-waseda/comet-gpt2-small-japanese) - COMET-GPT2 jaFinetuned GPT-2 on ATOMIC ja using a causal language modeling (CLM) objective. It was introduced in this paper.
 * 📥 43 [kkuramitsu/mt5-mini9L](https://huggingface.co/kkuramitsu/mt5-mini9L) - Model Card for Model IDThis is a small T5 (Text-to-Text Transfer Transformer) model pretrained on Japanese and English corpus. This modelcard aims to be a base template for new models.
 * 📥 42 [drewschaub/whisper-large-v3-japanese-4k-steps](https://huggingface.co/drewschaub/whisper-large-v3-japanese-4k-steps) - whisper-large-v3-japanese-4k-stepsThis model is a fine-tuned version of openai/whisper-large-v3 on the Common Voice 16.1 dataset. I followed a post by Sanchit Gandhi, https://huggingface.co/blog/fine-tune-whisperIt took 24 hours using an A100 on Google Colab to complete 4000 steps using the Common Voice 16.1 dataset.
 * 📥 42 [furnqse/elyza-fork2](https://huggingface.co/furnqse/elyza-fork2) - ELYZA-japanese-Llama-2-7bModel DescriptionELYZA-japanese-Llama-2-7b は、 Llama2をベースとして日本語能力を拡張するために追加事前学習を行ったモデルです。詳細は Blog記事 を参照してください。Usageimport torchfrom transformers import AutoModelForCausalLM, AutoTokenizerB_INST, E_INST = "[INST]", "[/INST]"B_SYS, E_SYS = "&lt;&lt;SYS&gt;&gt;\n", "\n&lt;&lt;/SYS&gt;&gt;\n\n"DEFAULT_SYSTEM_PROMPT = "あなたは誠実で優秀な日本人のアシスタントです。"text = "クマが海辺に行ってアザラシと友達になり、最終的には家に帰るというプロットの短編小説を書いてください。"model_name = "elyza/ELYZA-japanese-Llama-2-7b-instruct"tokenizer = AutoTokenizer.from_pre
 * 📥 41 [megagonlabs/t5-base-japanese-web-8k](https://huggingface.co/megagonlabs/t5-base-japanese-web-8k) - t5-base-japanese-web-8k (with Byte-fallback, 8K)Descriptionmegagonlabs/t5-base-japanese-web-8k is a T5 (Text-to-Text Transfer Transformer) model pre-trained on Japanese web texts. Training codes are available on GitHub.
 * 📥 41 [alfredplpl/gemma-2b-it-ja-poc](https://huggingface.co/alfredplpl/gemma-2b-it-ja-poc) - Noteこのモデルはマージに失敗してバグっているため、こちらをおすすめします。Google ColabUsagefrom transformers import AutoTokenizer, AutoModelForCausalLMimport torch# トークナイザーとモデルの準備tokenizer = AutoTokenizer.from_pretrained("alfredplpl/gemma-2b-it-ja-poc")model = AutoModelForCausalLM.from_pretrained("alfredplpl/gemma-2b-it-ja-poc")# プロンプトの準備prompt="""あなたは親切なアシスタントです。英語は喋らず、日本語だけ喋ってください。&lt;start_of_turn&gt;user人生で大切なことはなんですか？&lt;end_of_turn&gt;&lt;start_of_turn&gt;model"""# 推論の実行input_ids = tokenizer(prompt, return_tensors="pt").to(model.device
 * 📥 40 [sonoisa/byt5-small-japanese](https://huggingface.co/sonoisa/byt5-small-japanese) - 日本語ByT5事前学習済みモデルThis is a ByT5 (a tokenizer-free extension of the Text-to-Text Transfer Transformer) model pretrained on Japanese corpus. 次の日本語コーパス（約100GB）を用いて事前学習を行ったByT5 (a tokenizer-free extension of the Text-to-Text Transfer Transformer) モデルです。
 * 📥 40 [Formzu/bert-base-japanese-jsnli](https://huggingface.co/Formzu/bert-base-japanese-jsnli) - bert-base-japanese-jsnliThis model is a fine-tuned version of cl-tohoku/bert-base-japanese-v2 on the JSNLI dataset. It achieves the following results on the evaluation set:Loss: 0.2085Accuracy: 0.9288How to use the modelSimple zero-shot classification pipelinefrom transformers import pipelineclassifier = pipeline("zero-shot-classification", model="Formzu/bert-base-japanese-jsnli")sequence_to_classify = "いつか世界を見る。"candidate_labels =
 * 📥 40 [abhishek/autonlp-japanese-sentiment-59363](https://huggingface.co/abhishek/autonlp-japanese-sentiment-59363) - Model Trained Using AutoNLPProblem type: Binary ClassificationModel ID: 59363Validation MetricsLoss: 0.12651239335536957Accuracy: 0.9532079853817648Precision: 0.9729688278823665Recall: 0.9744633462616643AUC: 0.9717333684823413F1: 0.9737155136027014UsageYou can use cURL to access this model:$ curl -X POST -H "Authorization: Bearer YOUR_API_KEY" -H "Content-Type: application/json" -d '{"inputs": "I love AutoNLP"}' https://api-inference.huggingface.co/models/abhishek/autonlp-japanese-sentiment-59363Or Python A
 * 📥 39 [uzabase/luke-japanese-wordpiece-base](https://huggingface.co/uzabase/luke-japanese-wordpiece-base) - studio-ousia/luke-japanese-baseに対して次の変更を加えたモデルです。ベースのモデルをRoBERTaから日本語BERTに切り替え、それに伴ってトークナイザがSentencepieceからWordPieceになりました2023年7月1日時点の日本語Wikipediaのデータで事前学習をおこないました[UNK] (unknown) エンティティを扱えるようにしました詳細はブログ記事をご参照ください。使用方法from transformers import AutoTokenizer, AutoModel# 本モデル用のトークナイザのコードを使用するため、trust_remote_code=True の指定が必要ですtokenizer = AutoTokenizer.from_pretrained("uzabase/luke-japanese-wordpiece-base", trust_remote_code=True)model = AutoModel.from_pretrained("uzabase/luke-japanese-wordpiece-base")更新情報2023/11
 * 📥 38 [daisaku-s/medtxt_ner_roberta](https://huggingface.co/daisaku-s/medtxt_ner_roberta) - 日本語医療固有表現抽出モデル概要ソーシャル・コンピューティング研究室さまより公開されているMedTxt-CRを用いて、alabniiさまより公開されているRoBERTaをfine-tuningした固有表現抽出モデルです。 文中のトークンに対して以下のいずれかのタグをIOB2形式で付与します (PositiveやNegativeなどの事実性が同時に付与されるタグもあります)。
 * 📥 37 [recruit-jp/japanese-typo-detector-roberta-base](https://huggingface.co/recruit-jp/japanese-typo-detector-roberta-base) - recruit-jp/japanese-typo-detector-roberta-baseモデルの概要日本語の文章を入力すると各文字ごとに誤字脱字である確率を出力します各ラベルの意味は以下の通りですidlabelmeaning0OK誤字なし1deletion1文字の抜け2insertion_a余分な1文字の挿入3insertion_b直前の文字列と一致する２文字以上の余分な文字の挿入4kanji-conversion_a同一の読みを持つ漢字の入れ替え（誤変換）5kanji-conversion_b近い読みを持つ漢字の入れ替え（誤変換）6substitution1文字の入れ替え7transposition隣接する２文字間の転置8othersその他の入力誤り誤り種類の詳細については学習データセットの元論文をご参照ください日本語 Wikipedia の編集履歴に基づく 入力誤りデータセットと訂正システムの改良その他、モデルの詳細については当社ブログ記事をご参照ください誤字脱字検出モデルをHugging Face Hubに公開しました (Recruit Data Blog)学習データ京都大学大学院情報学研究科知能情
 * 📥 36 [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1-llava-620k](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v1-llava-620k) - Heron BLIP Japanese StableLM Base 7B llava-620kModel DetailsHeron BLIP Japanese StableLM
 * 📥 36 [rinna/japanese-hubert-large](https://huggingface.co/rinna/japanese-hubert-large) - rinna/japanese-hubert-largeOverviewThis is a Japanese HuBERT Large model trained by rinna Co., Ltd.Model summaryThe model architecture is the same as the original HuBERT Large model, which contains 24 transformer layers with 16 attention heads. The model was trained using code from the official repository, and the detailed training configuration can be found in the same repository and the original paper.
 * 📥 36 [taishi-i/awesome-japanese-nlp-classification-model](https://huggingface.co/taishi-i/awesome-japanese-nlp-classification-model) - Model overviewThis model is the baseline model for awesome-japanese-nlp-classification-dataset. It was trained on this dataset, saved using the development data, and evaluated using the test data.
 * 📥 36 [ku-nlp/roberta-large-japanese-char-wwm](https://huggingface.co/ku-nlp/roberta-large-japanese-char-wwm) - ku-nlp/roberta-large-japanese-char-wwmModel descriptionThis is a Japanese RoBERTa large model pre-trained on Japanese Wikipedia and the Japanese portion of CC-100.This model is trained with character-level tokenization and whole word masking. How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained('ku-nlp/roberta-large-japanese-char-wwm')model = AutoModelForMaskedLM.from_pretrained('ku-nlp/r
 * 📥 36 [zh-plus/faster-whisper-large-v2-japanese-5k-steps](https://huggingface.co/zh-plus/faster-whisper-large-v2-japanese-5k-steps) - Converted from clu-ling/whisper-large-v2-japanese-5k-steps using CTranslate2.Usage:Install pip install faster-whisper (Check faster-whisper for detailed instructions.)from faster_whisper import WhisperModelmodel = WhisperModel('zh-plus/faster-whisper-large-v2-japanese-5k-steps', device="cuda", compute_type="float16")segments, info = model.transcribe("audio.mp3", beam_size=5)print("Detected language '%s' with probability %f" % (info.language, info.language_probability))for segment in segments:print("[%.2fs -
 * 📥 35 [if001/tiny_mixtral_ja](https://huggingface.co/if001/tiny_mixtral_ja) - 275.86Mのmixtralを日本語データセットでpretrainingしたものですsamplefrom transformers import AutoTokenizer, AutoModelForCausalLMmodel = AutoModelForCausalLM.from_pretrained("if001/tiny_mixtral_ja")tokenizer = AutoTokenizer.from_pretrained("if001/sentencepiece_ja", trust_remote_code=True)prompt = "それは九月初旬のある蒸し暑い晩のことであった。私は、Ｄ坂の"inputs = tokenizer(prompt, return_tensors="pt")generate_ids = model.generate(inputs.input_ids,max_length=30,top_k=30,top_p=0.95,temperature=0.6,repetition_penalty=1.2,do_sample=True,)tokenizer.decode(gen
 * 📥 34 [napopoa32/swallow-hermes-st-v1](https://huggingface.co/napopoa32/swallow-hermes-st-v1) - swallow-hermes-st-v1物語作成に強めなモデルが出来ないかと考えて作ったモデルです。 Chat VectorとEvoLLMの手法に影響を受けています。
 * 📥 34 [ku-nlp/bart-large-japanese](https://huggingface.co/ku-nlp/bart-large-japanese) - Model Card for Japanese BART largeModel descriptionThis is a Japanese BART large model pre-trained on Japanese Wikipedia. How to useYou can use this model as follows:from transformers import AutoTokenizer, MBartForConditionalGenerationtokenizer = AutoTokenizer.from_pretrained('ku-nlp/bart-large-japanese')model = MBartForConditionalGeneration.from_pretrained('ku-nlp/bart-large-japanese')sentence = '京都 大学 で 自然 言語 処理
 * 📥 33 [Hemlok/REV-Mix](https://huggingface.co/Hemlok/REV-Mix) - ◆REV-Mix"レボリューション"なモデルです。 DateMixやRDtMixを意識して作成しました。
 * 📥 32 [cinmodel/electra-small-japanese-generator](https://huggingface.co/cinmodel/electra-small-japanese-generator) - Japanese ELECTRA-smallWe provide a Japanese ELECTRA-Small model, as described in ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators. Our pretraining process employs subword units derived from the Japanese Wikipedia, using the Byte-Pair Encoding method and building on an initial tokenization with mecab-ipadic-NEologd.
 * 📥 32 [ken11/bert-japanese-ner](https://huggingface.co/ken11/bert-japanese-ner) - bert-japanese-nerこのモデルは日本語の固有表現抽出タスクを目的として、京都大学 黒橋・褚・村脇研究室が公開しているBERT日本語Pretrainedモデルをベースにストックマーク株式会社が公開しているner-wikipedia-datasetでファインチューニングしたものです。 How to useこのモデルはTokenizerに上述の京都大学BERT日本語PretrainedモデルのTokenizerを利用します。
 * 📥 32 [aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct](https://huggingface.co/aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct) - 更新情報日本語機能とinstructベクトルのバランス調整したver.2をアップロードしましたSwallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2モデル概要Swallow-MX-8x7b-NVE-v0.1に対し、Mixtral-8x7B-Instruct-v0.1とMixtral-8x7B-v0.1の差分をマージしたモデルです。 Swallow-MX-8x7b-NVE-v0.1 + Mixtral-8x7B-Instruct-v0.1 - Mixtral-8x7B-v0.1Swallow-MX-8x7b-NVE-v0.1は、コンテクスト長4096までの日本語継続学習モデルですが、英語モデルのInstructベクトルをマージすることで、流暢な日本語機能を維持してコンテクスト長を32Kまで拡大、Instruct機能を大幅アップしました。
 * 📥 31 [sonoisa/t5-base-japanese-adapt](https://huggingface.co/sonoisa/t5-base-japanese-adapt) - 日本語T5 Prefix Language ModelThis is a T5 (Text-to-Text Transfer Transformer) Adapted Language Model fine-tuned on Japanese corpus.
 * 📥 31 [elyza/ELYZA-japanese-CodeLlama-7b](https://huggingface.co/elyza/ELYZA-japanese-CodeLlama-7b) - ELYZA-japanese-CodeLlama-7bModel DescriptionELYZA-japanese-CodeLlama-7b は、 Code Llamaをベースとして日本語能力を拡張するために追加事前学習を行ったモデルです。詳細は Blog記事 を参照してください。Usageimport torchfrom transformers import AutoModelForCausalLM, AutoTokenizerB_INST, E_INST = "[INST]", "[/INST]"B_SYS, E_SYS = "&lt;&lt;SYS&gt;&gt;\n", "\n&lt;&lt;/SYS&gt;&gt;\n\n"DEFAULT_SYSTEM_PROMPT = "あなたは誠実で優秀な日本人のアシスタントです。"text = "エラトステネスの篩についてサンプルコードを示し、解説してください。"model_name = "elyza/ELYZA-japanese-CodeLlama-7b-instruct"tokenizer = AutoTokenizer.from_pretrained
 * 📥 31 [nlp-waseda/gpt2-xl-japanese](https://huggingface.co/nlp-waseda/gpt2-xl-japanese) - nlp-waseda/gpt2-xl-japaneseThis is Japanese GPT2 with approximately　1.5B parameters pretrained on Japanese Wikipedia and CC-100The model architecture of the model are based on Radford+ 2019.Intended uses &amp; limitationsYou can use the raw model for text generation or fine-tune it to a downstream task. Note that the texts should be segmented into words using Juman++ in advance.
 * 📥 30 [watashiha/Watashiha-Llama-2-13B-Ogiri-sft](https://huggingface.co/watashiha/Watashiha-Llama-2-13B-Ogiri-sft) - The English document is here. モデル概要Llama2-13bに日本語語彙を追加して継続事前学習を行った大喜利言語モデルです。
 * 📥 30 [OrionStarAI/Orion-14B-Base-Int4](https://huggingface.co/OrionStarAI/Orion-14B-Base-Int4) - Orion-14B🌐English | 🇨🇳中文 | 🇯🇵日本語 |🇰🇷한국어🤗 HuggingFace Mainpage | 🤖 ModelScope Mainpage🎬 HuggingFace Demo | 🎫 ModelScope Demo😺 GitHub📖 Tech ReportTable of Contents📖 Model Introduction🔗 Model Download🔖 Model Benchmark📊 Model Inference📜 Declarations &amp; License🥇 Company Introduction1. Model IntroductionOrion-14B series models are open-source multilingual large language models trained from scratch by OrionStarAI.  
 * 📥 29 [llm-book/bert-base-japanese-v3-crf-ner-wikipedia-dataset](https://huggingface.co/llm-book/bert-base-japanese-v3-crf-ner-wikipedia-dataset) - llm-book/bert-base-japanese-v3-crf-ner-wikipedia-dataset「大規模言語モデル入門」の第6章で紹介している固有表現認識のモデルです。cl-tohoku/bert-base-japanese-v3の出力層にCRF層を組み合わせたモデルをllm-book/ner-wikipedia-datasetでファインチューニングして構築されています。関連リンクGitHubリポジトリColabノートブックデータセット大規模言語モデル入門（Amazon.co.jp）大規模言語モデル入門（gihyo.jp）使い方from transformers import pipelinefrom pprint import pprintner_pipeline = pipeline(model="llm-book/bert-base-japanese-v3-crf-ner-wikipedia-dataset",aggregation_strategy="simple",)text = "大谷翔平は岩手県水沢市出身のプロ野球選手"# text中の固有表現を抽出pprint(ner_pipe
 * 📥 29 [OrionStarAI/Orion-14B-Chat-Plugin](https://huggingface.co/OrionStarAI/Orion-14B-Chat-Plugin) - Orion-14B🌐English | 🇨🇳中文 | 🇯🇵日本語 | 🇰🇷한국어🤗 HuggingFace Mainpage | 🤖 ModelScope Mainpage🎬 HuggingFace Demo | 🎫 ModelScope Demo😺 GitHub📖 Tech ReportTable of Contents📖 Model Introduction🔗 Model Download🔖 Model Benchmark📊 Model Inference📜 Declarations &amp; License🥇 Company Introduction1. Model IntroductionOrion-14B series models are open-source multilingual large language models trained from scratch by OrionStarAI.  
 * 📥 29 [ybelkada/japanese-roberta-question-answering](https://huggingface.co/ybelkada/japanese-roberta-question-answering) - RoBERTa base Japanese - JaQuADDescriptionA Japanese Question Answering model fine-tuned on JaQuAD.Please refer RoBERTa base Japanese for details about the pre-training model. The codes for the fine-tuning are available on this notebookUsagefrom transformers import AutoModelForQuestionAnswering, AutoTokenizerquestion = 'アレクサンダー・グラハム・ベルは、どこで生まれたの?'context = 'アレクサンダー・グラハム・ベルは、スコットランド生まれの科学者、発明家、工学者である。世界初の&gt;実用的電話の発明で知られている。'model = AutoModelForQuestionAnswering.from_pretrained('ybelkada/japanese-roberta-quest
 * 📥 29 [sonoisa/t5-qiita-title-generation](https://huggingface.co/sonoisa/t5-qiita-title-generation) - 記事本文からタイトルを生成するモデルSEE: https://qiita.com/sonoisa/items/30876467ad5a8a81821f
 * 📥 28 [llm-book/bert-base-japanese-v3-jcommonsenseqa](https://huggingface.co/llm-book/bert-base-japanese-v3-jcommonsenseqa) - bert-base-japanese-v3-jcommonsenseqa「大規模言語モデル入門」の第5章で紹介している(多肢選択式質問応答)のモデルです。 cl-tohoku/bert-base-japanese-v3をJGLUEのJCommonsenseQAデータセットでファインチューニングして構築されています。
 * 📥 28 [Formzu/bart-base-japanese](https://huggingface.co/Formzu/bart-base-japanese) - bart-base-japaneseThis model is converted from the original Japanese BART Pretrained model released by Kyoto University. Both the encoder and decoder outputs are identical to the original Fairseq model.
 * 📥 26 [yellowback/gpt-neo-japanese-1.3B](https://huggingface.co/yellowback/gpt-neo-japanese-1.3B) - GPT-Neo 1.3B pre-trained model for JapaneseModel DescriptionGPT2/GPT3 like model trained on Japanese.corpus. Training datacc100 jaoscar jawikipedia jaHow to usefrom transformers import pipeline&gt;&gt;&gt; generator = pipeline('text-generation', model='yellowback/gpt-neo-japanese-1.3B')&gt;&gt;&gt; generator("こんばんは、徳川家康です。", do_sample=True, max_length=50, num_return_sequences=3)[{'generated_text': 'こんばんは、徳川家康です。 世の中を見渡してみても、残念なことだけれども、まぎれもなく「世のなか...\n5月になりました、家康です。 ゴールデンウィークも終ってしまい、世間では'},{'generated_text':
 * 📥 26 [sambanovasystems/SambaLingo-Japanese-Base](https://huggingface.co/sambanovasystems/SambaLingo-Japanese-Base) - SambaLingo-Japanese-BaseSambaLingo-Japanese-Base is a pretrained Bi-lingual Japanese and English model that adapts Llama-2-7b to Japanese by training on 42 billion tokens from the Japanese split of the Cultura-X dataset. This model reports state of the art evaluation results in perplexity and FLORES-200 translation.
 * 📥 25 [Mizuiro-sakura/luke-japanese-base-marcja](https://huggingface.co/Mizuiro-sakura/luke-japanese-base-marcja) - このモデルはluke-japanese-baseをファインチューニングして、MARC-ja(positive or negativeの二値分類)に用いれるようにしたものです。このモデルはluke-japanese-baseをyahoo japan/JGLUEのMARC-ja( https://github.com/yahoojapan/JGLUE )を用いてファインチューニングしたものです。positive or negativeの二値分類タスクに用いることができます。This model is fine-tuned model for MARC-ja which is based on luke-japanese-baseThis model is fine-tuned by using yahoo japan JGLUE MARC-ja dataset. You could use this model for binary classification (positive or negative) tasks.モデルの精度 accuracy of modelprecision : 0.967,accura
 * 📥 25 [jovyan/Swallow-MS-7b-v0.1-ChatVector](https://huggingface.co/jovyan/Swallow-MS-7b-v0.1-ChatVector) - Swallow-MS-7b-v0.1-ChatVectorJapanese "instruction tuned" model made by the technique of Chat VectorThe weights of this model are obtained not by any instruction tuning but by the following arithmetic:Swallow-MS-7b-v0.1 + Mistral-7B-Instruct-v0.2 - Mistral-7B-v0.1Chat Vectorの手法を使って、学習済み重みの足し引きのみでSwallow-MS-7b-v0.1モデルにチャット形式の対話能力を与えたモデルです。詳細はこちらの日本語記事で解説しています。Instruction formatThe promot format should be the same as Mistral-7B-Instruct-v0.2.E.g.text = "&lt;s&gt;[INST] What is your favourite condiment?
 * 📥 25 [Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1-GGUF) - ELYZA-japanese-Llama-2-MoE-2x13B-v0.1-GGUF概要Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1の量子化済みGGUF版です。ライセンス等詳細は元モデルをご確認ください。現在はQ4_K_Mのみです。需要ありそうであれば他のものも用意します。DescriptionThis is the quantized GGUF version of Aratako/ELYZA-japanese-Llama-2-MoE-2x13B-v0.1. Please refer to the original model for license details and more information.
 * 📥 24 [AndrewMcDowell/wav2vec2-xls-r-300m-japanese](https://huggingface.co/AndrewMcDowell/wav2vec2-xls-r-300m-japanese) - This model is a fine-tuned version of facebook/wav2vec2-xls-r-300m on the MOZILLA-FOUNDATION/COMMON_VOICE_8_0 - JA dataset. Kanji are converted into Hiragana using the pykakasi library during training and evaluation.
 * 📥 24 [hakuhodo-tech/japanese-clip-vit-h-14-bert-wider](https://huggingface.co/hakuhodo-tech/japanese-clip-vit-h-14-bert-wider) - Japanese CLIP ViT-H/14 (Wider)Table of ContentsOverviewUsageModel DetailsEvaluationLimitations and BiasesCitationSee AlsoContact InformationOverviewDeveloped by: HAKUHODO Technologies Inc.Model type: Contrastive Language-Image Pre-trained ModelLanguage(s): JapaneseLICENSE: CC BY-NC-SA 4.0Presented here is a Japanese CLIP (Contrastive Language-Image Pre-training) model,mapping Japanese texts and images to a unified embedding space. Capable of multimodal tasks including zero-shot image classification,text-to-i
 * 📥 24 [abeja/Mixtral-8x7B-v0.1-japanese](https://huggingface.co/abeja/Mixtral-8x7B-v0.1-japanese) - Mixtral-8x7B-v0.1-japaneseMixtral-8x7B-v0.1-japaneseはMixtral-8x7B-v0.1をベースに日本語の語彙拡張継続事前学習を実施したモデルです。詳細はABEJAのテックブログを参照してください。学習を実施したMetagton-LMのレポジトリはこちらです。使い方import torchfrom transformers import AutoModelForCausalLM, AutoTokenizermodel_name = "abeja/Mixtral-8x7B-v0.1-japanese"tokenizer = AutoTokenizer.from_pretrained(model_name)model = AutoModelForCausalLM.from_pretrained(model_name,torch_dtype=torch.float16,use_cache=True,device_map="auto",)model.eval()input_text = """# system誠実で紳士的で優秀なAIアシスタントとして、簡潔でわかりや
 * 📥 23 [Bagus/wav2vec2-xlsr-japanese-speech-emotion-recognition](https://huggingface.co/Bagus/wav2vec2-xlsr-japanese-speech-emotion-recognition) - This is for (private) DEMO only.
 * 📥 23 [bardsai/finance-sentiment-ja-base](https://huggingface.co/bardsai/finance-sentiment-ja-base) - Finance Sentiment JA (base)Finance Sentiment JA (base) is a model based on bert-base-japanese for analyzing sentiment of Japanese financial news. It was trained on the translated version of Financial PhraseBank by Malo et al.
 * 📥 23 [akiFQC/bert-base-japanese-v3_nli-jsnli](https://huggingface.co/akiFQC/bert-base-japanese-v3_nli-jsnli) - Cross-Encoder for Natural Language Inference(NLI) for JapaneseConsidering the results of the JNLI evaluation result, we recommend using akiFQC/bert-base-japanese-v3_nli-jsnli-jnli-jsick for natural language inference in Japanese. This model was trained using SentenceTransformers Cross-Encoder class.
 * 📥 22 [sonoisa/t5-base-japanese-question-generation](https://huggingface.co/sonoisa/t5-base-japanese-question-generation) - 回答と回答が出てくるパラグラフを与えると質問文を生成するモデルSEE: https://github.com/sonoisa/deep-question-generation本モデルの作成ステップ概要SQuAD 1.1を日本語に機械翻訳し、不正なデータをクレンジング（有効なデータは約半分）。 回答が含まれるコンテキスト、質問文、解答の3つ組ができる。
 * 📥 22 [retrieva-jp/t5-large-medium](https://huggingface.co/retrieva-jp/t5-large-medium) - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus. Model detailsT5 is a Transformer-based Encoder-Decoder model, now in v1.1, with the following improvements over the original T5.GEGLU activation in feed-forward hidden layer, rather than ReLU - see
 * 📥 22 [kubota/luke-large-defamation-detection-japanese](https://huggingface.co/kubota/luke-large-defamation-detection-japanese) - luke-large-defamation-detection-japanese日本語誹謗中傷検出器This model is a fine-tuned version of studio-ousia/luke-japanese-large for the Japanese language finetuned for automatic defamation detection. The original foundation model was finetuned on a balanced dataset created by unifying two datasets:DefamationJapaneseYouTube : TBALabels:0 -&gt; "中傷性のない発言"1 -&gt; "脅迫的な発言"2 -&gt; "侮蔑的な発言"3"-&gt; "名誉を低下させる発言"Example Pipeline# !
 * 📥 21 [haqishen/h2o-Llama-3-8B-Japanese-Instruct](https://huggingface.co/haqishen/h2o-Llama-3-8B-Japanese-Instruct) - IntroductionWho am I: Qishen Ha [Kaggle] [X] [LinkedIn]This is a meta-llama/Meta-Llama-3-8B-Instruct model that finetuned on Japanese conversation dataset.
 * 📥 21 [kit-nlp/bert-base-japanese-basic-char-v2-cyberbullying](https://huggingface.co/kit-nlp/bert-base-japanese-basic-char-v2-cyberbullying) - electra-base-cyberbullyingThis is a BERT Base model for the Japanese language finetuned for automatic cyberbullying detection. The model was based on Hiroshi Matsuda's BERT base Japanese, and later finetuned on a balanced dataset created by unifying two datasets, namely "Harmful BBS Japanese comments dataset" and "Twitter Japanese cyberbullying dataset".
 * 📥 21 [Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUF) - ELYZA-japanese-Llama-2-MoE-2x7B-v0.1-GGUF概要Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1の量子化済みGGUF版です。ライセンス等詳細は元モデルをご確認ください。現在はQ4_K_Mのみです。需要ありそうであれば他のものも用意します。DescriptionThis is the quantized GGUF version of Aratako/ELYZA-japanese-Llama-2-MoE-2x7B-v0.1. Please refer to the original model for license details and more information.
 * 📥 21 [ku-nlp/roberta-base-japanese-char-wwm](https://huggingface.co/ku-nlp/roberta-base-japanese-char-wwm) - ku-nlp/roberta-base-japanese-char-wwmModel descriptionThis is a Japanese RoBERTa base model pre-trained on Japanese Wikipedia and the Japanese portion of CC-100.This model is trained with character-level tokenization and whole word masking. How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained('ku-nlp/roberta-base-japanese-char-wwm')model = AutoModelForMaskedLM.from_pretrained('ku-nlp/robe
 * 📥 20 [kz/mt5base-finetuned-patentsum-japanese-small](https://huggingface.co/kz/mt5base-finetuned-patentsum-japanese-small) - Google's mt5-base fine-tuned in Japanese to summarize patent claims in a limited Pharmaceutical domain. 日本語特許請求項要約（医薬特定ドメイン限定）"""【請求項１】ヒトＣＤ３８（配列番号１）及びカニクイザルＣＤ３８（配列番号２）に特異的に結合する単離された抗体であって、ａ）以下を含む重鎖可変領域：ｉ）配列番号３を含む第１のＣＤＲ；ｉｉ）配列番号４を含む第２のＣＤＲ；ｉｉｉ）配列番号５を含む第３のＣＤＲ；及びｂ）以下を含む軽鎖可変領域：ｉ）配列番号６を含む第１のＣＤＲ；ｉｉ）配列番号７を含む第２のＣＤＲ；ｉｉｉ）配列番号８を含む第３のＣＤＲ；を含む、抗体。
 * 📥 20 [snu-nia-12/wav2vec2-xls-r-300m_nia12_phone-hiragana_japanese](https://huggingface.co/snu-nia-12/wav2vec2-xls-r-300m_nia12_phone-hiragana_japanese) - Wav2Vec2-XLS-R-300M-Japanese-HiraganaFine-tuned facebook/wav2vec2-xls-r-300m on Japanese Hiragana characters using JSUT, JVS, Common Voice, and in-house dataset. The sentence outputs do not contain word boundaries.
 * 📥 20 [akiFQC/japanese-dialogpt-small-aozora](https://huggingface.co/akiFQC/japanese-dialogpt-small-aozora) - Japanese DialoGPT trained with Aozora(ja) 青空文庫のセリフで学習した日本語のDialoGPT Smallです(en) Japanese DialoGPT Small trained on Aozora Bunko. DemoDemo in this page is not working so well.
 * 📥 19 [Mizuiro-sakura/bert-large-japanese-v2-finetuned-ner](https://huggingface.co/Mizuiro-sakura/bert-large-japanese-v2-finetuned-ner) - このモデルはcl-tohoku/bert-large-japanese-v2をファインチューニングして、固有表現抽出（NER）に用いれるようにしたものです。このモデルはcl-tohoku/bert-large-japanese-v2をWikipediaを用いた日本語の固有表現抽出データセット(ストックマーク社、https://github.com/stockmarkteam/ner-wikipedia-dataset ) を用いてファインチューニングしたものです。固有表現抽出（NER）タスクに用いることができます。This model is fine-tuned model for Named-Entity-Recognition(NER) which is based on cl-tohoku/bert-large-japanese-v2This model is fine-tuned by using Wikipedia dataset.
 * 📥 19 [turing-motors/heron-chat-git-ja-stablelm-base-7b-v0](https://huggingface.co/turing-motors/heron-chat-git-ja-stablelm-base-7b-v0) - Heron GIT Japanese StableLM Base 7BModel DetailsHeron GIT Japanese StableLM
 * 📥 19 [Mizuiro-sakura/deberta-v2-base-japanese-finetuned-ner](https://huggingface.co/Mizuiro-sakura/deberta-v2-base-japanese-finetuned-ner) - このモデルはdeberta-v2-base-japaneseをファインチューニングして固有表現抽出（NER）に用いれるようにしたものです。このモデルはdeberta-v2-base-japaneseを Wikipediaを用いた日本語の固有表現抽出データセット(ストックマーク社、https://github.com/stockmarkteam/ner-wikipedia-dataset )を用いてファインチューニングしたものです。This model is fine-tuned model for Named Entity Recognition (NER) which is based on deberta-v2-base-japaneseThis model is fine-tuned by using Wikipedia dataset. You could use this model for NER tasks.
 * 📥 19 [stockmark/bart-base-japanese-news](https://huggingface.co/stockmark/bart-base-japanese-news) - bart-base-japanese-news(base-sized model)This repository provides a Japanese BART model. The model was trained by Stockmark Inc.
 * 📥 18 [KoichiYasuoka/bert-large-japanese-luw-upos](https://huggingface.co/KoichiYasuoka/bert-large-japanese-luw-upos) - bert-large-japanese-luw-uposModel DescriptionThis is a BERT model pre-trained on Japanese Wikipedia texts for POS-tagging and dependency-parsing, derived from bert-large-japanese-char-extended.
 * 📥 18 [if001/llama2_ja_ss](https://huggingface.co/if001/llama2_ja_ss) - 日本語でtrainingしたllama2model size:  130.78Mtrainingは以下のscript参照https://github.com/Lightning-AI/lit-gpt/tree/mainusefrom transformers import AutoTokenizer, AutoModelForCausalLMtokenizer = AutoTokenizer.from_pretrained("if001/sentencepiece_ja", trust_remote_code=True)model = AutoModelForCausalLM.from_pretrained("if001/llama2_ja_ss")import torchfrom transformers import GenerationConfigprompt="あのイーハトーヴォのすきとおった風、"inputs = tokenizer(prompt, return_tensors="pt")input_ids = inputs["input_ids"]generation_config = Gener
 * 📥 18 [izumi-lab/electra-base-japanese-discriminator](https://huggingface.co/izumi-lab/electra-base-japanese-discriminator) - ELECTRA base Japanese discriminatorThis is a ELECTRA model pretrained on texts in the Japanese language. The codes for the pretraining are available at retarfi/language-pretraining.
 * 📥 17 [KoichiYasuoka/deberta-large-japanese-unidic-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-unidic-luw-upos) - deberta-large-japanese-unidic-luw-uposModel DescriptionThis is a DeBERTa(V2) model pre-trained on 青空文庫 texts for POS-tagging and dependency-parsing, derived from deberta-large-japanese-unidic.
 * 📥 17 [NilanE/tinyllama-en_ja-translation-v2](https://huggingface.co/NilanE/tinyllama-en_ja-translation-v2) - In-progess long-context Japanese-English translation model based on tinyllama. Input should be 500-1000 tokens long.
 * 📥 17 [taishi-i/nagisa_bert](https://huggingface.co/taishi-i/nagisa_bert) - nagisa_bertA BERT model for nagisa. The model is available in Transformers 🤗.A tokenizer for nagisa_bert is available here.
 * 📥 17 [hiroshi-matsuda-rit/ja_gsd_bert_wwm_unidic_lite](https://huggingface.co/hiroshi-matsuda-rit/ja_gsd_bert_wwm_unidic_lite) - Japanese transformer pipeline (bert-base). Components: transformer, parser, ner.FeatureDescriptionNameja_gsd_bert_wwm_unidic_liteVersion3.1.1spaCy&gt;=3.1.0,&lt;3.2.0Default Pipelinetransformer, parser, nerComponentstransformer, parser, nerVectors0 keys, 0 unique vectors (0 dimensions)SourcesUD_Japanese-GSDUD_Japanese-GSD r2.8+NESudachiDict_corecl-tohoku/bert-base-japanese-whole-word-maskingunidic_liteLicenseCC BY-SA 4.0AuthorMegagon Labs Tokyo.
 * 📥 16 [colorfulscoop/bert-base-ja](https://huggingface.co/colorfulscoop/bert-base-ja) - BERT base Japanese modelThis repository contains a BERT base model trained on Japanese Wikipedia dataset. Training dataJapanese Wikipedia dataset as of June 20, 2021 which is released under Creative Commons Attribution-ShareAlike 3.0 is used for training.
 * 📥 16 [vumichien/wav2vec2-large-xlsr-japanese](https://huggingface.co/vumichien/wav2vec2-large-xlsr-japanese) - Wav2Vec2-Large-XLSR-53-JapaneseFine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the Common Voice and Japanese speech corpus of Saruwatari-lab, University of Tokyo JSUT.When using this model, make sure that your speech input is sampled at 16kHz. UsageThe model can be used directly (without a language model) as follows:!pip install mecab-python3!pip install unidic-lite!python -m unidic downloadimport torchimport torchaudioimport librosafrom datasets import load_datasetimport MeCabfrom transformers
 * 📥 16 [turing-motors/heron-chat-blip-ja-stablelm-base-7b-v0](https://huggingface.co/turing-motors/heron-chat-blip-ja-stablelm-base-7b-v0) - Heron BLIP Japanese StableLM Base 7BDEMOYou can play the demo of this model here.
 * 📥 16 [izumi-lab/electra-small-japanese-discriminator](https://huggingface.co/izumi-lab/electra-small-japanese-discriminator) - ELECTRA small Japanese discriminatorThis is a ELECTRA model pretrained on texts in the Japanese language. The codes for the pretraining are available at retarfi/language-pretraining.
 * 📥 16 [hiroshi-matsuda-rit/bert-base-japanese-basic-char-v2](https://huggingface.co/hiroshi-matsuda-rit/bert-base-japanese-basic-char-v2) - BERT base Japanese (character-level tokenization with whole word masking, jawiki-20200831)This pretrained model is almost the same as cl-tohoku/bert-base-japanese-char-v2 but do not need fugashi or unidic_lite. The only difference is in word_tokenzer_type property (specify basic instead of mecab) in tokenizer_config.json.
 * 📥 15 [Jumtra/mpt-7b-base](https://huggingface.co/Jumtra/mpt-7b-base) - MPT-7B-baseこのモデルは、MosaicMLのllm-foundryリポジトリを使用してmosaicml/mpt-7bをファインチューニングしたモデルです。 Model DateJune 28, 2023Model LicenseApache-2.0評価Jumtra/test_data_100QAを用いてモデルの正答率を評価したmodel name正答率mosaicml/mpt-7b16/100mosaicml/mpt-7b-instruct28/100Jumtra/mpt-7b-base47/100Jumtra/mpt-7b-inst46/100使用方法注意：このモデルでは、from_pretrainedメソッドにtrust_remote_code=Trueを渡す必要があります。
 * 📥 15 [Jumtra/mpt-7b-inst](https://huggingface.co/Jumtra/mpt-7b-inst) - MPT-7B-instこのモデルは、MosaicMLのllm-foundryリポジトリを使用してmosaicml/mpt-7b-instructをファインチューニングしたモデルです。 Model DateJune 28, 2023Model LicenseCC-BY-SA-3.0評価Jumtra/test_data_100QAを用いてモデルの正答率を評価したmodel name正答率mosaicml/mpt-7b16/100mosaicml/mpt-7b-instruct28/100Jumtra/mpt-7b-base47/100Jumtra/mpt-7b-inst46/100使用方法注意：このモデルでは、from_pretrainedメソッドにtrust_remote_code=Trueを渡す必要があります。
 * 📥 15 [Mizuiro-sakura/deberta-v2-japanese-tiny-finetuned-commonsenseqa](https://huggingface.co/Mizuiro-sakura/deberta-v2-japanese-tiny-finetuned-commonsenseqa) - このモデルはdeberta-v2-tiny-japaneseをファインチューニングしてCommonsenseQA(選択式の質問)に用いれるようにしたものです。 このモデルはdeberta-v2-tiny-japaneseをyahoo japan/JGLUEのJCommonsenseQA( https://github.com/yahoojapan/JGLUE ) を用いてファインチューニングしたものです。
 * 📥 15 [ptaszynski/yacis-electra-small-japanese-cyberbullying](https://huggingface.co/ptaszynski/yacis-electra-small-japanese-cyberbullying) - yacis-electra-small-cyberbullyingThis is an ELECTRA Small model for the Japanese language finetuned for automatic cyberbullying detection. The original foundation model was originally pretrained on 5.6 billion words YACIS blog corpus, and later finetuned on a balanced dataset created by unifying two datasets, namely "Harmful BBS Japanese comments dataset" and "Twitter Japanese cyberbullying dataset".
 * 📥 14 [nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanpp](https://huggingface.co/nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanpp) - nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanppModel descriptionThis is a Japanese RoBERTa large model pretrained on Japanese Wikipedia and the Japanese portion of CC-100 with the maximum sequence length of 512.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained("nlp-waseda/roberta-large-japanese-seq512-with-auto-jumanpp")model = AutoModelForMaskedLM.from_pretrained("nlp-wase
 * 📥 14 [retrieva-jp/t5-base-short](https://huggingface.co/retrieva-jp/t5-base-short) - Model card for model IDThis is a T5 v1.1 model, pre-trained on a Japanese corpus. Model detailsT5 is a Transformer-based Encoder-Decoder model, now in v1.1, with the following improvements over the original T5.GEGLU activation in feed-forward hidden layer, rather than ReLU - see
 * 📥 14 [line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-128g-actorder_False](https://huggingface.co/line-corporation/japanese-large-lm-1.7b-instruction-sft-4bit-128g-actorder_False) - japanese-large-lm-1.7b-instruction-sft-4bit-128g-actorder_FalseThis repository provides a 1.7B parameters Japanese language quantized model, fine-tuned and trained by LINE Corporation. For Japanese詳細な説明や実験に関しては「【インターンレポート】量子化による大規模言語モデル軽量化の効果測定」をご覧ください。How to useimport torchfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipelinetokenizer = AutoTokenizer.from_pretrained("line-corporation/japanese-large-lm-1.7b-instruction-sft", use_fast=False)model = AutoModelForCausalLM.from_pretrained("line-c
 * 📥 14 [mpasila/calm2-7b-safetensors](https://huggingface.co/mpasila/calm2-7b-safetensors) - This is a conversion of cyberagent/calm2-7b  to safetensors so you don't have to worry about getting hacked by downloading dirty pickled files. Original model card:CyberAgentLM2-7B (CALM2-7B)Model DescriptionCyberAgentLM2 is a decoder-only language model pre-trained on the 1.3T tokens of publicly available Japanese and English datasets.
 * 📥 14 [leia-llm/Leia-Swallow-7b](https://huggingface.co/leia-llm/Leia-Swallow-7b) - Leia-Swallow-7BLEIA is a training technique for autoregressive LLMs that effectively improves their performance in languages other than English by enhancing cross-lingual knowledge transfer from English to a target language. This model is constructed by applying LEIA to Swallow, a Japanese-English bilingual LLM based on LLaMA 2.The model achieves enhanced performance on six Japanese question-answering benchmarks, as reported below.
 * 📥 14 [minkhantycc/translation-en-ja](https://huggingface.co/minkhantycc/translation-en-ja) - This model is the fine-tuned version of Helsinki-NLP/opus-mt-ja-en on bsd_ja_en dataset. This will translate Japanese sentences to English sentences.
 * 📥 14 [KoichiYasuoka/deberta-base-japanese-wikipedia-luw-upos](https://huggingface.co/KoichiYasuoka/deberta-base-japanese-wikipedia-luw-upos) - deberta-base-japanese-wikipedia-luw-uposModel DescriptionThis is a DeBERTa(V2) model pre-trained on Japanese Wikipedia and 青空文庫 texts for POS-tagging and dependency-parsing, derived from deberta-base-japanese-wikipedia.
 * 📥 14 [NovelAI/genji-jp](https://huggingface.co/NovelAI/genji-jp) - Genji-JP 6BPlease check our blog post for more details, samples, evaluations and more:BlogpostModel DescriptionGenji-JP 6B is a model finetuned on our Japanese storytelling dataset based on EleutherAI's GPT-J 6B model. This particular model is trained on Japanese web novels.
 * 📥 13 [Ivydata/whisper-small-japanese](https://huggingface.co/Ivydata/whisper-small-japanese) - Fine-tuned Japanese Whisper model for speech recognition using whisper-smallFine-tuned openai/whisper-small on Japanese using Common Voice, JVS and JSUT.When using this model, make sure that your speech input is sampled at 16kHz. UsageThe model can be used directly as follows.from transformers import WhisperForConditionalGeneration, WhisperProcessorfrom datasets import load_datasetimport librosaimport torchLANG_ID
 * 📥 13 [hitachi-nlp/bert-base-japanese_nothing-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_nothing-unigram) - Japanese BERT-base (Nothing + Unigram)How to load the tokenizerPlease download the dictionary file for Nothing + Unigram from our GitHub repository. Then you can load the tokenizer by specifying the path of the dictionary file to dict_path.from typing import Optionalfrom tokenizers import Tokenizer, NormalizedString, PreTokenizedStringfrom tokenizers.processors import BertProcessingfrom tokenizers.pre_tokenizers import PreTokenizerfrom transformers import PreTrainedTokenizerFast# load a tokenizerdict_path =
 * 📥 13 [liwii/line-distilbert-base-japanese-fork](https://huggingface.co/liwii/line-distilbert-base-japanese-fork) - LINE DistilBERT Japanese (forked by liwii)This is a forked version of DistilBERT model pre-trained on 131 GB of Japanese web text. The teacher model is BERT-base that built in-house at LINE.The model was trained by LINE Corporation.
 * 📥 13 [hakuhodo-tech/japanese-clip-vit-h-14-bert-base](https://huggingface.co/hakuhodo-tech/japanese-clip-vit-h-14-bert-base) - Japanese CLIP ViT-H/14 (Base)Table of ContentsOverviewUsageModel DetailsEvaluationLimitations and BiasesCitationSee AlsoContact InformationOverviewDeveloped by: HAKUHODO Technologies Inc.Model type: Contrastive Language-Image Pre-trained ModelLanguage(s): JapaneseLICENSE: CC BY-NC-SA 4.0Presented here is a Japanese CLIP (Contrastive Language-Image Pre-training) model,mapping Japanese texts and images to a unified embedding space. Capable of multimodal tasks including zero-shot image classification,text-to-im
 * 📥 13 [aken12/splade-japanese-efficient](https://huggingface.co/aken12/splade-japanese-efficient) - output筑波 2.0035860538482666つくば 1.6586617231369019研究 1.6227693557739258大学 1.3798155784606934実験 0.5522942543029785学生 0.42351895570755005分析 0.37844282388687134国立 0.3685397505760193キャンパス 0.36495038866996765茨城 0.3056415021419525科学 0.2876652181148529関東 0.24301066994667053地域 0.21340851485729218実施 0.1976248174905777先端 0.192025288939476サイト 0.11629197001457214調査 0.09159307181835175プロジェクト 0.08552580326795578議論 0.07484486699104309検討 0.007034890353679657
 * 📥 13 [tohoku-nlp/stable-diffusion-xl-jp-refiner-1.0](https://huggingface.co/tohoku-nlp/stable-diffusion-xl-jp-refiner-1.0) - (English part follows Japanese one. )SD-XL 1.0-jp-refiner Model Card総計5.8Bのパラメータを持つ画像生成モデル，SDXLを日本語入力に対応させたモデルです．
 * 📥 13 [hitachi-nlp/bert-base-japanese_jumanpp-unigram](https://huggingface.co/hitachi-nlp/bert-base-japanese_jumanpp-unigram) - Japanese BERT-base (Juman++ + Unigram)How to load the tokenizerPlease download the dictionary file for Juman++ + Unigram from our GitHub repository. Then you can load the tokenizer by specifying the path of the dictionary file to dict_path.from typing import Optionalfrom tokenizers import Tokenizer, NormalizedString, PreTokenizedStringfrom tokenizers.processors import BertProcessingfrom tokenizers.pre_tokenizers import PreTokenizerfrom transformers import PreTrainedTokenizerFastfrom pyknp import Jumanimport
 * 📥 13 [Ivydata/wav2vec2-large-xlsr-53-japanese](https://huggingface.co/Ivydata/wav2vec2-large-xlsr-53-japanese) - Fine-tuned Japanese Wav2Vec2 model for speech recognition using XLSR-53 largeFine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using Common Voice, JVS and JSUT.When using this model, make sure that your speech input is sampled at 16kHz. UsageThe model can be used directly (without a language model) as follows.import torchimport librosafrom datasets import load_datasetfrom transformers import Wav2Vec2ForCTC, Wav2Vec2ProcessorLANG_ID =
 * 📥 13 [qqpann/w2v_hf_jsut_xlsr53](https://huggingface.co/qqpann/w2v_hf_jsut_xlsr53) - Wav2Vec2-Large-XLSR-53-JapaneseFine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the Common Voice, and JSUT dataset{s}.When using this model, make sure that your speech input is sampled at 16kHz.
 * 📥 13 [Momerio/meigen_generate_Japanese](https://huggingface.co/Momerio/meigen_generate_Japanese) - 名言推論モデル
 * 📥 12 [kanhatakeyama/Tanuki-ZeRo](https://huggingface.co/kanhatakeyama/Tanuki-ZeRo) - Tanuki-ZeroBase model: llm-jp/llm-jp-13b-v1.0Instruction data: Randomly sampled, 15k Jaster dataset (train)Code is here.
 * 📥 12 [turing-motors/heron-chat-git-ELYZA-fast-7b-v0](https://huggingface.co/turing-motors/heron-chat-git-ELYZA-fast-7b-v0) - Heron GIT Japanese ELYZA Llama 2 Fast 7BModel DetailsHeron GIT Japanese ELYZA Llama 2 Fast 7B is a vision-language model that can converse about input images.
 * 📥 12 [Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF](https://huggingface.co/Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF) - ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1-GGUF概要Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1の量子化済みGGUF版です。ライセンス等詳細は元モデルをご確認ください。現在はQ4_K_Mのみです。需要ありそうであれば他のものも用意します。DescriptionThis is the quantized GGUF version of Aratako/ELYZA-japanese-Llama-2-fast-MoE-2x7B-v0.1. Please refer to the original model for license details and more information.
 * 📥 12 [dddump/Japanese-TextGen-Kage-v0.1-2x7B-gguf](https://huggingface.co/dddump/Japanese-TextGen-Kage-v0.1-2x7B-gguf) - Japanese-TextGen-Kage-v0.1-2x7BThis is a merge model using Mergekit-Evolve.We merged our model and Ninja-v1 with Mergekit-Evolve and then franken MoE.This model has been made more powerful by merging Ninja-v1! Prompt formatWe recommend Vicuna format.SYSTEM: &lt;ANY SYSTEM CONTEXT&gt;USER:ASSISTANT:ExampleUSER: 「甘い」、「パソコン」、「女性」を使って文章を書いてくださいASSISTANT: ある喫茶店で、隣のテーブルから漂う美味しそうな甘い香りにつられ、ふと振り向けば、可愛らしい女性がパソコンに向かって真剣な表情で仕事をしていました。
 * 📥 12 [schroneko/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf](https://huggingface.co/schroneko/ELYZA-japanese-Llama-2-13b-fast-instruct-gguf) - ELYZA-japanese-Llama-2-13b-fast-instruct-ggufELYZA-japanese-Llama-2-13b-fast-instructの GGUF 変換モデルです。Usagegit clone https://github.com/ggerganov/llama.cpp.gitcd llama.cppmake
 * 📥 12 [oshizo/japanese-e5-mistral-7b_slerp](https://huggingface.co/oshizo/japanese-e5-mistral-7b_slerp) - This model was created by merging intfloat/e5-mistral-7b-instruct and stabilityai/japanese-stablelm-base-gamma-7b. See intfloat/e5-mistral-7b-instruct page or evaluation notebook of oshizo/JapaneseEmbeddingEval for model usage.
 * 📥 12 [dahara1/ELYZA-japanese-Llama-2-7b-instruct-AWQ](https://huggingface.co/dahara1/ELYZA-japanese-Llama-2-7b-instruct-AWQ) - Model Card for Model IDOriginal model elyza/ELYZA-japanese-Llama-2-7b-instruct which is based on Meta's "Llama 2" and has undergone additional pre-training in Japanese instruction. This model is a AWQ quantized(miniaturized to 3.89GB) version of the original model(13.48GB).Model DetailsCurrently, this model is confirmed to work with Colab A100 or RTX 3000 Series on local PC.This is because autoAWQ uses NVIDIA's PTX assembly instructions, some of which are only supported on sm80 and higher architectu.
 * 📥 12 [ku-nlp/deberta-v2-base-japanese-with-auto-jumanpp](https://huggingface.co/ku-nlp/deberta-v2-base-japanese-with-auto-jumanpp) - Model Card for Japanese DeBERTa V2 baseModel descriptionThis is a Japanese DeBERTa V2 base model pre-trained on Japanese Wikipedia, the Japanese portion of CC-100, and the Japanese portion of OSCAR.How to useYou can use this model for masked language modeling as follows:from transformers import AutoTokenizer, AutoModelForMaskedLMtokenizer = AutoTokenizer.from_pretrained('ku-nlp/deberta-v2-base-japanese-with-auto-jumanpp', trust_remote_code=True)model = AutoModelForMaskedLM.from_pretrained('ku-nlp/deberta-v2
 * 📥 12 [arc-r/faster-whisper-large-v2-mix-jp](https://huggingface.co/arc-r/faster-whisper-large-v2-mix-jp) - whisper-large-v2-mix-jp model for CTranslate2This repository contains the conversion of vumichien/whisper-large-v2-mix-jp to the CTranslate2 model format. This model can be used in CTranslate2 or projects based on CTranslate2 such as faster-whisper.
 * 📥 12 [KoichiYasuoka/deberta-large-japanese-aozora](https://huggingface.co/KoichiYasuoka/deberta-large-japanese-aozora) - deberta-large-japanese-aozoraModel DescriptionThis is a DeBERTa(V2) model pre-trained on 青空文庫 texts. NVIDIA A100-SXM4-40GB took 127 hours 8 minutes for training.
 * 📥 12 [ptaszynski/yacis-electra-small-japanese](https://huggingface.co/ptaszynski/yacis-electra-small-japanese) - yacis-electra-smallThis is ELECTRA Small model for Japanese pretrained on 354 million sentences / 5.6 billion words of YACIS blog corpus. The corpus was tokenized for pretraining with MeCab.
 * 📥 11 [tohoku-nlp/bert-large-japanese-char](https://huggingface.co/tohoku-nlp/bert-large-japanese-char) - BERT large Japanese (character-level tokenization with whole word masking, jawiki-20200831)This is a BERT model pretrained on texts in the Japanese language. This version of the model processes input texts with word-level tokenization based on the Unidic 2.1.2 dictionary (available in unidic-lite package), followed by character-level tokenization.
 * 📥 11 [astremo/friendly_JA](https://huggingface.co/astremo/friendly_JA) - friendly_JA-Model　(T5 fine-tuned model)MT model trained using the friendly_JA Corpus attempting to make Japanese easier/more accessible to occidental people by using the Latin/English derived katakana lexicon instead of the standard Sino-Japanese lexiconExamplesinputoutput最適化を応用した機械翻訳モデルは高精度だオプティマイゼーションを応用したマシントランスレーションモデルは高いアキュラシーだ彼は架空の世界に住んでいる彼はイマジナリー世界に住んでいる新型コロナウイルスに感染してしまったコロナウイルスにかかってしまった深層学習は難しいディープラーニングはむずかしい新たな概念を紹介する新しいコンセプトを紹介する津波の警報が流れたツナミのアラートが流れた南海トラフの災害は震源地による南海トラフのディザスターはエピセンターによる息子は際どい内容の本を
 * 📥 11 [kit-nlp/electra-small-japanese-discriminator-cyberbullying](https://huggingface.co/kit-nlp/electra-small-japanese-discriminator-cyberbullying) - electra-base-cyberbullyingThis is an ELECTRA Small model for the Japanese language finetuned for automatic cyberbullying detection. The model was based on Izumi Lab ELECTRA small Japanese discriminator, and later finetuned on a balanced dataset created by unifying two datasets, namely "Harmful BBS Japanese comments dataset" and "Twitter Japanese cyberbullying dataset".
 * 📥 11 [alabnii/jmedroberta-base-sentencepiece](https://huggingface.co/alabnii/jmedroberta-base-sentencepiece) - alabnii/jmedroberta-base-sentencepieceModel descriptionThis is a Japanese RoBERTa base model pre-trained on academic articles in medical sciences collected by Japan Science and Technology Agency (JST).This model is released under the Creative Commons 4.0 International License (CC BY-NC-SA 4.0).ReferenceJa:@InProceedings{sugimoto_nlp2023_jmedroberta,author =    "杉本海人 and 壹岐太一 and 知田悠生 and 金沢輝一 and 相澤彰子",title =     "J{M}ed{R}o{BERT}a: 日本語の医学論文にもとづいた事前学習済み言語モデルの構築と評価",booktitle = "言語処理学会第29回年次大会",year =      
 * 📥 11 [Mizuiro-sakura/deberta-v2-japanese-base-finetuned-commonsenseqa](https://huggingface.co/Mizuiro-sakura/deberta-v2-japanese-base-finetuned-commonsenseqa) - このモデルはdeberta-v2-base-japaneseをファインチューニングしてCommonsenseQA(選択式の質問)に用いれるようにしたものです。このモデルはdeberta-v2-base-japaneseをyahoo japan/JGLUEのJCommonsenseQA( https://github.com/yahoojapan/JGLUE ) を用いてファインチューニングしたものです。This model is fine-tuned model for CommonsenseQA which is based on deberta-v2-base-japaneseThis model is fine-tuned by using JGLUE/JCommonsenseQA dataset. You could use this model for CommonsenseQA tasks.
 * 📥 11 [TheBloke/japanese-stablelm-instruct-beta-70B-GPTQ](https://huggingface.co/TheBloke/japanese-stablelm-instruct-beta-70B-GPTQ) - Chat &amp; support: TheBloke's Discord serverWant to contribute? TheBloke's Patreon pageTheBloke's LLM work is generously supported by a grant from andreessen horowitz (a16z)Japanese StableLM
 * 📥 11 [oshizo/japanese-e5-mistral-1.9b](https://huggingface.co/oshizo/japanese-e5-mistral-1.9b) - Model trained on 800,000 Japanese sentences after reducing oshizo/japanese-e5-mistral-7b_slerp to 8 layers. See this article for details(Japanese)https://note.com/oshizo/n/n9140df790315See intfloat/e5-mistral-7b-instruct page for model usage.
 * 📥 11 [leia-llm/Leia-Swallow-13b](https://huggingface.co/leia-llm/Leia-Swallow-13b) - Leia-Swallow-13BLEIA is a training technique for autoregressive LLMs that effectively improves their performance in languages other than English by enhancing cross-lingual knowledge transfer from English to a target language. This model is constructed by applying LEIA to Swallow, a Japanese-English bilingual LLM based on LLaMA 2.The model achieves enhanced performance on four out of six Japanese question answering benchmarks and equivalent performance on the remaining two, as reported below.
 * 📥 11 [AdapterHub/bert-base-multilingual-cased-ja-wiki_pfeiffer](https://huggingface.co/AdapterHub/bert-base-multilingual-cased-ja-wiki_pfeiffer) - Adapter bert-base-multilingual-cased-ja-wiki_pfeiffer for bert-base-multilingual-casedPfeiffer Adapter trained with Masked Language Modelling on Japanese Wikipedia Articles for 250k steps and a batch size of 64.This adapter was created for usage with the Adapters library. UsageFirst, install adapters:pip install -U
 * 📥 11 [akiFQC/bert-base-japanese-v3_nli-jsnli-jnli-jsick](https://huggingface.co/akiFQC/bert-base-japanese-v3_nli-jsnli-jnli-jsick) - Cross-Encoder for Natural Language Inference(NLI) for JapaneseThis model was trained using SentenceTransformers Cross-Encoder class. This model is based on tohoku-nlp/bert-base-japanese-v3.Training DataThe model was trained on following datasets.
 * 📥 11 [thefrigidliquidation/nllb-jaen-1.3B-lightnovels](https://huggingface.co/thefrigidliquidation/nllb-jaen-1.3B-lightnovels) - NLLB 1.3B fine-tuned on Japanese to English Light Novel translationThis model was fine-tuned on light and web novel for Japanese to English translation. It can translate sentences and paragraphs up to 512 tokens.
 * 📥 11 [qqpann/wav2vec2-large-xlsr-japanese-0325-1200](https://huggingface.co/qqpann/wav2vec2-large-xlsr-japanese-0325-1200) - Wav2Vec2-Large-XLSR-53-{language} #TODO: replace language with your {language}, e.g. FrenchFine-tuned facebook/wav2vec2-large-xlsr-53 on {language} using the Common Voice, ... and ... dataset{s}. #TODO: replace {language} with your language, e.g. French and eventually add more datasets that were used and eventually remove common voice if model was not trained on common voiceWhen using this model, make sure that your speech input is sampled at 16kHz.

## Datasets

This list is sorted by downloads as of May 13, 2024.

 * 📥 27586 [shunk031/JGLUE](https://huggingface.co/datasets/shunk031/JGLUE) - Please feel free to open an issue or pull request. Dataset SummaryFrom JGLUE's README.md:JGLUE, Japanese General Language Understanding Evaluation, is built to measure the general NLU ability in Japanese.
 * 📥 26232 [juletxara/mgsm](https://huggingface.co/datasets/juletxara/mgsm) - The same 250 problems from GSM8K are each translated via human annotators in 10 languages. The 10 languages are:SpanishFrenchGermanRussianChineseJapaneseThaiSwahiliBengaliTeluguGSM8K (Grade School Math 8K) is a dataset of 8.5K high quality linguistically diverse grade school math word problems.
 * 📥 11934 [datasets/iwslt2017](https://huggingface.co/datasets/iwslt2017) - SummaryThe IWSLT 2017 Multilingual Task addresses text translation, including zero-shot translation, with a single MT systemacross all directions including English, German, Dutch, Italian and Romanian. As unofficial task, conventionalbilingual text translation is offered between English and Arabic, French, Japanese, Chinese, German and Korean.
 * 📥 11209 [sbintuitions/JMTEB](https://huggingface.co/datasets/sbintuitions/JMTEB) - JMTEB: Japanese Massive Text Embedding BenchmarkJMTEB is a benchmark for evaluating Japanese text embedding models.
 * 📥 2884 [kunishou/databricks-dolly-15k-ja](https://huggingface.co/datasets/kunishou/databricks-dolly-15k-ja) - This dataset was created by automatically translating "databricks-dolly-15k" into Japanese. This dataset is licensed under CC-BY-SA-3.0Last Update : 2023-05-11databricks-dolly-15k-jahttps://github.com/kunishou/databricks-dolly-15k-jadatabricks-dolly-15khttps://github.com/databrickslabs/dolly/tree/master/data
 * 📥 1805 [llm-book/wrime-sentiment](https://huggingface.co/datasets/llm-book/wrime-sentiment) - GitHub リポジトリ ids-cv/wrime で公開されているデータセットを利用しています。 Avg. 
 * 📥 1684 [cl-nagoya/auto-wiki-qa](https://huggingface.co/datasets/cl-nagoya/auto-wiki-qa) - AutoWikiQA東工大が公開しているSwallow-MXを用いて、Wikipedia中のテキストを入力として「質問(query)」と「回答(answer)」を生成し、生成された質問と回答についてフィルタリングを行ったデータセットです。 日本語のフリーなQAデータセットとしては2024年4月現在で最大規模となっています。
 * 📥 1392 [range3/wiki40b-ja](https://huggingface.co/datasets/range3/wiki40b-ja) - range3/wiki40b-jaThis dataset consists of three parquet files from the wiki40b dataset with only Japanese data extracted. It is generated by the following python code.
 * 📥 1259 [elyza/ELYZA-tasks-100](https://huggingface.co/datasets/elyza/ELYZA-tasks-100) - ELYZA-tasks-100: 日本語instructionモデル評価データセットData Description本データセットはinstruction-tuningを行ったモデルの評価用データセットです。 詳細は リリースのnote記事 を参照してください。
 * 📥 1186 [fujiki/japanese_hh-rlhf-49k](https://huggingface.co/datasets/fujiki/japanese_hh-rlhf-49k) - This is a little bit different version of kunishou/hh-rlhf-49k-ja without ng_translation == 1 examples. Please also refer to the original dataset kunishou/hh-rlhf-49k-ja.
 * 📥 941 [datasets/wiki_atomic_edits](https://huggingface.co/datasets/wiki_atomic_edits) - Leaderboards[More Information Needed]LanguagesThe languages in the dataset are:deenesfritjp: Japanese (ja)ruzhDataset StructureData InstancesHere are some examples of questions and facts:Data Fields[More Information Needed]Data Splits[More Information Needed]Dataset CreationCuration Rationale[More Information Needed]Source Data[More Information Needed]Initial Data Collection and Normalization[More Information Needed]
 * 📥 861 [kumapo/JAQKET](https://huggingface.co/datasets/kumapo/JAQKET) - 作成するデータセットは，既存研究 [7] に倣い，Wikipedia2 の記事名を答えとした，日本語のオープンドメイン QA タスクのデータセットである. Supported TasksJAQKET v1.0From the original paper:本研究で扱う日本語オープンドメイン QA タスクを定義する.
 * 📥 813 [mkshing/xlsum_ja](https://huggingface.co/datasets/mkshing/xlsum_ja) - This is the filtered Japanese subset of XL-Sum followed by PaLM 2filters15-gram overlap* code: https://gist.github.com/mkshing/d6371cbfdd50d4f352cee247fd4dd86anumber of examplestrain: 4215 (before: 7113)validation: 758 (before: 889)test: 766 (before: 889)
 * 📥 779 [llm-jp/oasst1-21k-ja](https://huggingface.co/datasets/llm-jp/oasst1-21k-ja) - oasst1-21k-jaThis repository provides an instruction tuning dataset developed by LLM-jp, a collaborative project launched in Japan. This dataset is a Japanese translation of an English subset of oasst1 using DeepL.English subset is here.
 * 📥 729 [llm-jp/databricks-dolly-15k-ja](https://huggingface.co/datasets/llm-jp/databricks-dolly-15k-ja) - databricks-dolly-15k-jaThis repository provides an instruction tuning dataset developed by LLM-jp, a collaborative project launched in Japan. This dataset is a Japanese translation of databricks-dolly-15k using DeepL.Send Questions tollm-jp(at)nii.ac.jpModel Card AuthorsThe names are listed in alphabetical order.
 * 📥 728 [datasets/bsd_ja_en](https://huggingface.co/datasets/bsd_ja_en) - The dataset was constructed in 3 steps:selecting business scenes,writing monolingual conversation scenarios according to the selected scenes, andtranslating the scenarios into the other language. Half of the monolingual scenarios were written in Japaneseand the other half were written in English.
 * 📥 539 [joujiboi/japanese-anime-speech](https://huggingface.co/datasets/joujiboi/japanese-anime-speech) - Japanese Anime Speech Dataset日本語はこちらjapanese-anime-speech is an audio-text dataset designed for the training of automatic speech recognition models. The dataset is comprised of thousands of audio clips and their corresponding transcriptions from different visual novels.
 * 📥 496 [nlp-waseda/JMMLU](https://huggingface.co/datasets/nlp-waseda/JMMLU) - JMMLUJapanese Massive Multitask Language Understanding BenchmarkJMMLU is a four-choice question set consisting of Japanese-translated questions of a portion of MMLU (Paper, Github) (Translated questions) and questions based on unique Japanese cultural context (Japanese questions). It is designed to assess the performance of large language models in Japanese.
 * 📥 453 [turing-motors/LLaVA-Instruct-150K-JA](https://huggingface.co/datasets/turing-motors/LLaVA-Instruct-150K-JA) - Dataset DetailsDataset Type:Japanese LLaVA Instruct 150K is a localized version of the original LLaVA Visual Instruct 150K dataset. This version is translated into Japanese using DeepL API and is aimed at serving similar purposes in the context of Japanese language.
 * 📥 450 [kogi-jwu/jhumaneval](https://huggingface.co/datasets/kogi-jwu/jhumaneval) - LLM のコード生成能力の標準ベンチマーク HumanEval の日本語翻訳版です。機械翻訳(DeepL, GPT-4)の翻訳結果を全て人手によって再修正し、 訳文を日本人のプログラマが読んで理解し、コードが書ける内容かチェックしました。ただし、英語版 HumanEval の間違いは、修正せずに残して、 HumanEval 同様に不完全なドキュメントからの生成能力を見るようになっています。日本語LLM のベンチマークとしてお使いください。LanguagesThe programmin
 * 📥 373 [nyanko7/danbooru2023](https://huggingface.co/datasets/nyanko7/danbooru2023) - Danbooru2023: A Large-Scale Crowdsourced and Tagged Anime Illustration DatasetDanbooru2023 is a large-scale anime image dataset with over 5 million images contributed and annotated in detail by an enthusiast community.
 * 📥 353 [llm-jp/hh-rlhf-12k-ja](https://huggingface.co/datasets/llm-jp/hh-rlhf-12k-ja) - hh-rlhf-12k-jaThis repository provides a human preference dataset developed by LLM-jp, a collaborative project launched in Japan. This dataset is a Japanese translation of a subset of hh-rlhf using DeepL.This dataset consists of 12,000 entries randomly sampled from hh-rlhf.
 * 📥 351 [llm-book/livedoor-news-corpus](https://huggingface.co/datasets/llm-book/livedoor-news-corpus) - オリジナルのサイトと同じものを使用しています。 本コーパスは、NHN Japan株式会社が運営する「livedoor ニュース」のうち、下記のクリエイティブ・コモンズライセンスが適用されるニュース記事を収集し、可能な限りHTMLタグを取り除いて作成したものです。
 * 📥 340 [yuzuai/rakuda-questions](https://huggingface.co/datasets/yuzuai/rakuda-questions) - Rakuda - Questions for Japanese modelsRepository: https://github.com/yuzu-ai/japanese-llm-rankingThis is a set of 40 questions in Japanese about Japanese-specific topics designed to evaluate the capabilities of AI Assistants in Japanese.
 * 📥 320 [globis-university/aozorabunko-clean](https://huggingface.co/datasets/globis-university/aozorabunko-clean) - OverviewThis dataset provides a convenient and user-friendly format of data from Aozora Bunko (青空文庫), a website that compiles public-domain books in Japan, ideal for Machine Learning applications.[For Japanese] 日本語での概要説明を Qiita に記載しました: https://qiita.com/akeyhero/items/b53eae1c0bc4d54e321fMethodologyThe code to reproduce this dataset is made available on GitHub: globis-org/aozorabunko-exctractor.1. Data collectionWe firstly downloaded the CSV file that lists all works.
 * 📥 284 [llm-book/ner-wikipedia-dataset](https://huggingface.co/datasets/llm-book/ner-wikipedia-dataset) - Githubリポジトリstockmarkteam/ner-wikipedia-datasetで公開されているデータセットを利用しています。 Citation@inproceedings{omi-2021-wikipedia,title = "Wikipediaを用いた日本語の固有表現抽出のデータセットの構築",author = "近江 崇宏",booktitle = "言語処理学会第27回年次大会",year = "2021",url = "https://anlp.jp/proceedings/annual_meeting/2021/pdf_dir/P2-7.pdf",}LicenceWikipedia日本語版と同じCC-BY-SA 3.0のライセンスに従います。
 * 📥 279 [SkelterLabsInc/JaQuAD](https://huggingface.co/datasets/SkelterLabsInc/JaQuAD) - JaQuAD is developed to provide a SQuAD-like QA dataset in Japanese. JaQuAD contains 39,696 question-answer pairs.
 * 📥 276 [neulab/odex](https://huggingface.co/datasets/neulab/odex) - ODEX is an Open-Domain EXecution-based NL-to-Code generation data benchmark. It contains 945 samples with a total of 1,707 human-written test cases, covering intents in four different natural languages --  439 in English, 90 in Spanish, 164 in Japanese, and 252 in Russian.
 * 📥 258 [p1atdev/ichikara-instruction](https://huggingface.co/datasets/p1atdev/ichikara-instruction) - ichikara-instruction (Non Commercial)LLMのための日本語インストラクションデータ 公開ページ公開ページより、本データに関して、言語処理学会第３０回年次大会において発表を行います。 データを使われた方は、HPと共に下記の通りにお願いします。
 * 📥 239 [kunishou/OpenMathInstruct-1-1.8m-ja](https://huggingface.co/datasets/kunishou/OpenMathInstruct-1-1.8m-ja) - OpenMathInstruct-1 を日本語に自動翻訳した商用利用可能な180万件の指示チューニングデータセットになります。 OpenMathInstruct-1 は、GSM8K および MATH ベンチマーク トレーニングセットの question と Mixtral-8x7B モデルを使用して生成された solution のペアで構成される数学分野のデータセットです。
 * 📥 225 [if001/aozorabunko-clean-sin](https://huggingface.co/datasets/if001/aozorabunko-clean-sin) - this is forkhttps://huggingface.co/datasets/globis-university/aozorabunko-cleanfilteredrow["meta"]["文字遣い種別"] == "新字新仮名"
 * 📥 222 [datasets/snow_simplified_japanese_corpus](https://huggingface.co/datasets/snow_simplified_japanese_corpus) - The corpus has 50,000 manually simplified and aligned sentences. This corpus contains the original sentences, simplified sentences and English translation of the original sentences.
 * 📥 192 [shunk031/jsnli](https://huggingface.co/datasets/shunk031/jsnli) - Dataset PreprocessingSupported Tasks and LeaderboardsLanguages注釈はすべて日本語を主要言語としています。 Dataset Structureデータセットは TSV フォーマットで、各行がラベル、前提、仮説の三つ組を表します。
 * 📥 189 [izumi-lab/llm-japanese-dataset](https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset) - llm-japanese-datasetLLM構築用の日本語インストラクション(チャット)データセット主に，英語で構築されたLLMモデルなどに対して，チャット(Instruction)応答タスクに関してLoRAなどでチューニングするために使用できます． ※様々な公開言語資源を利用させていただきました．
 * 📥 187 [transformersegmentation/CHILDES](https://huggingface.co/datasets/transformersegmentation/CHILDES) - Phonemized Child Directed Speech DatasetThis dataset contains utterance downloaded from CHILDES which have been pre-processed and converted to phonemic transcriptions by this processing script. Many of the columns from CHILDES have been preserved in case they may be useful for experiments (e.g. number of morphemes, part-of-speech tags, etc.).
 * 📥 185 [hotchpotch/JQaRA](https://huggingface.co/datasets/hotchpotch/JQaRA) - JQaRA : Japanese Question Answering with Retrieval Augmentation - 検索拡張(RAG)評価のための日本語 Q&amp;A データセット高性能な LLM の台頭に伴い、LLM を用いた質疑応答のユースケースが増加しています。 しかしながら、LLM は質問に対して適切な回答する知識を有していないと、答えることができないだけでなく、誤った回答を返答するといった課題が存在します。
 * 📥 182 [hatakeyama-llm-team/AutoGeneratedJapaneseQA](https://huggingface.co/datasets/hatakeyama-llm-team/AutoGeneratedJapaneseQA) - 自動生成Q&amp;A種々のデータソースから､MaziyarPanahi/Mixtral-8x22B-Instruct-v0.1-GGUFを使ってQ&amp;Aを自動生成したものです｡二種類の自動生成された回答が存在しますCommonCrawlまたは、CC-BY系のデータソースから生成しています。 元の文章との類似度(依拠性)が低くなるようにするため、元文章からランダムに部分抜粋したテキストを用いています
 * 📥 174 [kunishou/oasst1-89k-ja](https://huggingface.co/datasets/kunishou/oasst1-89k-ja) - This dataset was created by automatically translating "OpenAssistant/oasst1" into Japanese. The "ng_translation" flag indicates that the translation was not successful, and "1" means that the translation failed.
 * 📥 166 [range3/cc100-ja](https://huggingface.co/datasets/range3/cc100-ja) - range3/cc100-jaThis dataset consists of parquet files from the cc100 dataset with only the Japanese language extracted and sharded. このデータセットは、cc100データセットの日本語のみを抽出し、シャーディングしたparquetファイルで構成されます。
 * 📥 163 [taishi-i/awesome-japanese-nlp-classification-dataset](https://huggingface.co/datasets/taishi-i/awesome-japanese-nlp-classification-dataset) - Dataset overviewThis dataset identifies whether a GitHub repository description pertains to Japanese natural language processing (NLP).The labels are categorized as "Relevant (1)" and "Not Relevant (0)". Problem Setting:Training Data: Repository descriptions from before 2022Test Data: Repository descriptions from 2023Objective: To detect repositories related to Japanese NLPData Collection:Positive Examples: Repositories listed in "awesome-japanese-nlp-resources" as of September 9, 2023Negative Examples: Coll
 * 📥 160 [kunishou/oasst2-135k-ja](https://huggingface.co/datasets/kunishou/oasst2-135k-ja) - Update:2023/12/25oasst2-135k-jaをチャット形式に変換したoasst2-chat-68k-jaを公開しました。This dataset was created by automatically translating "OpenAssistant/oasst2" into Japanese by DeepL."OpenAssistant/oasst2" を DeepL翻訳を用いて日本語に自動翻訳したデータセットになります。以下のコードを用いることで、 Instruction と Output （prompterの命令とassistantの回答）の形式に変換することができます。ファインチューニングで使用する場合はこちらのコードで変換して下さい（変換には5分程度かかります）。変換コード参考https://github.com/h2oai/h2o-llmstudio/blob/5ebfd3879e226b4e1afd0a0b45eb632e60412129/app_utils/utils.py#L1888pip install datasetsfrom datasets import l
 * 📥 158 [llm-book/jawiki-sentences](https://huggingface.co/datasets/llm-book/jawiki-sentences) - GitHub リポジトリ singletongue/wikipedia-utils で公開されているデータセットを利用しています。 Licence本データセットで使用している Wikipedia のコンテンツは、クリエイティブ・コモンズ表示・継承ライセンス 3.0 (CC BY-SA 3.0) および GNU 自由文書ライセンス (GFDL) の下に配布されているものです。
 * 📥 154 [hatakeyama-llm-team/japanese2010](https://huggingface.co/datasets/hatakeyama-llm-team/japanese2010) - 日本語ウェブコーパス2010こちらのデータをhuggingfaceにアップロードしたものです｡2009 年度における著作権法の改正（平成21年通常国会　著作権法改正等について | 文化庁）に基づき，情報解析研究への利用に限って利用可能です｡形態素解析を用いて､自動で句点をつけました｡変換コード変換スクリプト形態素解析など
 * 📥 131 [fujiki/japanese_alpaca_data](https://huggingface.co/datasets/fujiki/japanese_alpaca_data) - [github]. Please also refer to this repo.
 * 📥 111 [taishi-i/nagisa_stopwords](https://huggingface.co/datasets/taishi-i/nagisa_stopwords) - Japanese stopwords for nagisaThis is a stopword list of frequently used words in the Japanese language, created according to the tokenization rules of the Japanese text analysis library, nagisa. This list is constructed by extracting the top 100 most commonly used words from the CC-100 dataset and Wikipedia.
 * 📥 111 [hotchpotch/wikipedia-ja-20231030](https://huggingface.co/datasets/hotchpotch/wikipedia-ja-20231030) - Wikipedia Japanese data (20231030)Source Date: 2023/10/30Source: https://dumps.wikimedia.org/other/cirrussearch/LicenseCC BY-SA 4.0ExampleWIP
 * 📥 107 [Nexdata/multi_language](https://huggingface.co/datasets/Nexdata/multi_language) - SummaryThe dataset contains 25,000 hours of multi-language reading speech data. It's recorded by native speakers, covering English, French, German, Russian, Spanish, Portuguese, Italian, Japanese, Korean, Hindi, Vietnamese, Tagalog, Thai etc.
 * 📥 107 [NilanE/ParallelFiction-Ja_En-100k](https://huggingface.co/datasets/NilanE/ParallelFiction-Ja_En-100k) - Dataset detailsEach entry in this dataset is a sentence-aligned Japanese web novel chapter and English fan translation. The intended use-case is for document translation tasks.
 * 📥 106 [datasets/covid_tweets_japanese](https://huggingface.co/datasets/covid_tweets_japanese) - The annotation is by majority decision by 5 - 10 crowd workers. Target tweets include "COVID" or "コロナ".
 * 📥 106 [Atsushi/fungi_indexed_mycological_papers_japanese](https://huggingface.co/datasets/Atsushi/fungi_indexed_mycological_papers_japanese) - fungi_indexed_mycological_papers_japanese大菌輪「論文3行まとめ」データセット最終更新日：2024/2/23（R3-11457まで）====LanguagesJapaneseThis dataset is available in Japanese only. 概要Atsushi Nakajima（中島淳志）が個人で運営しているWebサイト大菌輪 では、数千件以上の菌類分類学論文を「論文3行まとめ」という形で要約および索引付け（インデキシング）した情報を提供しています。
 * 📥 106 [Atsushi/fungi_diagnostic_chars_comparison_japanese](https://huggingface.co/datasets/Atsushi/fungi_diagnostic_chars_comparison_japanese) - fungi_diagnostic_chars_comparison_japanese大菌輪「識別形質まとめ」データセット最終更新日：2024/2/23（R3-11457まで）====LanguagesJapaneseThis dataset is available in Japanese only. 概要Atsushi Nakajima（中島淳志）が個人で運営しているWebサイト大菌輪 では、数千件以上の菌類分類学論文を「論文3行まとめ」という形で要約および索引付け（インデキシング）した情報を提供しています。
 * 📥 105 [matsuxr/JaGovFaqs-22k](https://huggingface.co/datasets/matsuxr/JaGovFaqs-22k) - このデータセットについてこのデータは、日本の官公庁のWebサイトに掲載されている「よくある質問」を手作業で抽出し、インストラクション用のデータセットとしたものです。 日本の官公庁のWebサイトは多くが「政府標準利用規約（第2.0版）」に準拠しており、この規約はCC-BY-4.0（国際）と互換性があると記述されています。
 * 📥 105 [Atsushi/fungi_trait_circus_database](https://huggingface.co/datasets/Atsushi/fungi_trait_circus_database) - fungi_trait_circus_database大菌輪「Trait Circus」データセット（統制形質）最終更新日：2023/12/29====LanguagesJapanese and EnglishPlease do not use this dataset for academic purposes for the time being.  (casual use only)当面の間仮公開とします。
 * 📥 103 [kunishou/J-ResearchCorpus](https://huggingface.co/datasets/kunishou/J-ResearchCorpus) - J-ResearchCorpusUpdate:2024/3/16言語処理学会第30回年次大会(NLP2024)を含む、論文 1,343 本のデータを追加2024/2/25言語処理学会誌「自然言語処理」のうち CC-BY-4.0 で公開されている論文 360 本のデータを追加概要CC-BY-* ライセンスで公開されている日本語論文や学会誌等から抜粋した高品質なテキストのデータセットです。 言語モデルの事前学習や RAG 等でご活用下さい。
 * 📥 81 [izumi-lab/llm-japanese-dataset-vanilla](https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset-vanilla) - llm-japanese-dataset-vanillaLLM構築用の日本語チャットデータセットizumi-lab/llm-japanese-dataset から，日英翻訳のデータセット等を抜いたものです． 主に，日本語LLMモデルなどに対して，チャット(Instruction)応答タスクに関してLoRAなどでチューニングするために使用できます．
 * 📥 81 [Verah/JParaCrawl-Filtered-English-Japanese-Parallel-Corpus](https://huggingface.co/datasets/Verah/JParaCrawl-Filtered-English-Japanese-Parallel-Corpus) - IntroductionThis is a LLM-filtered set of the first 1M rows from ntt's JParaCrawl v3 large English-Japanese parallel corpus. The original JParaCrawl corpus was put together by automated means - aligning Japanese texts with their apparent English translations that were found in-the-wild, on the internet.
 * 📥 74 [SakanaAI/JA-VG-VQA-500](https://huggingface.co/datasets/SakanaAI/JA-VG-VQA-500) - JA-VG-VQA-500Dataset DescriptionJA-VG-VQA-500 is a 500-sample subset of Japanese Visual Genome VQA dataset. This dataset was used in the evaluation of EvoVLM-JP-v1-7B.Please refer to our report and blog for more details.
 * 📥 64 [gbenson/webui-dom-snapshots](https://huggingface.co/datasets/gbenson/webui-dom-snapshots) - It has been generated using this raw template. Dataset DetailsDataset DescriptionCurated by: Gary BensonLanguages: Mostly English (87%); Dutch, French, Chinese, Japanese (1-2% each);
 * 📥 57 [range3/wikipedia-ja-20230101](https://huggingface.co/datasets/range3/wikipedia-ja-20230101) - range3/wikipedia-ja-20230101This dataset consists of a parquet file from the wikipedia dataset with only Japanese data extracted. It is generated by the following python code.
 * 📥 56 [kunishou/jp-effective-instructions](https://huggingface.co/datasets/kunishou/jp-effective-instructions) - oasst1-89k-ja , databricks-dolly-15k-ja , hh-rlhf-49k-ja の中から JGLUE（ JcommonsenseQA , MARC-ja , JSQuAD ）の観点で高品質なデータセットに絞り込んだデータセットです。 品質スコアリングの詳細はこちらを参考にして下さい。
 * 📥 55 [llm-book/jsnli](https://huggingface.co/datasets/llm-book/jsnli) - JSNLI Version 1.1 のデータセットのうち、フィルタリング後の訓練セット (train_w_filtering)  と検証セット (dev) を使用しています。
 * 📥 53 [hpprc/jsick](https://huggingface.co/datasets/hpprc/jsick) - Dataset. JSICK is the Japanese NLI and STS dataset by manually translating the English dataset SICK (Marelli et al., 2014) into Japanese.
 * 📥 52 [svjack/pokemon-blip-captions-en-ja](https://huggingface.co/datasets/svjack/pokemon-blip-captions-en-ja) - Dataset used to train Pokémon text to image model, add a Japanese Column of Pokémon BLIP captionsBLIP generated captions for Pokémon images from Few Shot Pokémon dataset introduced by Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis (FastGAN). Original images were obtained from FastGAN-pytorch and captioned with the pre-trained BLIP model.
 * 📥 52 [GENIAC-Team-Ozaki/chatbot-arena-ja-calm2-7b-chat-experimental_deduped](https://huggingface.co/datasets/GENIAC-Team-Ozaki/chatbot-arena-ja-calm2-7b-chat-experimental_deduped) - chatbot-arena-ja-calm2-7b-chatからpromptが一致するデータを削除したデータセットです。
 * 📥 51 [hpprc/jawiki](https://huggingface.co/datasets/hpprc/jawiki) - JaWikiWikipediaのHTML形式のダンプファイルから抽出したテキストデータセットです。 Wikiextractorによって抽出したテキストデータと異なり、段落などの文書構造を維持したまま、不要なマークアップのないテキストが利用できます。
 * 📥 51 [hpprc/mmarco-ja](https://huggingface.co/datasets/hpprc/mmarco-ja) - mmarcoデータセットのquery--passageのペアについて、queryをkeyとして重複を削除したデータセットです。 元データ中のエンコーディング周りのミスの修正やNFKC正規化などの前処理を行ってあります。
 * 📥 51 [Amani27/massive_translation_dataset](https://huggingface.co/datasets/Amani27/massive_translation_dataset) - Supported Tasks and LeaderboardsTranslationLanguagesEnglish (en_US)German (de_DE)Hindi (hi_IN)Spanish (es_ES)French (fr_FR)Italian (it_IT)Arabic (ar_SA)Dutch (nl_NL)Japanese (ja_JP)Portugese (pt_PT)
 * 📥 46 [llm-book/aio-retriever](https://huggingface.co/datasets/llm-book/aio-retriever) - GitHub リポジトリ cl-tohoku/quiz-datasets で公開されているデータセットを利用しています。 Licence本データセットに含まれる一部のクイズ問題の著作権は abc/EQIDEN 実行委員会に帰属するものであり、これらのクイズ問題は本書における使用許諾を得ているものです。
 * 📥 46 [globis-university/aozorabunko-chats](https://huggingface.co/datasets/globis-university/aozorabunko-chats) - OverviewThis dataset is of conversations extracted from Aozora Bunko (青空文庫), which collects public-domain books in Japan, using a simple heuristic approach.[For Japanese] 日本語での概要説明を Qiita に記載しました: https://qiita.com/akeyhero/items/b53eae1c0bc4d54e321fMethodFirst, lines surrounded by quotation mark pairs (「」) are extracted as utterances from the text field of globis-university/aozorabunko-clean. Then, consecutive utterances are collected and grouped together.
 * 📥 45 [kunishou/oasst1-chat-44k-ja](https://huggingface.co/datasets/kunishou/oasst1-chat-44k-ja) - oasst1-89k-jaをチャット形式に変換したデータセットになります。 マルチターン会話でのファインチューニングをする際にご活用下さい（1レコードのトークン長が大きいのでそれなりの計算リソースが必要になります）。
 * 📥 42 [stockmark/ner-wikipedia-dataset](https://huggingface.co/datasets/stockmark/ner-wikipedia-dataset) - Wikipediaを用いた日本語の固有表現抽出データセットGitHub: https://github.com/stockmarkteam/ner-wikipedia-dataset/LICENSE: CC-BY-SA 3.0Developed by Stockmark Inc.
 * 📥 42 [dichmau/ja_vi_translation](https://huggingface.co/datasets/dichmau/ja_vi_translation) - Japanese-Vietnamese Translated Sentence Pairs.
 * 📥 38 [Fhrozen/CABankSakuraCHJP](https://huggingface.co/datasets/Fhrozen/CABankSakuraCHJP) - CABank Japanese CallHome CorpusParticipants:   120Type of Study:  phone callLocation:   United StatesMedia type: audioDOI:    doi:10.21415/T5H59VWeb: https://ca.talkbank.org/access/CallHome/jpn.htmlCitation informationSome citation here. In accordance with TalkBank rules, any use of data from this corpus must be accompanied by at least one of the above references.
 * 📥 36 [neulab/mconala](https://huggingface.co/datasets/neulab/mconala) - Spanish, Japanese, and Russian. LanguagesSpanish, Japanese, Russian; PythonDataset StructureHow to Usefrom datasets import load_dataset# Spanish subsetload_dataset("neulab/mconala", "es")DatasetDict({test: Dataset({features: ['question_id', 'intent', 'rewritten_intent', 'snippet'],num_rows: 341})})
 * 📥 34 [simon3000/genshin-voice](https://huggingface.co/datasets/simon3000/genshin-voice) - Genshin VoiceGenshin Voice is a dataset of voice lines from the popular game Genshin Impact. Hugging Face 🤗  Genshin-VoiceLast update at 2024-04-30413429 wavs18016 without speaker (4%)22956 without transcription (6%)720 without inGameFilename (0%)Dataset DetailsDataset DescriptionThe dataset contains voice lines from the game's characters in multiple languages, including Chinese, English, Japanese, and Korean.
 * 📥 33 [y2lan/japan-law](https://huggingface.co/datasets/y2lan/japan-law) - Japanese LawsThis dataset comprises 8.75K law records retrieved from the official Japanese government website e-Gov. Each entry furnishes comprehensive details about a particular law, encapsulating its number, title, unique ID, the date it came into effect, and its complete text. To ensure the dataset's uniqueness, deduplication was executed based on the most recent effective version as of August 1, 2023.A typical entry in this dataset is structured as follows:{"num": "Law Number (e.g., Reiwa 5th Year Pollut
 * 📥 32 [kunishou/amenokaku-code-instruct](https://huggingface.co/datasets/kunishou/amenokaku-code-instruct) - Amenokaku-Code-InstructUpdate:2023/12/27データセットに JaxTon , プロになるJava のコードデータ 180 レコードを追加しました。 概要コードに特化した5.2KのInstructionデータセットです。
 * 📥 32 [hotchpotch/JaCWIR](https://huggingface.co/datasets/hotchpotch/JaCWIR) - JaCWIR: Japanese Casual Web IR - 日本語情報検索評価のための小規模でカジュアルなWebタイトルと概要のデータセット近年、大規模言語モデル（LLM）の台頭により、一般的な日本語を用いた自然な検索クエリで質問するユースケースが増えています。 しかしながら、多様なジャンルの Web 記事に対して、ユーザーの質問に適切に答えられるような情報検索システムを評価するための日本語データセットは十分ではありません。
 * 📥 29 [sudy-super/CoTangent](https://huggingface.co/datasets/sudy-super/CoTangent) - CoTangentは人手で作成された高品質でクリーンな100セットの日本語CoT用データセットです。 CoTangent_ja.json: CoT部分とoutput部分が繋がっています。
 * 📥 28 [llm-book/ner-wikinews-dataset](https://huggingface.co/datasets/llm-book/ner-wikinews-dataset) - 固有表現ラベルはllm-book/ner-wikipedia-datasetと同様のものを採用しており、全部で8種類 (人名、法人名、地名、製品名、政治的組織名、施設名、その他の組織名、イベント名)あります。 テストセットのみのデータセットとなっています。
 * 📥 28 [turing-motors/Japanese-Heron-Bench](https://huggingface.co/datasets/turing-motors/Japanese-Heron-Bench) - Japanese-Heron-BenchDataset DescriptionJapanese-Heron-Bench is a benchmark for evaluating Japanese VLMs (Vision-Language Models). We collected 21 images related to Japan.
 * 📥 27 [kunishou/databricks-dolly-69k-ja-en-translation](https://huggingface.co/datasets/kunishou/databricks-dolly-69k-ja-en-translation) - This dataset was created by automatically translating "databricks-dolly-15k" into Japanese. This dataset contains 69K ja-en-translation task data and is licensed under CC BY SA 3.0.Last Update : 2023-04-18databricks-dolly-15k-jahttps://github.com/kunishou/databricks-dolly-15k-jadatabricks-dolly-15khttps://github.com/databrickslabs/dolly/tree/master/data
 * 📥 26 [fujiki/wiki40b_ja](https://huggingface.co/datasets/fujiki/wiki40b_ja) - This dataset is a reformatted version of the Japanese portion of wiki40b dataset. When you use this dataset, please cite the original paper:@inproceedings{guo-etal-2020-wiki,title = "{W}iki-40{B}: Multilingual Language Model Dataset",author = "Guo, Mandy  andDai, Zihang  andVrande{\v{c}}i{\'c}, Denny  andAl-Rfou, Rami",booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",month = may,year = "2020",address = "Marseille, France",publisher = "European Language Resources Associati
 * 📥 26 [bclavie/mmarco-japanese-hard-negatives](https://huggingface.co/datasets/bclavie/mmarco-japanese-hard-negatives) - [Under Construction]This is a repository containing all the queries from the Japanese part of the MMarco dataset, the multilingual version of the MSMarco dataset. For each query, there are matching hard negatives:25 of them retrieved by the multilingual e5 base model.
 * 📥 25 [polm-stability/jblimp](https://huggingface.co/datasets/polm-stability/jblimp) - JBLiMPThis is the data from "JBLiMP: Japanese Benchmark of Linguistic Minimal Pairs" (Someya and Oseki, 2023). Only the validated pairs used for benchmarks are included, and only in JSONL format, since it's redundant with the TSV.For details see the original git repo or the paper.
 * 📥 23 [llm-book/jawiki-paragraphs](https://huggingface.co/datasets/llm-book/jawiki-paragraphs) - GitHub リポジトリ singletongue/wikipedia-utils で公開されているデータセットを利用しています。 Licence本データセットで使用している Wikipedia のコンテンツは、クリエイティブ・コモンズ表示・継承ライセンス 3.0 (CC BY-SA 3.0) および GNU 自由文書ライセンス (GFDL) の下に配布されているものです。
 * 📥 23 [Verah/tatoeba_dedupe_en-jp_2024-March-01](https://huggingface.co/datasets/Verah/tatoeba_dedupe_en-jp_2024-March-01) - English - Japanese pairs taken from https://tatoeba.org/en/downloads and then deduplicated. Row order has also been randomized to avoid clusters of similar translations.
 * 📥 21 [aixsatoshi/Swallow-MX-chatbot-DPO](https://huggingface.co/datasets/aixsatoshi/Swallow-MX-chatbot-DPO) - Chatbot Arena Conversationsの質問文から、aixsatoshi/Swallow-MX-8x7b-NVE-chatvector-Mixtral-instruct-v2を使用して応答文を作成しました質問文は、以下のモデルのPrompt部分を使用しましたChatbot Arena Conversations JA (calm2)以下引用です。 指示文（prompt）はlmsys/chatbot_arena_conversationsのユーザ入力（CC-BY 4.0）を和訳したものです。
 * 📥 20 [oshizo/japanese-wikipedia-paragraphs](https://huggingface.co/datasets/oshizo/japanese-wikipedia-paragraphs) - A slightly modified version of the parsing and chunking method for singletongue/wikipedia-utils. Pre-processing was performed using oshizo/wikipedia-utils, which is a fork of the original repository, singletongue/wikipedia-utils.
 * 📥 20 [yulanfmy/databricks-qa-ja](https://huggingface.co/datasets/yulanfmy/databricks-qa-ja) - データセット概要手動で作成したDatabricksに関する質問と回答ペアの日本語データセットです。 件数：約1,300件情報源：Databricks HPの日本語ブログやFAQなど、データブリック社員がポストしたQitta記事https://github.com/yulan-yan/build-your-chat-bot-JP　デモに利用したデータです。
 * 📥 19 [shi3z/ja_conv_wikipedia_llama2pro8b_30k](https://huggingface.co/datasets/shi3z/ja_conv_wikipedia_llama2pro8b_30k) - This dataset is based on the Japanese version of Wikipedia dataset and converted into a multi-turn conversation format using llama2Pro8B.Since it is a llama2 license, it can be used commercially for services. Some strange dialogue may be included as it has not been screened by humans.
 * 📥 19 [sudy-super/JetCopper-10B](https://huggingface.co/datasets/sudy-super/JetCopper-10B) - DescriptionThis dataset was used to pre-train Co-Encoder's Context Encoder when we participated in LOCAL AI HACKATHON #000.The number of tokens (Using tokenizer of calm2-chat)LanguageThe number of tokensJapanese4.7bEnglish5bCode0.9bNOTEThis dataset has not passed sentence end boundary determination or Perplexity Filtering, so there is room for improvement in quality.
 * 📥 19 [saldra/sakura_japanese_dataset](https://huggingface.co/datasets/saldra/sakura_japanese_dataset) - Sakura_dataset商用利用可能な超小規模高品質日本語データセット。categoryは以下commonsense_qa: 常識問題Calc-ape210k: 数学問題japanese-commonsense-openqa: 日本の常識問題(自作)下記データセットを使用しています。commonsense_qaMU-NLPC/Calc-ape210kLICENSEThis dataset is licensed under Database Contents License (DbCL) v1.0UpdateLast Update : 2023-06-07Example Code# モデルの読み込みimport osfrom peft.utils.config import TaskTypeos.environ["CUDA_VISIBLE_DEVICES"]="0"import peftimport transformersimport datasets#
 * 📥 17 [hpprc/mqa-ja](https://huggingface.co/datasets/hpprc/mqa-ja) - mqaデータセットのquery--passageのペアについて重複を削除したデータセットです。 元データ中のノイジーなテキストのクリーニングやNFKC正規化などの前処理を行ってあります。
 * 📥 17 [RyokoAI/Syosetu711K](https://huggingface.co/datasets/RyokoAI/Syosetu711K) - Not all information here may be accurate or accessible. Dataset SummarySyosetu711K is a dataset composed of approximately 711,700 novels scraped from the Japanese novel self-publishingwebsite Syosetuka ni Narou (JA: 小説家になろう, lit.
 * 📥 15 [sakusakumura/databricks-dolly-15k-ja-scored](https://huggingface.co/datasets/sakusakumura/databricks-dolly-15k-ja-scored) - For the English version, please click here. 概要databricks-dolly-15k-ja-scoredはkunishou/databricks-dolly-15k-jaの派生であり、BERTScoreによって提供される翻訳品質スコアが追加されています。
 * 📥 14 [baobab-trees/wikipedia-human-retrieval-ja](https://huggingface.co/datasets/baobab-trees/wikipedia-human-retrieval-ja) - Japanese Wikipedia Human Retrieval datasetThis is a Japanese question answereing dataset with retrieval on Wikipedia articlesby trained human workers. ContributorsYusuke Odadefined the dataset specification, data structure, and the scheme of data collection.
 * 📥 14 [CausalLM/GPT-4-Self-Instruct-Japanese](https://huggingface.co/datasets/CausalLM/GPT-4-Self-Instruct-Japanese) - Sorry, it's no longer available on Hugging Face. Please reach out to those who have already downloaded it.
 * 📥 14 [kunishou/ApolloCorpus-ja](https://huggingface.co/datasets/kunishou/ApolloCorpus-ja) - ApolloCorpus-ja概要多言語医療データセットの ApolloCorpus を日本語に自動翻訳した 525k の指示チューニングデータセットになります。 ApolloCorpus は、オープンソースでかつ品質を担保できるデータのみをスクリーニングし収集されたデータセットになります。
 * 📥 14 [toshi456/llava-bench-in-the-wild-ja](https://huggingface.co/datasets/toshi456/llava-bench-in-the-wild-ja) - This dataset is the data that corrected the translation errors and untranslated data of the Japanese data in MBZUAI/multilingual-llava-bench-in-the-wild. Original dataset is liuhaotian/llava-bench-in-the-wild.
 * 📥 14 [aixsatoshi/cosmopedia-japanese-100k](https://huggingface.co/datasets/aixsatoshi/cosmopedia-japanese-100k) - cosmopedia-japanese-20kのデータに、kunishou様から20k-100kをご提供いただけることになり100kまで拡大しました。 https://huggingface.co/datasets/kunishou/cosmopedia-100k-ja-previewテキスト生成プロンプトの翻訳も含むデータは、上記レポジトリを確認してください。
